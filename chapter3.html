<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Hans Pinckaers, Wouter Bulten, Jeroen van der Laak, Geert Litjens  " />
  <title>Introduction</title>
  <style>
    html {
      font-size: 12pt;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    div.abstract {
      margin: 2em 2em 2em 2em;
      text-align: left;
      font-size: 85%;
    }
    div.abstract-title {
      font-weight: bold;
      text-align: center;
      padding: 0;
      margin-bottom: 0.5em;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Introduction</h1>
<p class="author">Hans Pinckaers, Wouter Bulten, Jeroen van der Laak,
Geert Litjens <a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> <a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
<p>Prostate cancer is the most prevalent cancer among men in Western
countries, with 1.1 million new diagnoses every year. The gold standard
for the diagnosis of prostate cancer is a pathologists’ evaluation of
prostate tissue.</p>
<p>To potentially assist pathologists deep-learning-based cancer
detection systems have been developed. Many of the state-of-the-art
models are patch-based convolutional neural networks, as the use of
entire scanned slides is hampered by memory limitations on accelerator
cards. Patch-based systems typically require detailed, pixel-level
annotations for effective training. However, such annotations are seldom
readily available, in contrast to the clinical reports of pathologists,
which contain slide-level labels. As such, developing algorithms which
do not require manual pixel-wise annotations, but can learn using only
the clinical report would be a significant advancement for the
field.</p>
<p>In this paper, we propose to use a streaming implementation of
convolutional layers, to train a modern CNN (ResNet-34) with 21 million
parameters end-to-end on 4712 prostate biopsies. The method enables the
use of entire biopsy images at high-resolution directly by reducing the
GPU memory requirements by 2.4 TB. We show that modern CNNs, trained
using our streaming approach, can extract meaningful features from
high-resolution images without additional heuristics, reaching similar
performance as state-of-the-art patch-based and multiple-instance
learning methods. By circumventing the need for manual annotations, this
approach can function as a blueprint for other tasks in
histopathological diagnosis.</p>
<p>The source code to reproduce the streaming models is available at <a
href="https://github.com/DIAGNijmegen/pathology-streaming-pipeline"
class="uri">https://github.com/DIAGNijmegen/pathology-streaming-pipeline</a>.</p>
</div>
</header>
<h1 id="introduction">Introduction</h1>
<p>The current state-of-the-art in computer vision for image
classification tasks are convolutional neural networks (CNNs). Commonly,
convolutional neural networks are developed with low-resolution labeled
images, for example 0.001 megapixels for CIFAR-10<span class="citation"
data-cites="Krizhevsky2009"><sup>1</sup></span>, and 0.09-0.26
megapixels for ImageNet<span class="citation"
data-cites="Russakovsky2015"><sup>2</sup></span>. These images are
evaluated by the network and the parameters are optimized with
stochastic gradient descent by backpropagating the classification error.
Neural networks learn to extract relevant features from their input. To
effectively learn relevant features, optimizing these networks requires
relatively large datasets<span class="citation"
data-cites="Sun2017RevisitingUE"><sup>3</sup></span>.</p>
<p>In histopathology, due to the gigapixel size of scanned samples,
generally referred to as whole-slide images (WSIs), the memory
limitation of current accelerator cards prohibits training on the entire
image, in contrast to most of the natural images used in general
computer vision tasks. As such, most networks are trained on tiny
patches from the whole-slide image. Acquiring labels for these patches
can be expensive. They are generally based on detailed outlines of the
classes (e.g., tumor regions) by an experienced pathologist. This
outlining is not done in clinical practice, and is a tedious and
time-consuming task. This limits the dataset size for training models.
Also, we will need to create these annotations for every individual
task.</p>
<p>Besides time constraints, the diagnosis also suffers from substantial
inter-observer and intra-observer variability<span class="citation"
data-cites="Ozkan2016"><sup>4</sup></span>. For prostate cancer,
pathologists report the Gleason grading scheme<span class="citation"
data-cites="Epstein2010"><sup>5</sup></span>. Prognostically interesting
growth patterns are categorized, resulting in three levels of
aggressiveness. When cancer is present, the reports will mention a
Gleason score, a combination of the two most informative growth
patterns. These are the most common patterns or the highest pattern.
There is disagreement in the detection of prostate cancer, as in the
grading using the Gleason scheme. Since pathologists can disagree
between therapeutically relevant growth patterns and the presence of a
tumor, there are clinically relevant consequences per individual
case.</p>
<p>However, if we could circumvent labeling on a patch level, clinically
evaluated biopsies could be cheaply labeled using their clinical
reports. These reports contain all relevant information for clinical
decisions, and are thus of large value for machine learning
algorithms.</p>
<p>In this paper we will focus on prostate cancer detection, determining
whether a biopsy contains cancerous glands or not. The diagnosis of
prostate cancer—the most prevalent cancer for men in Western
countries—is established by detection on histopathological slides by a
pathologist. The microscopy slides containing cross-sections of biopsies
can exhibit morphological changes to prostate glandular structures. In
low-grade tumors, the epithelial cells still form glandular structures;
however, in the case of high-grade tumors, the glandular structures are
eventually lost<span class="citation"
data-cites="Fine2012"><sup>6</sup></span>.</p>
<p>In the presence of cancer, the percentage of cancerous tissue in a
prostate biopsy can be as low as 1%, the evaluation of the biopsies can
be tedious and error-prone, causing disagreement in the detection of
prostate cancer, as in the grading using the Gleason scheme<span
class="citation" data-cites="Ozkan2016"><sup>4</sup></span>.</p>
<p>Besides substantial inter-observer and intra-observer variability,
diagnosing prostate cancer is additionally challenging due to increasing
numbers of biopsies as a result of the introduction of prostate-specific
antigen (PSA) testing<span class="citation"
data-cites="10.1093/jnci/djp278"><sup>7</sup></span>. This number is
likely to increase further due to the aging population. In the light of
a shortage of pathologists<span class="citation"
data-cites="Wilson2018"><sup>8</sup></span>, automated methods could
alleviate workload.</p>
<p>To reduce potential errors and workload, recent work<span
class="citation"
data-cites="Bulten2020 Campanella2019 Nagpal2019 Litjens2016 Arvaniti2018 Lucas2019 Strom2020"><sup>9–15</sup></span>,
has shown the potential to automatically detect prostate cancer in
biopsies. These studies either use expensive, pixel-level annotations or
train CNNs with slide-level labels only, using a patch-based
approach.</p>
<p>One popular strategy is based on multiple-instance-learning
(MIL)<span class="citation"
data-cites="Courtiol2018 Ilse2018 Amores2013"><sup>16–18</sup></span>.
In this approach, the whole-slide image (WSI) is subdivided into a grid
of patches. The MIL assumption states that in a cancerous slide
(‘positive bag’), at least one patch will contain tumorous tissue,
whereas negative slides have no patches containing tumour. Under this
assumption, a CNN is trained on a patch-level to find the most tumorous
patch.</p>
<p>However, this approach has several disadvantages<span
class="citation" data-cites="VanderLaak2019"><sup>19</sup></span>.
First, this method only works for tasks where the label can be predicted
from one individual patch and a single adversarial patch can result in a
false positive detection. Second, it is essentially a patch-based
approach, therefore, the size of the patch constrains the field-of-view
of the network.</p>
<p>In this paper, we propose a novel method, using streaming<span
class="citation" data-cites="Pinckaers2019"><sup>20</sup></span>, to
train a modern CNN (ResNet-34) with 21 million parameters end-to-end to
detect prostate cancer in whole-slide images of biopsies. We also
investigate the use of transfer learning with this approach. This method
does not suffer from the same disadvantages as the aforementioned
approaches based on MIL: it can use the entire content of the
whole-slide image for its prediction and the field-of-view is not
limited to an arbitrary patch-size. We compare our approach against the
methods by Campanella <em>et al.</em><span class="citation"
data-cites="Campanella2019"><sup>10</sup></span> and Bulten <em>et
al.</em><span class="citation"
data-cites="Bulten2020"><sup>9</sup></span>. Since deep learning
algorithm in computational pathology can suffer from bad generalization
towards other scanners<span class="citation"
data-cites="Swiderska-Chadaj2020"><sup>21</sup></span>, we evaluated the
generalization of the MIL- and streaming-trained ResNet-34 on additional
biopsies acquired with a different scanner, previously used by Litjens
<em>et al.</em><span class="citation"
data-cites="Litjens2016"><sup>12</sup></span>.</p>
<p>The streaming implementation allows us to train a convolutional
neural network directly on entire biopsy images at high-resolution (268
megapixels) using only slide-level labels. We show that a
state-of-the-art CNN can extract meaningful features from
high-resolution images using labels from pathology reports without
additional heuristics or post-processing. Subsequently, we show that
transfer learning from ImageNet performs well for images that are 5000x
bigger than the original images used for training (224x224), improving
accuracy en decreasing train time.</p>
<p><img src="chpt3_imgs/data_overview.png" /></p>
<p><img src="chpt3_imgs/overview_olympus.png" /></p>
<h1 id="related-works">Related works</h1>
<p>For prostate cancer detection, previous works have used more
traditional machine learning (i.e., feature-engineering) approaches<span
class="citation"
data-cites="Gertych2015 nguyen2017 Naik2007"><sup>22–24</sup></span>.
Recently, researchers transitioned to using deep-learning-based methods
for the detection of cancer<span class="citation"
data-cites="Campanella2019 Litjens2016"><sup>10,12</sup></span>. Besides
detection, research on prostate cancer grading has also been
published<span class="citation"
data-cites="Arvaniti2018 Lucas2019 Bulten2020"><sup>9,13,14</sup></span>.</p>
<p>In this work, we train on labels for individual biopsies. Since in
other work, the memory of the accelerator restricts the input size of
the image, published methods are based on searching relevant patches of
the original slide<span class="citation"
data-cites="Ianni2020 Campanella2019 lu2020data Li2019a Caner2018"><sup>10,25–28</sup></span>,
or compressing the slide into a smaller latent space<span
class="citation" data-cites="Tellez2019"><sup>29</sup></span>.</p>
<p>We explicitly compare against the state-of-the-art method from
Campanella <em>et al.</em><span class="citation"
data-cites="Campanella2019"><sup>10</sup></span>. As mentioned before,
their multiple-instance-learning approach is based on the single
most-informative patch, and thus leads to a small field-of-view for the
network, and potential false positives because of a few adversarial
patches. To circumvent some of these problems, Campanelle <em>et
al.</em><span class="citation"
data-cites="Campanella2019"><sup>10</sup></span>, tried to increase the
field-of-view to multiple patches using a recurrent neural networks with
some improvement. Their system achieved an
area-under-the-receiver-operating curve (AUC) of 0.986. the aggregation
method increased the AUC to 0.991. To make the comparison fair, we
trained a ResNet-34 network architecture for both methods. However, when
training end-to-end, the context of the whole image is automatically
taken into account.</p>
<p>Campanella <em>et al.</em> showed that performance decreases when
using smaller datasets, concluding that at least 10,000 biopsies are
necessary for a good performance. Since they did not use data
augmentation (probably because of the big dataset at hand), we
investigated if we could reach similar performances with smaller dataset
sizes using data augmentation.</p>
<p>Since the mentioned implementation of multiple-instance-learning only
considers one patch, which may be less efficient, others<span
class="citation" data-cites="lu2020data Li2019a"><sup>26,27</sup></span>
improved the method by using multiple resolution patches and attention
mechanisms. Li <em>et al.</em> trained two models on low and high
resolution patches, only patches that were predicted as suspicious by
the lower resolution model were used to train the higher resolution
model. Additionally, to calculate the attention mechanisms, all patches
need to be kept in memory, limiting the size of the patches. Lu <em>et
al.</em><span class="citation"
data-cites="lu2020data"><sup>26</sup></span> showed that, additionally
to attention mechanisms, a frozen model pretrained on ImageNet decreases
training time and improves data efficiency. We also use ImageNet
weights, but by using the streaming-implementation of convolutions, can
unfreeze the model and train the whole network end-to-end. However, in
both papers, no comparison to the original method of Campanella <em>et
al.</em> was performed.</p>
<h1 id="materials">Materials</h1>
<p>We used the same dataset as Bulten <em>et al.</em><span
class="citation" data-cites="Bulten2020"><sup>9</sup></span>, we will
briefly reiterate the collection of the dataset here. We built our
dataset by retrospectively collecting biopsies and associated pathology
reports of patients. Subsequently, we divided the patients between
training, validation, and test set. As standard practice, we optimized
the model using the training set and assessed generalization using the
validation set during development. After development, we evaluated the
model on the test set. The dataset, except for the test set, is publicly
available as a Kaggle challenge at <a
href="https://www.kaggle.com/c/prostate-cancer-grade-assessment"
class="uri">https://www.kaggle.com/c/prostate-cancer-grade-assessment</a>.
An additional set, termed Olympus set, was used for evaluation with a
different scanner, originally extracted by Litjens et al<span
class="citation" data-cites="Litjens2016"><sup>12</sup></span>.</p>
<h2 id="data-collection">Data collection</h2>
<p>We retrieved pathologists reports of prostate biopsies for patients
with a suspicion of prostate cancer, dated between Jan 1, 2012, and Dec
31, 2017, from digital patient records at the Radboud University Medical
Center, excluding patients who underwent neoadjuvant or adjuvant
therapy. The local ethics review board waived the need for informed
consent (IRB approval 2016–2275).</p>
<p>After anonymization, we performed a text search on the anonymized
pathology reports to divide the biopsies into positive and negative
cases. Afterward, we divided the patient reports randomly into training,
validation, and test set. By stratifying the biopsies on the primary
Gleason score, we retrieved a comparable grade distribution in all sets.
From the multiple cross-sections which were available per patient, we
selected the standard hematoxylin-and-eosin-stained glass slide
containing the most aggressive or prevalent part of malignant tissue for
scanning.</p>
<p>We digitized the selected glass slides using a 3DHistech Pannoramic
Flash II 250 (3DHistech, Hungary) scanner at a pixel resolution of <span
class="math inline">\(0.24 \mu m\)</span>. Since each slide could
contain one to six unique biopsies, commonly with two consecutive
sections of the biopsies per slide, trained non-experts coarsely
outlined each biopsy, assigning each with either the reported Gleason
score, or labeling negative, based on the individual biopsy descriptions
in the pathology report.</p>
<p>We collected 1243 glass slides, containing 5759 biopsies sections.
After division, the training set consisted of 4712 biopsies, the
validation set of 497 biopsies, and the test set of 550 biopsies (Table
<a href="#tab:extset" data-reference-type="ref"
data-reference="tab:extset">1</a>, Fig. <a href="#fig:example"
data-reference-type="ref"
data-reference="fig:example">[fig:example]</a>). We extracted the
individual biopsies from the scanned slides at a pixel resolution of
<span class="math inline">\(0.96 \mu m\)</span>, visually approximately
equivalent to 100x total magnification (i.e., 10x microscope objective
with a standard 10x ocular lens). Subsequently, we trimmed the
whitespace around the tissue using a tissue-segmentation neural
network<span class="citation"
data-cites="Bandi2019a"><sup>30</sup></span>.</p>
<h2 id="reference-standard-test-set">Reference standard test set</h2>
<p>To determine a strong reference standard, three specialized
pathologists reviewed the slides in three rounds. In the first round,
each pathologist graded the biopsies independently. In the second round,
each biopsy for which no consensus was reached in the first round,
consensus was regraded by the pathologist whose score differed from the
other two, with the help of the pathologist’s first score and the two
anonymous Gleason scores of the other pathologists. In the third round,
the pathologists discussed the biopsies without consensus after round
two. In total 15 biopsies were discarded by the panel as they could not
be reliably graded, resulting in a total test set size of 535 biopsies.
See<span class="citation" data-cites="Bulten2020"><sup>9</sup></span>
for a complete overview of the grading protocol.</p>
<h2 id="smaller-subsampled-training-set">Smaller subsampled training
set</h2>
<p>To test our method with smaller datasets, we sampled 250 (5%) and 500
(10%) biopsies from the training set. Half of the cases in the new sets
were negatives. For the positive biopsies, we stratified on primary
Gleason grade and sampled equal amounts of each. Thus, we kept the
distribution of the positive biopsies equal over all the datasets. We
used the 5% (250 biopsies) and 10% (500 biopsies) datasets for training.
The validation- and test-sets were equal to the ones used in the
development of the model on the whole set.</p>
<div id="tab:extset">
<table>
<caption>Distribution of datasets used in the experiments, stratisfied
on primary Gleason pattern.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Dataset</strong></th>
<th style="text-align: left;"><strong>Total</strong></th>
<th style="text-align: left;"><strong>Negative</strong></th>
<th style="text-align: left;"><strong>3</strong></th>
<th style="text-align: left;"><strong>4</strong></th>
<th style="text-align: left;"><strong>5</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Training set</td>
<td style="text-align: left;">4712</td>
<td style="text-align: left;">16%</td>
<td style="text-align: left;">32%</td>
<td style="text-align: left;">45%</td>
<td style="text-align: left;">7%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Validation set</td>
<td style="text-align: left;">497</td>
<td style="text-align: left;">39%</td>
<td style="text-align: left;">23%</td>
<td style="text-align: left;">29%</td>
<td style="text-align: left;">9%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">10% set</td>
<td style="text-align: left;">500</td>
<td style="text-align: left;">50%</td>
<td style="text-align: left;">17%</td>
<td style="text-align: left;">17%</td>
<td style="text-align: left;">17%</td>
</tr>
<tr class="even">
<td style="text-align: left;">5% set</td>
<td style="text-align: left;">250</td>
<td style="text-align: left;">51%</td>
<td style="text-align: left;">16%</td>
<td style="text-align: left;">16%</td>
<td style="text-align: left;">16%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Test set</td>
<td style="text-align: left;">535</td>
<td style="text-align: left;">47%</td>
<td style="text-align: left;">25%</td>
<td style="text-align: left;">19%</td>
<td style="text-align: left;">9%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Olympus set</td>
<td style="text-align: left;">205</td>
<td style="text-align: left;">58%</td>
<td style="text-align: left;">25%</td>
<td style="text-align: left;">11%</td>
<td style="text-align: left;">4%</td>
</tr>
</tbody>
</table>
</div>
<h2 id="olympus-set">Olympus set</h2>
<p>For the Olympus set, we used the slides of Litjens <em>et al.</em>,
2016<span class="citation"
data-cites="Litjens2016"><sup>12</sup></span>. That set contained 255
glass slides, scanned using an Olympus VS120-S5 system (Olympus, Japan).
In comparison to the original paper, we used all biopsies on a negative
slide, instead of only one, resulting in 291 biopsies (Fig. <a
href="#fig:olympusexample" data-reference-type="ref"
data-reference="fig:olympusexample">[fig:olympusexample]</a>). Since
patients in this set were biopsied in 2012, there was a small overlap
with the primary dataset used in this paper. We excluded 86 biopsies
from 53 duplicate patients, resulting in a set of 205 biopsies.</p>
<h1 id="methods">Methods</h1>
<h2 id="end-to-end-streaming-model">End-to-end streaming model</h2>
<p>We trained a ResNet-34<span class="citation"
data-cites="He2016"><sup>31</sup></span> convolutional neural network.
Since the individual biopsy images differ in size, we padded or
center/cropped them to 16384<span
class="math inline">\(\times\)</span><!-- -->16384 input. 99% of our
dataset biopsies fitted within this input size. Since padding small
biopsies results in a lot of whitespace, we changed the final pooling
layer of ResNet-34 to a global max-pool layer.</p>
<p>For regularization, we used extensive data augmentation. To make
augmentation of these images feasible with reasonable memory usage and
speed, we used the open-source library VIPS<span class="citation"
data-cites="VIPS1996"><sup>32</sup></span>. Elastic random
transformation, color augmentation (hue, saturation, and brightness),
random horizontal and vertical flipping, and rotations were applied. We
normalized the images based on training dataset statistics.</p>
<p>We initialized the networks using ImageNet-trained weights. As an
optimizer, we used standard SGD (learning rate of <span
class="math inline">\(2e-4\)</span>) with momentum (0.9) and a
mini-batch size of 16 images. Because when using streaming, we do not
have a full image on the GPU, we cannot use batch normalization, thus we
froze the batch normalization mean and variance, using the
transfer-learned ImageNet running mean and variance. We randomly
oversampled negative cases to counter the imbalance in the dataset<span
class="citation" data-cites="Buda2018"><sup>33</sup></span>.</p>
<p>For the experiments with random weights, we initialized the networks
using He <em>et al.</em><span class="citation"
data-cites="He2015"><sup>34</sup></span>. We also used mixed precision
training<span class="citation"
data-cites="micikevicius_mixed"><sup>35</sup></span> to speed up
training since these networks needed more epochs to convergence.</p>
<div class="figure*">
<p><img src="chpt3_imgs/methods_overview_cropped.png" /></p>
</div>
<h3 id="streaming-cnn">Streaming CNN</h3>
<p>Most convolutional neural network architectures trained for a
classification task require more memory in the first layers than in the
latter because of the large feature maps. Our previously published
method termed ‘streaming’<span class="citation"
data-cites="Pinckaers2019"><sup>20</sup></span> circumvents these high
memory requirements in the first layers by performing the operations on
a tile-by-tile basis. This method is possible because CNNs use small
kernels; hence the result at any given location is only defined by a
small area of the input. This area is called the field-of-view. Since
the field-of-view at the beginning of a network is vastly smaller than
the full input image, we can use tiles (which have to be bigger than the
field-of-view) to perform the convolutions serially. Thereby only
requiring the amount of memory for the calculation on a single tile
instead of the whole input image. After streaming, we concatenate the
tile outputs to retrieve the complete intermediate feature map of the
last streamed layer. This complete feature map is equal to the feature
map we would get when training on a infinite-memory GPU.</p>
<p>During the forward pass of these memory-heavy first layers, we keep
the final layer output and remove the output of the other intermediate
layers, to save memory. We stream as many layers as needed until the
last streamed layer’s output can fit into GPU memory. This feature map
can subsequently be fed through the rest of the neural network at once,
resulting in the final output.</p>
<p>For the backward pass, we can use a similar implementation. The last
layers, until the last streamed layer, can be backpropagated as usual.
Then, we correctly tile the gradient of the last streamed layer’s
output. We use these gradient tiles for tile-by-tile backpropagation of
the streamed layers. Leveraging the input tile, we recalculate the first
layers’ intermediate feature maps with a forward pass (this is commonly
called gradient checkpointing<span class="citation"
data-cites="Chen2016a"><sup>36</sup></span>. With the recalculated
features and the gradient tile, we can finish the backpropagation for
the respective tile. We perform this for every tile. This way, we can
recover the gradients of all parameters, as would be the case if
training with the original input image. See Figure <a
href="#figure:streamingSGD" data-reference-type="ref"
data-reference="figure:streamingSGD">[figure:streamingSGD]</a> for a
graphical representation of the methods.</p>
<p>To train the ResNet-34, we streamed with a tile size of 2800<span
class="math inline">\(\times\)</span><!-- -->2800 (Fig. <a
href="#fig:memrelation" data-reference-type="ref"
data-reference="fig:memrelation">[fig:memrelation]</a>) over the first
28 layers of the network. After these layers, the whole feature map
(with dimensions 512<span
class="math inline">\(\times\)</span><!-- -->512<span
class="math inline">\(\times\)</span><!-- -->512) could fit into GPU
memory. It is possible to use the streaming implementation for more
layers of the network, however, to improve speed it is better to stream
until the feature map is just small enough. Finally, we fed the map
through the remaining six layers to calculate the final output.</p>
<p>For the experiments with random weights in mixed precision, due to
the decrease in memory usage, we could use a tile size of 3136<span
class="math inline">\(\times\)</span><!-- -->3136 to increase speed, and
decrease the number of streamed layers to the first 27.</p>
<h3 id="training-schedule">Training schedule</h3>
<p>In transfer learning, often the first layers are treated as a feature
extraction algorithm. After the feature extraction part, the second part
is trained for the specific task<span class="citation"
data-cites="TanSKZYL18"><sup>37</sup></span>. Since the domain of
histopathology differs significantly from the natural images in
ImageNet, we froze the first three (of the four) residual blocks of the
network (the first 27 layers) as feature extractor, only training the
last block for our task. This also has the benefit of training faster,
since we do not need to calculate gradients for the first layers. After
25 epochs, all the networks were stabilized and stopped improving the
validation loss, showing slightly lower train losses.</p>
<p>From these epochs, we picked a checkpoint with a low validation loss
to resume fine-tuning the whole network, unfreezing the weights of the
first three residual blocks. Due to the relatively small validation set,
the loss curve was less smooth than the training loss curve. To account
for a sporadic checkpoint with a low loss, we calculated a moving
average over five epochs. From these averages, we picked the window with
the lowest loss, taking the middle checkpoint of the averaging
window.</p>
<p>Starting from this checkpoint, we fine-tuned the whole network with a
learning rate of <span class="math inline">\(6e-5\)</span>. After
approximately 50 epochs, all the networks stopped improving. For
inference, we choose the checkpoints based on a moving average of five
epochs with the lowest validation set loss. We averaged the weights of
these checkpoints to improve generalization<span class="citation"
data-cites="Izmailov2018"><sup>38</sup></span>.</p>
<p>For the streaming experiments with random weights, we used the exact
same training schedule except for the learning rate. The loss would go
to infinity in the first few batches. When training from scratch, we
could not use the first layers as feature extractor. We fine-tuned the
whole network with a learning rate of <span
class="math inline">\(1e-5\)</span> requiring 100 epochs until the
validation loss did stabilized. We subsequently lowered the learning
rate to <span class="math inline">\(3e-6\)</span> for 200 epochs after
which the validation loss stopped improving.</p>
<p>The optimization and training procedure was fully conducted using the
validation set, the test set, and the Olympus set were untouched during
the development of the model.</p>
<h3 id="gradient-accumulation-and-parallelization">Gradient accumulation
and parallelization</h3>
<p>Gradient accumulation is a technique to do a forward and backward
pass on multiple images in series on the accelerator card, and averaging
the parameter gradients over those images. Only after averaging, we
perform a gradient descent step. Averaging the gradients over multiple
images in series results in effectively training a mini-batch of these
multiple images, while only requiring the memory for one image at a
time. We used gradient accumulation over multiple biopsies to achieve an
effective mini-batch size of 16 images.</p>
<p>We trained over multiple GPUs by splitting the mini-batch. For the
streaming experiments, we used four GPUs (either NVIDIA RTX 2080ti or
GTX 1080ti).</p>
<div class="figure*">
<p><img src="chpt3_imgs/auc_both.png" /></p>
</div>
<h2 id="multiple-instance-learning-model">Multiple-instance-learning
model</h2>
<p>As a baseline, we implemented the multiple-instance-learning method
as described in<span class="citation"
data-cites="Campanella2019"><sup>10</sup></span>.</p>
<p>This method divides the images into a grid of smaller patches with
the assumption that an individual patch could determine the image-level
label. The task is to find the most informative patch. In our binary
detection task, the most informative patch is determined by the patch
with the highest probability of tumor. If there is a patch with a high
probability of tumorous tissue, the whole biopsy is labeled
tumorous.</p>
<p>We train such a model, per epoch, in two phases. The first phase is
the inference phase, where we process all the patches of a biopsy,
thereby finding the patch with the highest probability. This patch gets
assigned the image-level label. Then, in the training phase, using only
patches with the highest probability (the top-1 patch), the model
parameters are optimized with a loss calculated on the patch probability
and the label.</p>
<p>We followed the implementation from Campanella <em>et al.</em><span
class="citation" data-cites="Campanella2019"><sup>10</sup></span>, but
tweaked it for our dataset sizes. We used standard SGD (learning rate of
<span class="math inline">\(1e-5\)</span>) with momentum (0.9) with a
mini-batch size of 16 images. We froze the BatchNormalization mean and
variance, due to the smaller mini-batch size and to keep the features
equal between the inference phase and the training phase. Equally, we
oversampled negative cases to counter the imbalance in the dataset,
instead of weighting<span class="citation"
data-cites="Buda2018"><sup>33</sup></span>.</p>
<p>We updated the whole model for 100 epochs when transfer learning, and
200 epochs when training from random weights. From these epochs, we
picked the checkpoint with the lowest loss using the same scheme as the
streaming model. Afterward, we trained for another 100 epochs with a
learning rate of <span class="math inline">\(3e-6\)</span>. The networks
trained from random initialization on the 10% and 5% required 300
epochs. We again choose the checkpoint based on the lowest validation
set loss, using a moving average of 5 epochs. We also used weight
averaging for these checkpoints.</p>
<p>For regularization, we used the same data augmentation as the
streaming model. We made sure that the same augmented patch was used in
the inferencing and training phase. We used ImageNet statistics to
normalize the patches.</p>
<h2 id="quantitative-evaluation">Quantitative evaluation</h2>
<p>The quantitative evaluation of both methods is performed using
receiver-operating characteristic (ROC) analysis. Specifically, we look
at the area under the ROC curve. To calculate a confidence interval, we
used bootstrapping. We sampled the number of the biopsies in the set,
with replacement, and calculated the area under the
receiver-operating-curve based on the new sample. Repeating this
procedure 10.000 times resulted in a distribution from which we
calculated the 95% confidence interval (2.5 and 97.5 percentile)</p>
<h2 id="qualitative-evaluation">Qualitative evaluation</h2>
<p>To assess the correlation of certain regions to the cancerous label,
we created heatmaps for both techniques. For MIL, we used the patch
probabilities. For streaming, we used sensitivity maps using
SmoothGrad<span class="citation"
data-cites="Smilkov2017"><sup>39</sup></span>. As implementation of
SmoothGrad, we averaged 25 sensitivity maps on Gaussian-noise-augmented
versions of a biopsy. We used a standard deviation of 5% of the
image-wide standard deviation for the Gaussian noise. As a comparison,
we show pixel-level segmentations from the model published in Bulten
<em>et al.</em><span class="citation"
data-cites="Bulten2020"><sup>9</sup></span> as well.</p>
<p>In addition, we did a thorough analysis of the false positives and
negatives of both the MIL and the streaming methods.</p>
<h1 id="experiments">Experiments</h1>
<p>We performed three experiments for both methods using three datasets.
One experiment on all the data, and two on subsampled training sets, the
10% (500 biopsies) and 5% (250 biopsies) datasets.</p>
<div id="tab:test_results">
<table>
<caption>Area under the receiver-operating-curve comparison between the
methods on the test set, <em>trained using transfer
learning</em>.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Dataset</strong></th>
<th style="text-align: left;"><strong>Method</strong></th>
<th style="text-align: left;"><strong>AUC</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Whole set</td>
<td style="text-align: left;">Streaming</td>
<td style="text-align: left;">0.992 (0.985–0.997)</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">MIL</td>
<td style="text-align: left;">0.990 (0.984–0.995)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">Bulten <em>et al.</em><span
class="citation" data-cites="Bulten2020"><sup>9</sup></span></td>
<td style="text-align: left;">0.990 (0.982–0.996)</td>
</tr>
<tr class="even">
<td style="text-align: left;">10% set</td>
<td style="text-align: left;">Streaming</td>
<td style="text-align: left;">0.982 (0.972–0.990)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">MIL</td>
<td style="text-align: left;">0.981 (0.970–0.990)</td>
</tr>
<tr class="even">
<td style="text-align: left;">5% set</td>
<td style="text-align: left;">Streaming</td>
<td style="text-align: left;">0.971 (0.960–0.982)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">MIL</td>
<td style="text-align: left;">0.965 (0.949–0.978)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Olympus set</td>
<td style="text-align: left;">Streaming</td>
<td style="text-align: left;">0.909 (0.863–0.949)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">MIL</td>
<td style="text-align: left;">0.799 (0.732–0.861)</td>
</tr>
</tbody>
</table>
</div>
<div id="tab:test_results_scratch">
<table>
<caption>Area under the receiver-operating-curve comparison between the
methods on the test set, <em>trained from random
initialization</em>.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Dataset</strong></th>
<th style="text-align: left;"><strong>Method</strong></th>
<th style="text-align: left;"><strong>AUC</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Whole set</td>
<td style="text-align: left;">Streaming</td>
<td style="text-align: left;">0.967 (0.952–0.980)</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">MIL</td>
<td style="text-align: left;">0.918 (0.894–0.941)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">10% set</td>
<td style="text-align: left;">Streaming</td>
<td style="text-align: left;">0.924 (0.900–0.945)</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">MIL</td>
<td style="text-align: left;">0.899 (0.871–0.924)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">5% set</td>
<td style="text-align: left;">Streaming</td>
<td style="text-align: left;">0.915 (0.889–0.939)</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">MIL</td>
<td style="text-align: left;">0.862 (0.831–0.892)</td>
</tr>
</tbody>
</table>
</div>
<p>On the whole dataset, the streaming model achieved an AUC of 0.992
(0.985–0.997) and the MIL model an AUC of 0.990 (0.984–0.995).
Interestingly, our models trained on the whole dataset reached similar
performance to previous work on this dataset<span class="citation"
data-cites="Bulten2020"><sup>9</sup></span>, which utilized a
segmentation network trained using dense annotations obtained in a
semi-supervised fashion.</p>
<p>For streaming, the performance on the smaller dataset sizes are
similar between the two. 5% dataset has an AUC of 0.971 (0.960–0.982)
for 5% and 0.982 (0.972–0.990) for 10% (Table <a
href="#tab:test_results" data-reference-type="ref"
data-reference="tab:test_results">2</a>). The models trained with more
data generalize better (Fig. <a href="#figure:ROCcomparison"
data-reference-type="ref"
data-reference="figure:ROCcomparison">[figure:ROCcomparison]</a>).</p>
<p>Also for multiple-instance learning there is a clear improvement
going from a model trained on the smallest dataset size, with an AUC of
0.965 (0.949–0.978), increasing to 0.981 (0.970–0.990) on the 10%
dataset.</p>
<p>There seems to be a trend that the MIL model performs slightly worse
(Fig. <a href="#figure:ROCcomparison" data-reference-type="ref"
data-reference="figure:ROCcomparison">[figure:ROCcomparison]</a>),
however, this difference falls within the confidence intervals.</p>
<p>In the experiments trained from random weights, there is a larger
separation between the methods, without overlap of the confidence
intervals. Streaming achieves an AUC of 0.967 (0.952–0.980) when using
the whole set (Table <a href="#tab:test_results_scratch"
data-reference-type="ref"
data-reference="tab:test_results_scratch">3</a>) in comparison to MIL
with 0.918 (0.894–0.941). For the 10% set using streaming also results
in higher metrics 0.924 (0.900–0.945) versus 0.899 (0.871–0.924).
Finally, the 5% set gets an AUC of 0.915 (0.889–0.939) for streaming and
0.862 (0.831–0.892) for MIL.</p>
<p>In general, the areas identified by MIL and streaming in the heatmaps
correspond well to the pixel-level segmentations from Bulten <em>et
al.</em>, showing that both methods pick up the relevant regions for
cancer identification (Figure <a href="#fig:heatmaps"
data-reference-type="ref"
data-reference="fig:heatmaps">[fig:heatmaps]</a>). Most errors of the
models seem to be due to normal epithelium mimicking tumorous glands in
the case for false positives, and the small size of some tumorous
regions as a possible reason for the false negatives. (Table <a
href="#tab:errors" data-reference-type="ref"
data-reference="tab:errors">4</a>)</p>
<p>For the Olympus set, existing of biopsies scanned by the Olympus
VS-system, there is a larger separation between the methods. Streaming
reaches an AUC of 0.909 (0.863–0.949), with MIL scoring 0.799
(0.732–0.861). For this dataset, MIL has 36 false negatives versus 20
for streaming, and 8 false positive versus 5 from streaming.</p>
<p>Identified by the MIL model.<img src="chpt3_imgs/FP_MIL_1.png" />
Identified by the streaming model.<img
src="chpt3_imgs/FP_e2e_1.png" /></p>
<p>Small tumorous glands mimicking vessels. Missed by both models.<img
src="chpt3_imgs/both_missed_FN.png" /> Very limited amount of tumor
(four glands), missed by the streaming network.<img
src="chpt3_imgs/e2e_FN.png" /></p>
<div id="tab:errors">
<table>
<caption>The predictions were manually judged and divided in the
following categories. False positives and negatives were selected at the
point of maximum accuracy in the ROC curve.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>False positives</strong></th>
<th style="text-align: left;"><strong>Streaming</strong> (5)</th>
<th style="text-align: left;"><strong>MIL</strong> (13)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Normal mimicking tumor</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">Inflammation</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Tissue artefacts</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">Bladder epithelium</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Colon epithelium</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>False negatives</strong></td>
<td style="text-align: left;"><strong>Streaming</strong> (13)</td>
<td style="text-align: left;"><strong>MIL</strong> (12)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Little amount of tumor</td>
<td style="text-align: left;">7</td>
<td style="text-align: left;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">Tissue artefacts</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Low-grade tumor</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">2</td>
</tr>
<tr class="even">
<td style="text-align: left;">Inflammation-like</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Unclear reason</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">2</td>
</tr>
</tbody>
</table>
</div>
<div class="figure*">
<p><img src="chpt3_imgs/saliency_maps_darker.png" /></p>
</div>
<div id="tab:times">
<table>
<caption>When fine-tuning only the last six of the ResNet-34 are
trained, all other layers are frozen. All metrics are in seconds.
Preprocessing times not shown, in our experiments they accounted for 8
seconds for new biopsies. Mixed precision streaming is relatively fast
due to bigger tile-size and one less layer to stream.</caption>
<thead>
<tr class="header">
<th></th>
<th style="text-align: left;"><strong>Training</strong></th>
<th style="text-align: left;"><strong>Fine-tuning</strong></th>
<th style="text-align: left;"><strong>Inferencing</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Full precision streaming</td>
<td style="text-align: left;">32.3 s</td>
<td style="text-align: left;">12.5 s</td>
<td style="text-align: left;">8.5 s</td>
</tr>
<tr class="even">
<td>Mixed precision streaming</td>
<td style="text-align: left;">17.2 s</td>
<td style="text-align: left;">3.6 s</td>
<td style="text-align: left;">3.5 s</td>
</tr>
<tr class="odd">
<td>MIL</td>
<td style="text-align: left;">0.5 s</td>
<td style="text-align: left;">n.a.</td>
<td style="text-align: left;">0.25 s</td>
</tr>
</tbody>
</table>
</div>
<h1 id="discussion-and-conclusions">Discussion and conclusions</h1>
<p>In this paper, we proposed using streaming<span class="citation"
data-cites="Pinckaers2019"><sup>20</sup></span> convolution neural
networks to directly train a state-of-the-art ResNet-34 architecture on
whole prostate biopsies with slide-level labels from pathology reports.
We are the first to train such high-resolution (268 megapixels) images
end-to-end, without further heuristics. Accomplishing this without the
streaming implementation would require a accelerator card with 2.4
terabyte of memory.</p>
<p>We showed it is possible to train a residual neural network with
biopsy level labels and reach similar performance to a popular
multiple-instance-learning (MIL) based method. Our models trained on the
whole dataset reached an AUC of 0.992 for streaming training, and 0.990
for MIL. In addition, we achieved equal performance to a method trained
on patch-based labels, with an AUC of 0.990<span class="citation"
data-cites="Bulten2020"><sup>9</sup></span> on the same dataset.
Although, it should be noted that Bulten <em>et al.</em> used
weakly-supervised labels, they used a cascade of models to go from
epithelium antibody-staining to semi-automatic pixel-level annotations,
to generate a model trained at the patch level.</p>
<p>Looking at the failure cases (Table <a href="#tab:errors"
data-reference-type="ref" data-reference="tab:errors">4</a>),
multiple-instance-learning suffers from interpreting normal glands as
tumorous (Fig. <a href="#fig:errors_pos" data-reference-type="ref"
data-reference="fig:errors_pos">[fig:errors_pos]</a> and <a
href="#fig:errors_neg" data-reference-type="ref"
data-reference="fig:errors_neg">[fig:errors_neg]</a>). We hypothesize
this is due to the lack of context, in all but three cases the
misclassification was due to one patch. For false negatives, both models
fail when there is a small amount of tumor, however the streaming model
seems to suffer more from this. A possible solution would be to
incorporate attention mechanisms into the network, allowing it to focus
to smaller parts of the biopsy.</p>
<p>To study the benefits of transfer learning, we trained the networks
from randomly initialized weights according to He <em>et al.</em><span
class="citation" data-cites="He2015"><sup>34</sup></span>. These
networks took longer to converge (approximately 3-4x more iterations
needed) and reached lower performances. In this case, MIL is less
capable of extracting relevant information from the patches and scores
worse than networks trained with streaming, scoring an AUC of 0.918
versus 0.959, respectively. We think training from random weights
introduced additional noise in the MIL-training process. Since some
biopsies contain cancerous tissue that only falls within a few patches,
ImageNet weights can provide a better starting point to find these
relevant patches during training. However, when training from random
initialization, the noise of the benign patches in a cancerous biopsy
may make it harder to learn. When possible, we advise the usage of
pretrained network to increase convergence speed and final
performance.</p>
<p>MIL performs weaker than the streaming network on the Olympus set,
with the main error being misclassifying 36 biopsies with tumor as
negative. The external dataset has other color characteristics due to
the different scanner used. Since both network have been trained with
the same data augmentation, MIL seems to benefit less from this
augmentation thus generalizing worse. The improvement seen in
generalization on the Olympus set and the trend of higher performance
overall suggest that streaming extracts more meaningful features from
the same dataset.</p>
<p>In this paper, we compared against a MIL implementation of Campanella
<em>et al.</em> In their MIL implementation, only the top-1 patch is
used for training per epoch. The method’s data efficiency is reliant on
how often different patches are selected in the first phase. Our results
on the smallest dataset sample (5%, 250 slides) hint towards reduced
data efficiency for MIL. However, the performance on the smaller
datasets was already close to optimal, suggesting effective use of the
transferred ImageNet-weights. Even though it is not the same test set as
in their original paper, this seems to suggest a better performance for
smaller datasets than Campanella <em>et al.</em> reported.
Hypothetically, this could be due to data augmentation, which they did
not use, and increased randomness with smaller mini-batch size in our
study.</p>
<p>For MIL, selecting different patches per image, every epoch, is
important to circumvent overfitting. We used lower minibatch-sizes, 16
vs 512, and learning rates, <span class="math inline">\(1e-5\)</span> vs
<span class="math inline">\(1e-4\)</span> as the original
implementation<span class="citation"
data-cites="Campanella2019"><sup>10</sup></span>. We saw increased
stability in training using smaller mini-batch sizes and learning rates,
especially for the smaller datasets, where the whole dataset would
otherwise fit in one mini-batch. Lower mini-batch sizes increased some
noise, thereby picking different patches per epoch.</p>
<p>The streaming implementation of convolutional neural networks is
computationally slower than our baseline. Mainly due to the number (121)
and overlap (~650 pixels) of the tiles during backpropagation. For
inference new slides, taking into account the preprocessing that needs
to happen (roughly 8 seconds for extracting patches or extracting the
whole biopsy), MIL takes half the time (8.25 seconds) compared to
streaming (16.5 seconds) (Table IV). The most significant difference
lies in the train speed, where for full precision, streaming is ~65
times slower than MIL (Table <a href="#tab:times"
data-reference-type="ref" data-reference="tab:times">5</a>). Streaming
did require half the number of epochs needed to converge, but the gap is
still large. However, an algorithm only needs to be trained once, and
the inference speed for both streaming and MIL is fast enough for use in
clinical practice.</p>
<p><img src="chpt3_imgs/timevsmemory_final.png" /></p>
<p>Improving the speed of the streaming implementation of convolutional
operations is of interest. In this work, we improved training speed by
first freezing the first layers of the neural network, not having to
calculate gradients. Using this training scheme in the
multiple-instance-learning baseline resulted in unstable training and
worse performance. Further research could focus on decreasing the number
of calculations needed by using variable input sizes<a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> or
lower-level implementations that ignore whitespace, such as sparse
implementations of convolutions<span class="citation"
data-cites="park_faster_2016"><sup>40</sup></span>.</p>
<p>Streaming training with high-resolution images opens up the
possibility to quickly gather large datasets with labels from pathology
reports to train convolutional neural networks. Although linking
individual biopsies to the pathology report is still a manual task, it
is more efficient than annotating the individual slides. However, some
pathology labs will manufacture one slide per biopsy and report
systematically on these individual biopsies. Training from a whole
slide, with multiple biopsies, is left for future research.</p>
<p>Since multiple-instance-learning, in the end, predicts the final
label on a single patch, tasks that require information from different
sites of the biopsy could be hard to engineer in this framework. For
example, in the Gleason grading scheme, the two most informative growth
patterns are reported. These patterns could lie on different parts of
the biopsy, outside of the field-of-view of a single patch. Also,
additional growth patterns could be present. The first reported growth
pattern of Gleason grading is the most prevalent. Since
multiple-instance-learning works patch-based, volumes that span larger
than one patch are not used for the prediction. Streaming allows for
training complex tasks, such as cancer grading, even with slide-level
labels.</p>
<p>Our heatmaps show that indeed the streaming model uses information
from multiple regions in the biopsy (Fig. <a href="#fig:heatmaps"
data-reference-type="ref"
data-reference="fig:heatmaps">[fig:heatmaps]</a>). Even though our model
is not trained on a patch-level, the sensitivity maps highlight similar
regions as the MIL method and the segmentation algorithm from Bulten
<em>et al.</em> Thus, interestingly, a modern convolutional neural
network, originally developed for tiny input sizes, can extract useful
information from 268 megapixel images.</p>
<p>Besides allowing the entire slide to inform predictions, streaming
training also has the advantage of being able to learn with hard or
impossible to annotate global information. For example, in the medical
domain, survival prediction can be of great interest. Future work could
be to predict survival from histopathology tissue directly. Reliably
annotating for this task can be difficult. Since streaming can find
patterns and features from the whole image using just the retrospective
patient prognosis, this method can be beneficial in automatically
finding new relevant biomarkers.</p>
<p>We provide source code of the streaming pipeline at GitHub<a
href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>. We tried to make it easy to use
with other datasets. Additionally to methods used in this paper, we
added mixed precision support for even more memory efficient and faster
training.</p>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-Krizhevsky2009" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div
class="csl-right-inline">Krizhevsky A. <em><span class="nocase">Learning
Multiple Layers of Features from Tiny Images</span></em>. University of
Toronto; 2009.</div>
</div>
<div id="ref-Russakovsky2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div
class="csl-right-inline">Russakovsky O, Deng J, Su H, et al.
<span>ImageNet Large Scale Visual Recognition Challenge</span>.
<em>International Journal of Computer Vision</em>. 2015;115(3):211-252.
doi:<a
href="https://doi.org/10.1007/s11263-015-0816-y">10.1007/s11263-015-0816-y</a></div>
</div>
<div id="ref-Sun2017RevisitingUE" class="csl-entry" role="listitem">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Sun
C, Shrivastava A, Singh S, Gupta A. <span class="nocase">Revisiting
Unreasonable Effectiveness of Data in Deep Learning Era</span>. <em>2017
IEEE International Conference on Computer Vision (ICCV)</em>. Published
online 2017:843-852.</div>
</div>
<div id="ref-Ozkan2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">4. </div><div
class="csl-right-inline">Ozkan TA, Eruyar AT, Cebeci OO, Memik O, Ozcan
L, Kuskonmaz I. <span class="nocase">Interobserver variability in
Gleason histological grading of prostate cancer</span>. <em>Scandinavian
Journal of Urology</em>. 2016;50(6):420-424. doi:<a
href="https://doi.org/10.1080/21681805.2016.1206619">10.1080/21681805.2016.1206619</a></div>
</div>
<div id="ref-Epstein2010" class="csl-entry" role="listitem">
<div class="csl-left-margin">5. </div><div
class="csl-right-inline">Epstein JI. <span class="nocase">An Update of
the Gleason Grading System</span>. <em>Journal of Urology</em>.
2010;183(2):433-440. doi:<a
href="https://doi.org/10.1016/j.juro.2009.10.046">10.1016/j.juro.2009.10.046</a></div>
</div>
<div id="ref-Fine2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Fine
SW, Amin MB, Berney DM, et al. <span class="nocase">A contemporary
update on pathology reporting for prostate cancer: Biopsy and radical
prostatectomy specimens</span>. <em>European Urology</em>.
2012;62(1):20-39. doi:<a
href="https://doi.org/10.1016/j.eururo.2012.02.055">10.1016/j.eururo.2012.02.055</a></div>
</div>
<div id="ref-10.1093/jnci/djp278" class="csl-entry" role="listitem">
<div class="csl-left-margin">7. </div><div
class="csl-right-inline">Welch HG, Albertsen PC. <span
class="nocase">Prostate Cancer Diagnosis and Treatment After the
Introduction of Prostate-Specific Antigen Screening: 1986–2005</span>.
<em>JNCI: Journal of the National Cancer Institute</em>.
2009;101(19):1325-1329. doi:<a
href="https://doi.org/10.1093/jnci/djp278">10.1093/jnci/djp278</a></div>
</div>
<div id="ref-Wilson2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">8. </div><div
class="csl-right-inline">Wilson ML, Fleming KA, Kuti MA, Looi LM, Lago
N, Ru K. <span class="nocase">Access to pathology and laboratory
medicine services: a crucial gap.</span> <em>Lancet (London,
England)</em>. 2018;391(10133):1927-1938. doi:<a
href="https://doi.org/10.1016/S0140-6736(18)30458-6">10.1016/S0140-6736(18)30458-6</a></div>
</div>
<div id="ref-Bulten2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">9. </div><div
class="csl-right-inline">Bulten W, Pinckaers H, Boven H van, et al.
<span class="nocase">Automated deep-learning system for Gleason grading
of prostate cancer using biopsies: a diagnostic study</span>. <em>The
Lancet Oncology</em>. 2020;21(2):233-241. doi:<a
href="https://doi.org/10.1016/S1470-2045(19)30739-9">10.1016/S1470-2045(19)30739-9</a></div>
</div>
<div id="ref-Campanella2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">10. </div><div
class="csl-right-inline">Campanella G, Hanna MG, Geneslaw L, et al.
<span class="nocase">Clinical-grade computational pathology using weakly
supervised deep learning on whole slide images</span>. <em>Nature
Medicine</em>. 2019;25(8):1301-1309. doi:<a
href="https://doi.org/10.1038/s41591-019-0508-1">10.1038/s41591-019-0508-1</a></div>
</div>
<div id="ref-Nagpal2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">11. </div><div
class="csl-right-inline">Nagpal K, Foote D, Liu Y, et al. <span
class="nocase">Development and validation of a deep learning algorithm
for improving Gleason scoring of prostate cancer</span>. <em>npj Digital
Medicine</em>. 2019;2(1):48. doi:<a
href="https://doi.org/10.1038/s41746-019-0112-2">10.1038/s41746-019-0112-2</a></div>
</div>
<div id="ref-Litjens2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">12. </div><div
class="csl-right-inline">Litjens G, Sánchez CI, Timofeeva N, et al.
<span class="nocase">Deep learning as a tool for increased accuracy and
efficiency of histopathological diagnosis</span>. <em>Scientific
Reports</em>. 2016;6(1):26286. doi:<a
href="https://doi.org/10.1038/srep26286">10.1038/srep26286</a></div>
</div>
<div id="ref-Arvaniti2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">13. </div><div
class="csl-right-inline">Arvaniti E, Fricker KS, Moret M, et al. <span
class="nocase">Automated Gleason grading of prostate cancer tissue
microarrays via deep learning</span>. <em>Scientific Reports</em>.
2018;8(1):12054. doi:<a
href="https://doi.org/10.1038/s41598-018-30535-1">10.1038/s41598-018-30535-1</a></div>
</div>
<div id="ref-Lucas2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">14. </div><div
class="csl-right-inline">Lucas M, Jansen I, Savci-Heijink CD, et al.
<span class="nocase">Deep learning for automatic Gleason pattern
classification for grade group determination of prostate
biopsies</span>. <em>Virchows Archiv</em>. 2019;475(1):77-83. doi:<a
href="https://doi.org/10.1007/s00428-019-02577-x">10.1007/s00428-019-02577-x</a></div>
</div>
<div id="ref-Strom2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">15. </div><div
class="csl-right-inline">Ström P, Kartasalo K, Olsson H, et al. <span
class="nocase">Artificial intelligence for diagnosis and grading of
prostate cancer in biopsies: a population-based, diagnostic
study</span>. <em>The Lancet Oncology</em>. 2020;21(2):222-232. doi:<a
href="https://doi.org/10.1016/S1470-2045(19)30738-7">10.1016/S1470-2045(19)30738-7</a></div>
</div>
<div id="ref-Courtiol2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">16. </div><div
class="csl-right-inline">Courtiol P, Tramel EW, Sanselme M, Wainrib G.
<span class="nocase">Classification and Disease Localization in
Histopathology Using Only Global Labels: A Weakly-Supervised
Approach</span>. <em>arXiv preprint</em>. 2018;1802.02212. <a
href="http://arxiv.org/abs/1802.02212">http://arxiv.org/abs/1802.02212</a></div>
</div>
<div id="ref-Ilse2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">17. </div><div
class="csl-right-inline">Ilse M, Tomczak JM, Welling M. <span
class="nocase">Attention-based Deep Multiple Instance Learning</span>.
<em>arXiv preprint</em>. 2018;1802.04712. <a
href="http://arxiv.org/abs/1802.04712">http://arxiv.org/abs/1802.04712</a></div>
</div>
<div id="ref-Amores2013" class="csl-entry" role="listitem">
<div class="csl-left-margin">18. </div><div
class="csl-right-inline">Amores J. <span class="nocase">Multiple
instance classification: Review, taxonomy and comparative study</span>.
<em>Artificial Intelligence</em>. 2013;201:81-105. doi:<a
href="https://doi.org/10.1016/j.artint.2013.06.003">10.1016/j.artint.2013.06.003</a></div>
</div>
<div id="ref-VanderLaak2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">19. </div><div
class="csl-right-inline">Laak J van der, Ciompi F, Litjens G. <span
class="nocase">No pixel-level annotations needed</span>. <em>Nature
Biomedical Engineering</em>. 2019;3(11):855-856. doi:<a
href="https://doi.org/10.1038/s41551-019-0472-6">10.1038/s41551-019-0472-6</a></div>
</div>
<div id="ref-Pinckaers2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">20. </div><div
class="csl-right-inline">Pinckaers H, Ginneken B van, Litjens G. <span
class="nocase">Streaming convolutional neural networks for end-to-end
learning with multi-megapixel images</span>. <em>arXiv preprint</em>.
2019;1911.04432. <a
href="https://arxiv.org/abs/1911.04432">https://arxiv.org/abs/1911.04432</a></div>
</div>
<div id="ref-Swiderska-Chadaj2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">21. </div><div
class="csl-right-inline">Swiderska-Chadaj Z, Bel T de, Blanchet L, et
al. <span class="nocase">Impact of rescanning and normalization on
convolutional neural network performance in multi-center, whole-slide
classification of prostate cancer</span>. <em>Scientific Reports</em>.
2020;10(1):14398. doi:<a
href="https://doi.org/10.1038/s41598-020-71420-0">10.1038/s41598-020-71420-0</a></div>
</div>
<div id="ref-Gertych2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">22. </div><div
class="csl-right-inline">Gertych A, Ing N, Ma Z, et al. <span
class="nocase">Machine learning approaches to analyze histological
images of tissues from radical prostatectomies</span>. <em>Computerized
Medical Imaging and Graphics</em>. 2015;46:197-208. doi:<a
href="https://doi.org/10.1016/j.compmedimag.2015.08.002">10.1016/j.compmedimag.2015.08.002</a></div>
</div>
<div id="ref-nguyen2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">23. </div><div
class="csl-right-inline">Nguyen TH, Sridharan S, Macias V, et al. <span
class="nocase">Automatic Gleason grading of prostate cancer using
quantitative phase imaging and machine learning</span>. <em>Journal of
biomedical optics</em>. 2017;22(3):36015.</div>
</div>
<div id="ref-Naik2007" class="csl-entry" role="listitem">
<div class="csl-left-margin">24. </div><div
class="csl-right-inline">Naik S, Doyle S, Feldman M, Tomaszewski J,
Madabhushi A. <span class="nocase">Gland Segmentation and Computerized
Gleason Grading of Prostate Histology by Integrating Low- , High-level
and Domain Specific Information.</span> In: <em>Proceedings of 2nd
Workshop on Microsopic Image Analysis with Applications in
Biology</em>.; 2007:1-8.</div>
</div>
<div id="ref-Ianni2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">25. </div><div
class="csl-right-inline">Ianni JD, Soans RE, Sankarapandian S, et al.
<span class="nocase">Tailored for Real-World: A Whole Slide Image
Classification System Validated on Uncurated Multi-Site Data Emulating
the Prospective Pathology Workload</span>. <em>Scientific Reports</em>.
2020;10(1):3217. doi:<a
href="https://doi.org/10.1038/s41598-020-59985-2">10.1038/s41598-020-59985-2</a></div>
</div>
<div id="ref-lu2020data" class="csl-entry" role="listitem">
<div class="csl-left-margin">26. </div><div class="csl-right-inline">Lu
MY, Williamson DFK, Chen TY, Chen RJ, Barbieri M, Mahmood F. <span
class="nocase">Data Efficient and Weakly Supervised Computational
Pathology on Whole Slide Images</span>. <em>arXiv preprint</em>.
2020;2004.09666. <a
href="https://arxiv.org/abs/2004.09666">https://arxiv.org/abs/2004.09666</a></div>
</div>
<div id="ref-Li2019a" class="csl-entry" role="listitem">
<div class="csl-left-margin">27. </div><div class="csl-right-inline">Li
J, Li W, Gertych A, Knudsen BS, Speier W, Arnold CW. <span
class="nocase">An attention-based multi-resolution model for prostate
whole slide imageclassification and localization</span>. <em>arXiv
preprint</em>. 2019;1905.13208. <a
href="http://arxiv.org/abs/1905.13208">http://arxiv.org/abs/1905.13208</a></div>
</div>
<div id="ref-Caner2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">28. </div><div
class="csl-right-inline">Mercan C, Aksoy S, Mercan E, Shapiro LG, Weaver
DL, Elmore JG. <span class="nocase">Multi-Instance Multi-Label Learning
for Multi-Class Classification of Whole Slide Breast Histopathology
Images</span>. <em>IEEE Transactions on Medical Imaging</em>.
2018;37(1):316-325. doi:<a
href="https://doi.org/10.1109/TMI.2017.2758580">10.1109/TMI.2017.2758580</a></div>
</div>
<div id="ref-Tellez2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">29. </div><div
class="csl-right-inline">Tellez D, Litjens G, Laak J van der, Ciompi F.
<span class="nocase">Neural Image Compression for Gigapixel
Histopathology Image Analysis</span>. <em>IEEE Transactions on Pattern
Analysis and Machine Intelligence</em>. in press. doi:<a
href="https://doi.org/10.1109/TPAMI.2019.2936841">10.1109/TPAMI.2019.2936841</a></div>
</div>
<div id="ref-Bandi2019a" class="csl-entry" role="listitem">
<div class="csl-left-margin">30. </div><div
class="csl-right-inline">Bándi P, Balkenhol M, Ginneken B van, Laak J
van der, Litjens G. <span class="nocase">Resolution-agnostic tissue
segmentation in whole-slide histopathology images with convolutional
neural networks.</span> <em>PeerJ</em>. 2019;7:e8242. doi:<a
href="https://doi.org/10.7717/peerj.8242">10.7717/peerj.8242</a></div>
</div>
<div id="ref-He2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">31. </div><div class="csl-right-inline">He
K, Zhang X, Ren S, Sun J. <span class="nocase">Deep residual learning
for image recognition</span>. In: <em>Proceedings of the IEEE Computer
Society Conference on Computer Vision and Pattern Recognition</em>. Vol
2016. IEEE Computer Society; 2016:770-778. doi:<a
href="https://doi.org/10.1109/CVPR.2016.90">10.1109/CVPR.2016.90</a></div>
</div>
<div id="ref-VIPS1996" class="csl-entry" role="listitem">
<div class="csl-left-margin">32. </div><div
class="csl-right-inline">Cupitt J, Martinez K. <span
class="nocase">VIPS: An imaging processing system for large
images</span>. <em>Proceedings of SPIE - The International Society for
Optical Engineering</em>. 1996;1663:19-28. doi:<a
href="https://doi.org/10.1117/12.233043">10.1117/12.233043</a></div>
</div>
<div id="ref-Buda2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">33. </div><div
class="csl-right-inline">Buda M, Maki A, Mazurowski MA. <span
class="nocase">A systematic study of the class imbalance problem in
convolutional neural networks.</span> <em>Neural networks : the official
journal of the International Neural Network Society</em>.
2018;106:249-259. doi:<a
href="https://doi.org/10.1016/j.neunet.2018.07.011">10.1016/j.neunet.2018.07.011</a></div>
</div>
<div id="ref-He2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">34. </div><div class="csl-right-inline">He
K, Zhang X, Ren S, Sun J. <span class="nocase">Delving Deep into
Rectifiers: Surpassing Human-Level Performance on ImageNet
Classification</span>. In: <em>Proceedings of the 2015 IEEE
International Conference on Computer Vision (ICCV)</em>. IEEE Computer
Society; 2015:1026-1034. doi:<a
href="https://doi.org/10.1109/ICCV.2015.123">10.1109/ICCV.2015.123</a></div>
</div>
<div id="ref-micikevicius_mixed" class="csl-entry" role="listitem">
<div class="csl-left-margin">35. </div><div
class="csl-right-inline">Micikevicius P, Narang S, Alben J, et al.
<span>Mixed Precision Training</span>. <em>arXiv preprint</em>.
2017;1710.03740. <a
href="http://arxiv.org/abs/1710.03740">http://arxiv.org/abs/1710.03740</a></div>
</div>
<div id="ref-Chen2016a" class="csl-entry" role="listitem">
<div class="csl-left-margin">36. </div><div
class="csl-right-inline">Chen T, Xu B, Zhang C, Guestrin C. <span
class="nocase">Training Deep Nets with Sublinear Memory Cost</span>.
<em>arXiv preprint</em>. 2016;1604.06174. <a
href="http://arxiv.org/abs/1604.06174">http://arxiv.org/abs/1604.06174</a></div>
</div>
<div id="ref-TanSKZYL18" class="csl-entry" role="listitem">
<div class="csl-left-margin">37. </div><div class="csl-right-inline">Tan
C, Sun F, Kong T, Zhang W, Yang C, Liu C. <span class="nocase">A Survey
on Deep Transfer Learning</span>. In: <em>Artificial Neural Networks and
Machine Learning - <span>{</span>ICANN<span>}</span> 2018 - 27th
International Conference on Artificial Neural Networks, Rhodes, Greece,
October 4-7, 2018, Proceedings, Part
<span>{</span>III<span>}</span></em>. Vol 11141. Lecture notes in
computer science. Springer; 2018:270-279. doi:<a
href="https://doi.org/10.1007/978-3-030-01424-7_27">10.1007/978-3-030-01424-7_27</a></div>
</div>
<div id="ref-Izmailov2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">38. </div><div
class="csl-right-inline">Izmailov P, Podoprikhin D, Garipov T, Vetrov D,
Wilson AG. <span class="nocase">Averaging Weights Leads to Wider Optima
and Better Generalization</span>. <em>arXiv preprint</em>.
2018;1803.05407. <a
href="http://arxiv.org/abs/1803.05407">http://arxiv.org/abs/1803.05407</a></div>
</div>
<div id="ref-Smilkov2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">39. </div><div
class="csl-right-inline">Smilkov D, Thorat N, Kim B, Viégas FB,
Wattenberg M. <span class="nocase">SmoothGrad: removing noise by adding
noise</span>. In: <em>ICML Workshop on Visualization for Deep
Learning</em>.; 2017. <a
href="http://arxiv.org/abs/1706.03825">http://arxiv.org/abs/1706.03825</a></div>
</div>
<div id="ref-park_faster_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">40. </div><div
class="csl-right-inline">Park J, Li S, Wen W, et al. <span
class="nocase">Faster CNNs with Direct Sparse Convolutions and Guided
Pruning</span>. Published online November 2016. <a
href="https://openreview.net/forum?id=rJPcZ3txx">https://openreview.net/forum?id=rJPcZ3txx</a></div>
</div>
</div>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Manuscript submitted on June 6, 2020. This work was
supported by the Dutch Cancer Society under Grant KUN 2015-7970.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Hans Pinckaers, Wouter Bulten, Jeroen van der Laak and
Geert Litjens are with the Computational Pathology Group, Department of
Pathology, Radboud Institute for Health Sciences, Radboud University
Medical Center, Nijmegen, The Netherlands; E-mail: {hans.pinckaers,
wouter.bulten, jeroen.vanderlaak, geert.litjens}@radboudumc.nl.
Additionally, Jeroen van der Laak is with the Center for Medical Image
Science and Visualization, Linköping University, Linköping, Sweden.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>An example implementation of this can be found in the
open source repository.<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a
href="https://github.com/DIAGNijmegen/pathology-streaming-pipeline"
class="uri">https://github.com/DIAGNijmegen/pathology-streaming-pipeline</a><a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</body>
</html>
