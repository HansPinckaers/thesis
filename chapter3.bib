Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Caner2018,
abstract = {Digital pathology has entered a new era with the availability of whole slide scanners that create the high-resolution images of full biopsy slides. Consequently, the uncertainty regarding the correspondence between the image areas and the diagnostic labels assigned by pathologists at the slide level, and the need for identifying regions that belong to multiple classes with different clinical significances have emerged as two new challenges. However, generalizability of the state-of-the-art algorithms, whose accuracies were reported on carefully selected regions of interest (ROIs) for the binary benign versus cancer classification, to these multi-class learning and localization problems is currently unknown. This paper presents our potential solutions to these challenges by exploiting the viewing records of pathologists and their slide-level annotations in weakly supervised learning scenarios. First, we extract candidate ROIs from the logs of pathologists' image screenings based on different behaviors, such as zooming, panning, and fixation. Then, we model each slide with a bag of instances represented by the candidate ROIs and a set of class labels extracted from the pathology forms. Finally, we use four different multi-instance multi-label learning algorithms for both slide-level and ROI-level predictions of diagnostic categories in whole slide breast histopathology images. Slide-level evaluation using 5-class and 14-class settings showed average precision values up to 81{\%} and 69{\%}, respectively, under different weakly labeled learning scenarios. ROI-level predictions showed that the classifier could successfully perform multi-class localization and classification within whole slide images that were selected to include the full range of challenging diagnostic categories.},
author = {Mercan, C and Aksoy, S and Mercan, E and Shapiro, L G and Weaver, D L and Elmore, J G},
doi = {10.1109/TMI.2017.2758580},
issn = {1558-254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Algorithms,Breast,Breast Neoplasms,Cancer,Computer-Assisted,Digital pathology,Electronic mail,Histocytochemistry,Humans,Image Interpretation,Image analysis,Lesions,Pathology,ROI-level predictions,Training,binary benign classification,biomedical optical imaging,breast histopathology,cancer,cancer classification,diagnostic labels,digital pathology,fixation,full biopsy slides,high-resolution images,image classification,image screenings,learning (artificial intelligence),localization,medical image processing,multi-class classification,multi-instance multilabel learning,multiclass classification,multiclass learning,multiclass localization,panning,region of interest detection,slide-level annotations,tumours,weakly supervised learning,weakly-labeled learning,whole slide breast histopathology images,whole slide imaging,whole slide scanners,zooming},
month = {jan},
number = {1},
pages = {316--325},
title = {{Multi-Instance Multi-Label Learning for Multi-Class Classification of Whole Slide Breast Histopathology Images}},
volume = {37},
year = {2018}
}
@techreport{Krizhevsky2009,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels.},
author = {Krizhevsky, Alex},
file = {::},
institution = {University of Toronto},
title = {{Learning Multiple Layers of Features from Tiny Images}},
year = {2009}
}
@article{Singh2017,
abstract = {Glandular structural features are important for the tumor pathologist in the assessment of cancer malignancy of prostate tissue slides. The varying shapes and sizes of glands combined with the tedious manual observation task can result in inaccurate assessment. There are also discrepancies and low-level agreement among pathologists, especially in cases of Gleason pattern 3 and pattern 4 prostate adenocarcinoma. An automated gland segmentation system can highlight various glandular shapes and structures for further analysis by the pathologist. These objective highlighted patterns can help reduce the assessment variability. We propose an automated gland segmentation system. Forty-three hematoxylin and eosin-stained images were acquired from prostate cancer tissue slides and were manually annotated for gland, lumen, periacinar retraction clefting, and stroma regions. Our automated gland segmentation system was trained using these manual annotations. It identifies these regions using a combination of pixel and object-level classifiers by incorporating local and spatial information for consolidating pixel-level classification results into object-level segmentation. Experimental results show that our method outperforms various texture and gland structure-based gland segmentation algorithms in the literature. Our method has good performance and can be a promising tool to help decrease interobserver variability among pathologists.},
author = {Singh, Malay and Kalaw, Emarene Mationg and Giron, Danilo Medina and Chong, Kian-Tai and Tan, Chew Lim and Lee, Hwee Kuan},
doi = {10.1117/1.JMI.4.2.027501},
issn = {2329-4302},
journal = {Journal of medical imaging},
keywords = {AdaBoost,digital pathology,gland segmentation,prostate cancer,support vector machine},
number = {2},
pages = {27501},
pmid = {28653016},
title = {{Gland segmentation in prostate histopathological images.}},
url = {http://medicalimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JMI.4.2.027501{\%}0Ahttp://www.ncbi.nlm.nih.gov/pubmed/28653016{\%}0Ahttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5479152},
volume = {4},
year = {2017}
}
@article{DBLP:journals/corr/GomezRUG17,
archivePrefix = {arXiv},
arxivId = {1707.04585},
author = {Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
eprint = {1707.04585},
journal = {arXiv},
title = {{The Reversible Residual Network: Backpropagation Without Storing Activations}},
url = {http://arxiv.org/abs/1707.04585},
volume = {abs/1707.0},
year = {2017}
}
@article{Zagoruyko,
abstract = {Deep neural networks with skip-connections, such as ResNet, show excellent perfor-mance in various image classification benchmarks. It is though observed that the initial motivation behind them -training deeper networks -does not actually hold true, and the benefits come from increased capacity, rather than from depth. Motivated by this, and inspired from ResNet, we propose a simple Dirac weight parameterization, which allows us to train very deep plain networks without skip-connections, and achieve nearly the same performance. This parameterization has a minor computational cost at training time and no cost at all at inference. We're able to achieve 95.5{\%} accuracy on CIFAR-10 with 34-layer deep plain network, surpassing 1001-layer deep ResNet, and approaching Wide ResNet. Our parameterization also mostly eliminates the need of careful initializa-tion in residual and non-residual networks. The code and models for our experiments are available at https://github.com/szagoruyko/diracnets},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
file = {::},
title = {{DiracNets: Training Very Deep Neural Networks Without Skip-Connections}}
}
@article{Berland2006,
abstract = {Automatic differentiation is introduced to an audience with basic mathematical prerequisites. Numerical examples show the defiency of divided difference, and dual numbers serve to introduce the algebra being one example of how to derive automatic differentiation. An example with forward mode is given first, and source transformation and operator overloading is illustrated. Then reverse mode is briefly sketched, followed by some discussion. (45 minute talk)},
author = {Berland, H{\aa}vard},
file = {::},
title = {{Automatic differentiation}},
year = {2006}
}
@article{Chen2019,
abstract = {Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.},
archivePrefix = {arXiv},
arxivId = {1903.12174},
author = {Chen, Xinlei and Girshick, Ross and He, Kaiming and Doll{\'{a}}r, Piotr},
eprint = {1903.12174},
file = {::},
month = {mar},
title = {{TensorMask: A Foundation for Dense Object Segmentation}},
url = {http://arxiv.org/abs/1903.12174},
year = {2019}
}
@article{Sun2017RevisitingUE,
author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
pages = {843--852},
title = {{Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}},
year = {2017}
}
@misc{Kaggle2017,
title = {{Data Science Bowl 2017 | Kaggle}},
url = {https://www.kaggle.com/c/data-science-bowl-2017},
urldate = {2019-10-21}
}
@article{7486259,
abstract = {Deep-learning (DL) algorithms, which learn the representative and discriminative features in a hierarchical manner from the data, have recently become a hotspot in the machine-learning area and have been introduced into the geoscience and remote sensing (RS) community for RS big data analysis. Considering the low-level features (e.g., spectral and texture) as the bottom level, the output feature representation from the top level of the network can be directly fed into a subsequent classifier for pixel-based classification. As a matter of fact, by carefully addressing the practical demands in RS applications and designing the input?output levels of the whole network, we have found that DL is actually everywhere in RS data analysis: from the traditional topics of image preprocessing, pixel-based classification, and target recognition, to the recent challenging tasks of high-level semantic feature extraction and RS scene understanding.In this technical tutorial, a general framework of DL for RS data is provided, and the state-of-the-art DL methods in RS are regarded as special cases of input-output data combined with various deep networks and tuning tricks. Although extensive experimental results confirm the excellent performance of the DL-based algorithms in RS big data analysis, even more exciting prospects can be expected for DL in RS. Key bottlenecks and potential directions are also indicated in this article, guiding further research into DL for RS data.},
author = {Zhang, L and Zhang, L and Du, B},
doi = {10.1109/MGRS.2016.2540798},
issn = {2168-6831},
journal = {IEEE Geoscience and Remote Sensing Magazine},
keywords = {data analysis;data mining;feature extraction;geoph},
number = {2},
pages = {22--40},
title = {{Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art}},
volume = {4},
year = {2016}
}
@article{Arvaniti2018,
abstract = {The Gleason grading system remains the most powerful prognostic predictor for patients with prostate cancer since the 1960s. Its application requires highly-trained pathologists, is tedious and yet suffers from limited inter-pathologist reproducibility, especially for the intermediate Gleason score 7. Automated annotation procedures constitute a viable solution to remedy these limitations. In this study, we present a deep learning approach for automated Gleason grading of prostate cancer tissue microarrays with Hematoxylin and Eosin (H{\&}E) staining. Our system was trained using detailed Gleason annotations on a discovery cohort of 641 patients and was then evaluated on an independent test cohort of 245 patients annotated by two pathologists. On the test cohort, the inter-annotator agreements between the model and each pathologist, quantified via Cohen's quadratic kappa statistic, were 0.75 and 0.71 respectively, comparable with the inter-pathologist agreement (kappa = 0.71). Furthermore, the model's Gleason score assignments achieved pathology expert-level stratification of patients into prognostically distinct groups, on the basis of disease-specific survival data available for the test cohort. Overall, our study shows promising results regarding the applicability of deep learning-based solutions towards more objective and reproducible prostate cancer grading, especially for cases with heterogeneous Gleason patterns.},
author = {Arvaniti, Eirini and Fricker, Kim S. and Moret, Michael and Rupp, Niels and Hermanns, Thomas and Fankhauser, Christian and Wey, Norbert and Wild, Peter J. and R{\"{u}}schoff, Jan H. and Claassen, Manfred},
doi = {10.1038/s41598-018-30535-1},
issn = {2045-2322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {12054},
title = {{Automated Gleason grading of prostate cancer tissue microarrays via deep learning}},
url = {http://www.nature.com/articles/s41598-018-30535-1},
volume = {8},
year = {2018}
}
@article{He2018,
abstract = {We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10{\%} of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of `pre-training and fine-tuning' in computer vision.},
archivePrefix = {arXiv},
arxivId = {1811.08883},
author = {He, Kaiming and Girshick, Ross and Doll{\'{a}}r, Piotr},
eprint = {1811.08883},
file = {::},
month = {nov},
title = {{Rethinking ImageNet Pre-training}},
url = {http://arxiv.org/abs/1811.08883},
year = {2018}
}
@inproceedings{Cohen2016,
abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
archivePrefix = {arXiv},
arxivId = {1602.07576},
author = {Cohen, Taco S. and Welling, Max},
booktitle = {33rd International Conference on Machine Learning, ICML 2016},
eprint = {1602.07576},
file = {::},
isbn = {9781510829008},
month = {feb},
pages = {4375--4386},
title = {{Group equivariant convolutional networks}},
url = {http://arxiv.org/abs/1602.07576},
volume = {6},
year = {2016}
}
@misc{Su2017,
abstract = {Recent research has revealed that the output of Deep neural networks(DNN) is not continuous and very sensitive to tiny perturbation on the input vectors and accordingly several methods have been proposed for crafting effective perturbation against the networks. In this paper, we propose a novel method for optically calculating extremely small adversarial perturbation (few-pixels attack), based on differential evolution. It requires much less adversarial information and works with a broader classes of DNN models. The results show that 73.8{\$}\backslash{\%}{\$} of the test images can be crafted to adversarial images with modification just on one pixel with 98.7{\$}\backslash{\%}{\$} confidence on average. In addition, it is known that investigating the robustness problem of DNN can bring critical clues for understanding the geometrical features of the DNN decision map in high dimensional input space. The results of conducting few-pixels attack contribute quantitative measurements and analysis to the geometrical understanding from a different perspective compared to previous works.},
archivePrefix = {arXiv},
arxivId = {1710.08864},
author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
eprint = {1710.08864},
title = {{One pixel attack for fooling deep neural networks}},
url = {http://arxiv.org/abs/1710.08864},
year = {2017}
}
@inproceedings{He2015,
address = {Washington, DC, USA},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.123},
isbn = {978-1-4673-8391-2},
pages = {1026--1034},
publisher = {IEEE Computer Society},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
url = {http://dx.doi.org/10.1109/ICCV.2015.123},
year = {2015}
}
@inproceedings{Smilkov2017,
archivePrefix = {arXiv},
arxivId = {1706.03825},
author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'{e}}gas, Fernanda B and Wattenberg, Martin},
booktitle = {ICML workshop on Visualization for deep learning},
eprint = {1706.03825},
title = {{SmoothGrad: removing noise by adding noise}},
url = {http://arxiv.org/abs/1706.03825},
year = {2017}
}
@article{Courtiol,
abstract = {Analysis of histopathology slides is a critical step for many diagnoses, and in particular in oncology where it defines the gold standard. In the case of digital histopathological analysis, highly trained pathologists must review vast whole-slide-images of extreme digital resolution (100, 000 2 pixels) across multiple zoom levels in order to locate abnormal regions of cells, or in some cases single cells, out of millions. The application of deep learning to this problem is hampered not only by small sample sizes, as typical datasets contain only a few hundred samples, but also by the generation of ground-truth localized annotations for training inter-pretable classification and segmentation models. We propose a method for disease localization in the context of weakly supervised learning, where only image-level labels are available during training. Even without pixel-level annotations, we are able to demonstrate performance comparable with models trained with strong an-notations on the Camelyon-16 lymph node metastases detection challenge. We accomplish this through the use of pre-trained deep convolutional networks, fea-ture embedding, as well as learning via top instances and negative evidence, a multiple instance learning technique from the field of semantic segmentation and object detection.},
author = {Courtiol, Pierre and Tramel, Eric W and Sanselme, Marc and Wainrib, Gilles},
file = {::},
title = {{CLASSIFICATION AND DISEASE LOCALIZATION IN HISTOPATHOLOGY USING ONLY GLOBAL LABELS: A WEAKLY-SUPERVISED APPROACH}}
}
@article{DeVries2017,
abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56{\%}, 15.20{\%}, and 1.30{\%} test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
archivePrefix = {arXiv},
arxivId = {1708.04552},
author = {DeVries, Terrance and Taylor, Graham W.},
eprint = {1708.04552},
file = {::},
journal = {CoRR},
month = {aug},
title = {{Improved Regularization of Convolutional Neural Networks with Cutout}},
url = {http://arxiv.org/abs/1708.04552},
volume = {abs/1708.0},
year = {2017}
}
@inproceedings{Li2019,
abstract = {Brain image segmentation is used for visualizing and quantifying anatomical structures of the brain. We present an automated ap-proach using 2D deep residual dilated networks which captures rich context information of different tissues for the segmentation of eight brain structures. The proposed system was evaluated in the MICCAI Brain Segmentation Challenge and ranked 9th out of 22 teams. We further compared the method with traditional U-Net using leave-one-subject-out cross-validation setting on the public dataset. Experimental results shows that the proposed method outperforms traditional U-Net (i.e. 80.9{\%} vs 78.3{\%} in averaged Dice score, 4.35mm vs 11.59mm in averaged robust Hausdorff distance) and is computationally efficient.},
archivePrefix = {arXiv},
arxivId = {1811.04312},
author = {Li, Hongwei and Zhygallo, Andrii and Menze, Bjoern},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-11723-8_39},
eprint = {1811.04312},
file = {::},
isbn = {9783030117221},
issn = {16113349},
keywords = {Brain structure segmentation,Deep learning},
month = {nov},
pages = {385--393},
title = {{Automatic brain structures segmentation using deep residual dilated U-Net}},
url = {http://arxiv.org/abs/1811.04312},
volume = {11383 LNCS},
year = {2019}
}
@incollection{Sudre2017,
author = {Sudre, Carole H. and Li, Wenqi and Vercauteren, Tom and Ourselin, Sebastien and {Jorge Cardoso}, M.},
doi = {10.1007/978-3-319-67558-9_28},
file = {::},
month = {sep},
pages = {240--248},
publisher = {Springer, Cham},
title = {{Generalised Dice Overlap as a Deep Learning Loss Function for Highly Unbalanced Segmentations}},
url = {http://link.springer.com/10.1007/978-3-319-67558-9{\_}28},
year = {2017}
}
@article{10.1093/jnci/djp278,
abstract = {Although there is uncertainty about the effect of prostate-specific antigen (PSA) screening on the rate of prostate cancer death, there is little uncertainty about its effect on the rate of prostate cancer diagnosis. Systematic estimates of the number of men affected, however, to our knowledge, do not exist.We obtained data on age-specific incidence and initial course of therapy from the National Cancer Institute's Surveillance, Epidemiology, and End Results program. We then used age-specific male population estimates from the US Census to determine the excess (or deficit) in the number of men diagnosed and treated in each year after 1986—the year before PSA screening was introduced.Overall incidence of prostate cancer rose rapidly after 1986, peaked in 1992, and then declined, albeit to levels considerably higher than those in 1986. Overall incidence, however, obscured distinct age-specific patterns: The relative incidence rate (2005 relative to 1986) was 0.56 in men aged 80 years and older, 1.09 in men aged 70–79 years, 1.91 in men aged 60–69 years, 3.64 in men aged 50–59 years, and 7.23 in men younger than 50 years. Since 1986, an estimated additional 1 305 600 men were diagnosed with prostate cancer, 1 004 800 of whom were definitively treated for the disease. Using the most optimistic assumption about the benefit of screening—that the entire decline in prostate cancer mortality observed during this period is attributable to this additional diagnosis—we estimated that, for each man who experienced the presumed benefit, more than 20 had to be diagnosed with prostate cancer.The introduction of PSA screening has resulted in more than 1 million additional men being diagnosed and treated for prostate cancer in the United States. The growth is particularly dramatic for younger men. Given the considerable time that has passed since PSA screening began, most of this excess incidence must represent overdiagnosis.},
author = {Welch, H Gilbert and Albertsen, Peter C},
doi = {10.1093/jnci/djp278},
issn = {0027-8874},
journal = {JNCI: Journal of the National Cancer Institute},
number = {19},
pages = {1325--1329},
title = {{Prostate Cancer Diagnosis and Treatment After the Introduction of Prostate-Specific Antigen Screening: 1986–2005}},
url = {https://doi.org/10.1093/jnci/djp278},
volume = {101},
year = {2009}
}
@techreport{Shang,
abstract = {Deep Residual Networks (ResNets) have recently achieved state-of-the-art results on many challenging computer vision tasks. In this work we analyze the role of Batch Normal-ization (BatchNorm) layers on ResNets in the hope of improving the current architecture and better incorporating other normalization techniques, such as Normalization Propagation (NormProp), into ResNets. Firstly, we verify that BatchNorm helps distribute representation learning to residual blocks at all layers, as opposed to a plain ResNet without BatchNorm where learning happens mostly in the latter part of the network. We also observe that BatchNorm well regularizes Con-catenated ReLU (CReLU) activation scheme on ResNets, whose magnitude of activation grows by preserving both positive and negative responses when going deeper into the network. Secondly, we investigate the use of NormProp as a replacement for BatchNorm in ResNets. Though Norm-Prop theoretically attains the same effect as BatchNorm on generic convolutional neural networks, the identity mapping of ResNets invalidates its theoretical promise and NormProp exhibits a significant performance drop when naively applied. To bridge the gap between BatchNorm and NormProp in ResNets, we propose a simple modification to NormProp and employ the CReLU activation scheme. We experiment on visual object recognition benchmark datasets such as CIFAR-10/100 and ImageNet and demonstrate that 1) the modified NormProp performs better than the original NormProp but is still not comparable to BatchNorm and 2) CReLU improves the performance of ResNets with or without normalizations.},
author = {Shang, Wenling and Facebook, Justin Chiu and Sohn, Kihyuk},
file = {::},
title = {{Exploring Normalization in Deep Residual Networks with Concatenated Rectified Linear Units}},
url = {www.aaai.org}
}
@inproceedings{Chen2016,
abstract = {The morphology of glands has been used routinely by pathologists to assess the malignancy degree of adenocarcinomas. Accurate segmentation of glands from histology images is a crucial step to obtain reliable morphological statistics for quantitative diagnosis. In this paper, we proposed an efficient deep contour-aware network (DCAN) to solve this challenging problem under a unified multi-task learning framework. In the proposed network, multi-level contextual features from the hierarchical architecture are explored with auxiliary supervision for accurate gland segmentation. When incorporated with multi-task regularization during the training, the discriminative capability of intermediate features can be further improved. Moreover, our network can not only output accurate probability maps of glands, but also depict clear contours simultaneously for separating clustered objects, which further boosts the gland segmentation performance. This unified framework can be efficient when applied to large-scale histopathological data without resorting to additional steps to generate contours based on low-level cues for post-separating. Our method won the 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams, surpassing all the other methods by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1604.02677},
author = {Chen, Hao and Qi, Xiaojuan and Yu, Lequan and Heng, Pheng Ann},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.273},
eprint = {1604.02677},
file = {::},
isbn = {9781467388504},
issn = {10636919},
month = {apr},
pages = {2487--2496},
title = {{DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation}},
url = {http://arxiv.org/abs/1604.02677},
volume = {2016-Decem},
year = {2016}
}
@article{Rescigno2000,
abstract = {The Area Under the Curve (AUC) is proportional to the fraction absorbed only if the clearance is constant and the concentration uniform; in all other cases the bioavailability cannot be determined by comparing AUCs.},
author = {Rescigno, A},
institution = {rescigno@earthlink.net},
journal = {Pharmacological research the official journal of the Italian Pharmacological Society},
number = {6},
pages = {539--540},
pmid = {11058405},
title = {{Area under the curve and bioavailability.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11058405},
volume = {42},
year = {2000}
}
@article{Morren2009,
author = {Morren, Mattijn and Dulmen, Sandra and Ouwerkerk, Jessika and Bensing, Jozien},
journal = {European Journal of Pain},
month = {apr},
pages = {354--365},
title = {{Compliance with momentary pain measurement using electronic diaries: A systematic review}},
url = {http://readerific.com/exported/?a=0da420 papers3://publication/uuid/C4A3E8BA-6C89-4937-89C9-F403C2762C4C},
volume = {13},
year = {2009}
}
@inproceedings{BacMajMaoCotMcA20,
author = {Bachlechner, Thomas and {Majumder Bodhisattwa Prasad Mao}, Huanru Henry and Cottrell, Garrison W and McAuley, Julian},
booktitle = {arXiv},
title = {{ReZero is All You Need: Fast Convergence at Large Depth}},
url = {https://arxiv.org/abs/2003.04887},
year = {2020}
}
@article{Green2006,
abstract = {Concern has been raised about the lack of participant compliance in diary studies that use paper-and-pencil as opposed to electronic formats. Three studies explored the magnitude of compliance problems and their effects on data quality. Study 1 used random signals to elicit diary reports and found close matches to self-reported completion times, matches that could not plausibly have been fabricated. Studies 2 and 3 examined the psychometric and statistical equivalence of data obtained with paper versus electronic formats. With minor exceptions, both methods yielded data that were equivalent psychometrically and in patterns of findings. These results serve to at least partially mollify concern about the validity of paper diary methods.},
address = {Department of Psychology, New York University, New York, NY 10003, USA. asg228@nyu.edu},
author = {Green, Amie S and Rafaeli, Eshkol and Bolger, Niall and Shrout, Patrick E and Reis, Harry T},
journal = {Psychological methods},
month = {mar},
number = {1},
pages = {87--105},
publisher = {American Psychological Association},
title = {{Paper or plastic? Data equivalence in paper and electronic diaries.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.11.1.87 papers3://publication/doi/10.1037/1082-989X.11.1.87},
volume = {11},
year = {2006}
}
@article{Veta2019,
abstract = {Tumor proliferation is an important biomarker indicative of the prognosis of breast cancer patients. Assessment of tumor proliferation in a clinical setting is a highly subjective and labor-intensive task. Previous efforts to automate tumor proliferation assessment by image analysis only focused on mitosis detection in predefined tumor regions. However, in a real-world scenario, automatic mitosis detection should be performed in whole-slide images (WSIs) and an automatic method should be able to produce a tumor proliferation score given a WSI as input. To address this, we organized the TUmor Proliferation Assessment Challenge 2016 (TUPAC16) on prediction of tumor proliferation scores from WSIs. The challenge dataset consisted of 500 training and 321 testing breast cancer histopathology WSIs. In order to ensure fair and independent evaluation, only the ground truth for the training dataset was provided to the challenge participants. The first task of the challenge was to predict mitotic scores, i.e., to reproduce the manual method of assessing tumor proliferation by a pathologist. The second task was to predict the gene expression based PAM50 proliferation scores from the WSI. The best performing automatic method for the first task achieved a quadratic-weighted Cohen's kappa score of $\kappa$ = 0.567, 95{\%} CI [0.464, 0.671] between the predicted scores and the ground truth. For the second task, the predictions of the top method had a Spearman's correlation coefficient of r = 0.617, 95{\%} CI [0.581 0.651] with the ground truth. This was the first comparison study that investigated tumor proliferation assessment from WSIs. The achieved results are promising given the difficulty of the tasks and weakly-labeled nature of the ground truth. However, further research is needed to improve the practical utility of image analysis methods for this task.},
author = {Veta, Mitko and Heng, Yujing J. and Stathonikos, Nikolas and Bejnordi, Babak Ehteshami and Beca, Francisco and Wollmann, Thomas and Rohr, Karl and Shah, Manan A. and Wang, Dayong and Rousson, Mikael and Hedlund, Martin and Tellez, David and Ciompi, Francesco and Zerhouni, Erwan and Lanyi, David and Viana, Matheus and Kovalev, Vassili and Liauchuk, Vitali and Phoulady, Hady Ahmady and Qaiser, Talha and Graham, Simon and Rajpoot, Nasir and Sj{\"{o}}blom, Erik and Molin, Jesper and Paeng, Kyunghyun and Hwang, Sangheum and Park, Sunggyun and Jia, Zhipeng and Chang, Eric I.Chao and Xu, Yan and Beck, Andrew H. and van Diest, Paul J. and Pluim, Josien P.W.},
doi = {10.1016/j.media.2019.02.012},
file = {::},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Breast cancer,Cancer prognostication,Deep learning,Tumor proliferation},
month = {may},
pages = {111--121},
publisher = {Elsevier B.V.},
title = {{Predicting breast tumor proliferation from whole-slide images: The TUPAC16 challenge}},
volume = {54},
year = {2019}
}
@article{Nagpal2019,
abstract = {For prostate cancer patients, the Gleason score is one of the most important prognostic factors, potentially determining treatment independent of the stage. However, Gleason scoring is based on subjective microscopic examination of tumor morphology and suffers from poor reproducibility. Here we present a deep learning system (DLS) for Gleason scoring whole-slide images of prostatectomies. Our system was developed using 112 million pathologist-annotated image patches from 1226 slides, and evaluated on an independent validation dataset of 331 slides. Compared to a reference standard provided by genitourinary pathology experts, the mean accuracy among 29 general pathologists was 0.61 on the validation set. The DLS achieved a significantly higher diagnostic accuracy of 0.70 (p = 0.002) and trended towards better patient risk stratification in correlations to clinical follow-up data. Our approach could improve the accuracy of Gleason scoring and subsequent therapy decisions, particularly where specialist expertise is unavailable. The DLS also goes beyond the current Gleason system to more finely characterize and quantitate tumor morphology, providing opportunities for refinement of the Gleason system itself.},
author = {Nagpal, Kunal and Foote, Davis and Liu, Yun and Chen, Po-Hsuan Cameron and Wulczyn, Ellery and Tan, Fraser and Olson, Niels and Smith, Jenny L. and Mohtashamian, Arash and Wren, James H. and Corrado, Greg S. and MacDonald, Robert and Peng, Lily H. and Amin, Mahul B. and Evans, Andrew J. and Sangoi, Ankur R. and Mermel, Craig H. and Hipp, Jason D. and Stumpe, Martin C.},
doi = {10.1038/s41746-019-0112-2},
issn = {2398-6352},
journal = {npj Digital Medicine},
month = {dec},
number = {1},
pages = {48},
title = {{Development and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer}},
url = {http://www.nature.com/articles/s41746-019-0112-2},
volume = {2},
year = {2019}
}
@inproceedings{Shelhamer2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
isbn = {9781467369640},
issn = {01628828},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
year = {2015}
}
@techreport{Tellez,
abstract = {We present Neural Image Compression (NIC), a method to reduce the size of gigapixel images by mapping them to a compact latent space using neural networks. We show that this compression allows us to train convolutional neural networks on histopathology whole-slide images end-to-end using weak image-level labels.},
archivePrefix = {arXiv},
arxivId = {1811.02840v1},
author = {Tellez, David and Litjens, Geert and {Van Der Laak}, Jeroen},
eprint = {1811.02840v1},
file = {::},
keywords = {Index Terms-Gigapixel image analysis,convolutional neural networks,representation learning},
title = {{Neural Image Compression for Gigapixel Histopathology Image Analysis}},
url = {https://arxiv.org/pdf/1811.02840.pdf}
}
@inproceedings{Mnih2014,
address = {Cambridge, MA, USA},
author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2204--2212},
publisher = {MIT Press},
title = {{Recurrent Models of Visual Attention}},
url = {http://dl.acm.org/citation.cfm?id=2969033.2969073},
year = {2014}
}
@article{Bandi2019a,
abstract = {Modern pathology diagnostics is being driven toward large scale digitization of  microscopic tissue sections. A prerequisite for its safe implementation is the guarantee that all tissue present on a glass slide can also be found back in the digital image. Whole-slide scanners perform a tissue segmentation in a low resolution overview image to prevent inefficient high-resolution scanning of empty background areas. However, currently applied algorithms can fail in detecting all tissue regions. In this study, we developed convolutional neural networks to distinguish tissue from background. We collected 100 whole-slide images of 10 tissue samples-staining categories from five medical centers for development and testing. Additionally, eight more images of eight unfamiliar categories were collected for testing only. We compared our fully-convolutional neural networks to three traditional methods on a range of resolution levels using Dice score and sensitivity. We also tested whether a single neural network can perform equivalently to multiple networks, each specialized in a single resolution. Overall, our solutions outperformed the traditional methods on all the tested resolutions. The resolution-agnostic network achieved average Dice scores between 0.97 and 0.98 across the tested resolution levels, only 0.0069 less than the resolution-specific networks. Finally, its excellent generalization performance was demonstrated by achieving averages of 0.98 Dice score and 0.97 sensitivity on the eight unfamiliar images. A future study should test this network prospectively.},
author = {B{\'{a}}ndi, P{\'{e}}ter and Balkenhol, Maschenka and van Ginneken, Bram and van der Laak, Jeroen and Litjens, Geert},
doi = {10.7717/peerj.8242},
issn = {2167-8359 (Print)},
journal = {PeerJ},
language = {eng},
pages = {e8242},
pmid = {31871843},
title = {{Resolution-agnostic tissue segmentation in whole-slide histopathology images with  convolutional neural networks.}},
volume = {7},
year = {2019}
}
@article{Allsbrook2001,
abstract = {Gleason grading is now the most widely used grading system for prostatic carcinoma in the United States. However, there are only a few studies of the interobserver reproducibility of this system, and no extensive study of interobserver reproducibility among a large number of experienced urologic pathologists exists. Forty-six needle biopsies containing prostatic carcinoma were assigned Gleason scores by 10 urologic pathologists. The overall weighted kappa coefficient $\kappa$w for Gleason score for each of the urologic pathologists compared with each of the remaining urologic pathologists ranged from 0.56 to 0.70, all but one being at least 0.60 (substantial agreement). The overall kappa coefficient $\kappa$ for each pathologist compared with the others for Gleason score groups 2-4, 5-6, 7, and 8-10 ranged from 0.47 to 0.64 (moderate-substantial agreement), only one less than 0.50. At least 70{\%} of the urologic pathologists agreed on the Gleason grade group (2-4, 5-6, 7, 8-10) in 38 ("consensus" cases) of the 46 cases. The 8 "nonconsensus" cases included low-grade tumors, tumors with small cribriform proliferations, and tumors whose histology was on the border between Gleason patterns. Interobserver reproducibility of Gleason grading among urologic pathologists is in an acceptable range. Copyright {\textcopyright} 2001 by W.B. Saunders Company.},
author = {Allsbrook, William C. and Mangold, Kathy A. and Johnson, Maribeth H. and Lane, Roger B. and Lane, Cynthia G. and Amin, Mahul B. and Bostwick, David G. and Humphrey, Peter A. and Jones, Edward C. and Reuter, Victor E. and Sakr, Wael and Sesterhenn, Isabell A. and Troncoso, Patricia and Wheeler, Thomas M. and Epstein, Jonathan I.},
doi = {10.1053/hupa.2001.21134},
issn = {00468177},
journal = {Human Pathology},
keywords = {Gleason grading,Grading,Interobserver reproducibility,Prostatic adenocarcinoma,Prostatic carcinoma,Prostatic neoplasms},
pmid = {11172298},
title = {{Interobserver reproducibility of Gleason grading of prostatic carcinoma: Urologic pathologists}},
year = {2001}
}
@article{Devaux2012,
abstract = {BACKGROUND: Treatment adherence has been recognized as an important issue in the management of chronic diseases such as psoriasis. OBJECTIVE: The aim of this work was to analyse data about topical treatment adherence in psoriasis. METHODS: Systematic literature review (62 references) between 1980 and 2011 (database: PubMed, Embase and Cochrane; Mesh keywords: Patient Compliance [Mesh] OR Medication Adherence [Mesh] AND Psoriasis [Mesh]; limits: date of publication {\textgreater}1980, humans subjects, written in French or English, aged {\textgreater}/= 19 years). Two parameters were evaluated: (i) the ratio of number of product applications performed vs. number of applications expected according to physician recommendations, (ii) the ratio of amount of product used vs. amount of product prescribed. RESULTS: A total of 22 studies were selected. Nine studies reported on the frequency of topical treatment application in a real world setting. Five studies showed a frequency of applications varying between 50{\%} and 60{\%} of those expected. Because of the high variability in medication adherence assessment methods, the data could not be combined. Twelve articles reported on the frequency of topical treatment application in randomized controlled trials with adherence varying between 55{\%} and 100{\%}. Concerning the amount of product use, four studies showed patients applied between 35{\%} and 72{\%} of the recommended dose during a treatment period of 14 days to 8 weeks. The most frequently mentioned reasons for non-adherence to topical treatment were low efficacy, time consumption and poor cosmetic characteristics of topical agents. Patients experiencing adherence issues were significant younger, were men, had younger age at onset of psoriasis and had a higher self-assessed severity. To improve adherence, the following strategies were suggested: to give patients information about psoriasis, to recognize social impact, to give written instructions for use such as a care plan, to explain side effects of topical therapies, to choose treatment and its cosmetic properties in agreement with the patient. CONCLUSIONS: Literature data about topical treatment adherence are heterogeneous and scarce. They confirm the limited topical treatment adherence in psoriasis in real life, much lower than what is reported in randomized controlled trials. Therapeutic education and clear instructions on the use of topical agents are necessary to improve adherence. Studies are needed to identify predictors of limited adherence and to identify interventions improving adherence to topical medications in psoriasis.},
address = {Dermatology Department, Paul Sabatier University, Toulouse, France. devaux.s@chu-toulouse.fr},
author = {Devaux, S and Castela, A and Archier, E and Gallini, A and Joly, P and Misery, L and Aractingi, S and Aubin, F and Bachelez, H and Cribier, B and Jullien, D and {Le Maitre}, M and Richard, M-A and Ortonne, J-P and Paul, C},
journal = {Journal of the European Academy of Dermatology and Venereology : JEADV},
month = {may},
pages = {61--67},
title = {{Adherence to topical treatment in psoriasis: a systematic literature review.}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=22512682{\&}retmode=ref{\&}cmd=prlinks papers3://publication/doi/10.1111/j.1468-3083.2012.04525.x},
volume = {26 Suppl 3},
year = {2012}
}
@inproceedings{Chollet2017,
abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
author = {Chollet, F},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.195},
keywords = {Biological neural networks,Computer architecture,Convolutional codes,Correlation,Google,ImageNet dataset,Inception V3,Inception module,Xception,deep convolutional neural network architecture,deep learning,depthwise separable convolution operation,feedforward neural nets,image classification,image classification dataset,learning (artificial intelligence),neural net architecture,pointwise convolution,regular convolution},
pages = {1800--1807},
title = {{Xception: Deep Learning with Depthwise Separable Convolutions}},
year = {2017}
}
@article{Fine2012,
abstract = {Context: The diagnosis of and reporting parameters for prostate cancer (PCa) have evolved over time, yet they remain key components in predicting clinical outcomes. Objective: Update pathology reporting standards for PCa. Evidence acquisition: A thorough literature review was performed for articles discussing PCa handling, grading, staging, and reporting published as of September 15, 2011. Electronic articles published ahead of print were also considered. Proceedings of recent international conferences addressing these areas were extensively reviewed. Evidence synthesis: Two main areas of reporting were examined: (1) prostatic needle biopsy, including handling, contemporary Gleason grading, extent of involvement, and high-risk lesions/precursors and (2) radical prostatectomy (RP), including sectioning, multifocality, Gleason grading, staging of organ-confined and extraprostatic disease, lymph node involvement, tumor volume, and lymphovascular invasion. For each category, consensus views, controversial areas, and clinical import were reviewed. Conclusions: Modern prostate needle biopsy and RP reports are extremely detailed so as to maximize clinical utility. Accurate diagnosis of cancer-specific features requires up-to-date knowledge of grading, quantitation, and staging criteria. While some areas remain controversial, efforts to codify existing knowledge have had a significant impact on pathology practice. {\textcopyright}2012 European Association of Urology.},
author = {Fine, Samson W and Amin, Mahul B and Berney, Daniel M and Bjartell, Anders and Egevad, Lars and Epstein, Jonathan I and Humphrey, Peter A and Magi-Galluzzi, Christina and Montironi, Rodolfo and Stief, Christian},
doi = {10.1016/j.eururo.2012.02.055},
isbn = {1873-7560 (Electronic){\$}\backslash{\$}r0302-2838 (Linking)},
issn = {03022838},
journal = {European Urology},
keywords = {Gleason grading,Needle biopsy,Prostate cancer,Radical prostatectomy,Reporting,Staging},
number = {1},
pages = {20--39},
pmid = {22421083},
title = {{A contemporary update on pathology reporting for prostate cancer: Biopsy and radical prostatectomy specimens}},
volume = {62},
year = {2012}
}
@article{doi:10.1001/jama.2017.14585,
author = {Bejnordi, B Ehteshami and Veta, M and van Diest et al, P Johannes},
doi = {10.1001/jama.2017.14585},
journal = {JAMA},
number = {22},
pages = {2199--2210},
title = {{Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer}},
url = {+ http://dx.doi.org/10.1001/jama.2017.14585},
volume = {318},
year = {2017}
}
@article{Quellec2017,
author = {Quellec, Gwenole and Cazuguel, Guy and Cochener, Beatrice and Lamard, Mathieu},
doi = {10.1109/RBME.2017.2651164},
file = {::},
issn = {1937-3333},
journal = {IEEE Reviews in Biomedical Engineering},
pages = {213--234},
title = {{Multiple-Instance Learning for Medical Image and Video Analysis}},
url = {http://ieeexplore.ieee.org/document/7812612/},
volume = {10},
year = {2017}
}
@phdthesis{Laurent2014,
annote = {Depthwise separable convolutions},
author = {Laurent, Sifre},
title = {{Rigid-motion scattering for image classification.}},
year = {2014}
}
@inproceedings{Zagoruyko2016a,
abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
archivePrefix = {arXiv},
arxivId = {1605.07146},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
booktitle = {Procedings of the British Machine Vision Conference 2016},
doi = {10.5244/C.30.87},
eprint = {1605.07146},
file = {::},
isbn = {1-901725-59-6},
month = {may},
pages = {87.1--87.12},
publisher = {British Machine Vision Association},
title = {{Wide Residual Networks}},
url = {http://arxiv.org/abs/1605.07146 http://www.bmva.org/bmvc/2016/papers/paper087/index.html},
year = {2016}
}
@misc{AndrejKarpathy,
author = {{Andrej Karpathy}},
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/}
}
@article{Carroll2004a,
abstract = {INTRODUCTION: Medication nonadherence is common throughout medicine, and research into this area is increasing; however, knowledge about topical medication adherence is limited. METHODS: A total of 30 patients were enrolled in a clinical trial for psoriasis and followed up for 8 weeks using 3 methods of adherence monitoring: electronic monitoring caps; medication logs; and medication usage by weight. RESULTS: Adherence rates calculated from the medication logs and medication weights were consistently higher than those of the electronic monitors (P {\textless}.05). Electronically measured adherence rates declined from 84.6{\%} to 51{\%} during the 8-week study (P {\textless}.0001). Female sex and increasing age by 1 year predicted improved adherence of 5{\%} and 0.8{\%}, respectively (P {\textless}.0001). The number of treatment gaps increased from the first half to the last half of the study, and weekend days were overrepresented in treatment gaps. CONCLUSION: Medication logs and weights do not ensure medication adherence to topical therapy. Electronic monitoring allows a more precise method of adherence measurement.},
address = {Department of Dermatology, Wake Forest University Health Sciences, Winston-Salem, NC 27104, USA.},
author = {Carroll, Christie L and Feldman, Steven R and Camacho, Fabian T and Manuel, Janeen C and Balkrishnan, Rajesh},
journal = {Journal of the American Academy of Dermatology},
month = {aug},
number = {2},
pages = {212--216},
title = {{Adherence to topical therapy decreases during the course of an 8-week psoriasis clinical trial: commonly used methods of measuring adherence to topical therapy overestimate actual use.}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=15280839{\&}retmode=ref{\&}cmd=prlinks papers3://publication/uuid/2C78966B-73ED-4A97-809D-AA4BD0379D4A},
volume = {51},
year = {2004}
}
@misc{qiao2019weight,
archivePrefix = {arXiv},
arxivId = {cs.CV/1903.10520},
author = {Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
eprint = {1903.10520},
primaryClass = {cs.CV},
title = {{Weight Standardization}},
year = {2019}
}
@inproceedings{Folle2019,
abstract = {Tissue loss in the hippocampi has been heavily correlated with the progression of Alzheimer's Disease (AD). The shape and structure of the hippocampus are important factors in terms of early AD diagnosis and prognosis by clinicians. However, manual segmentation of such subcortical structures in MR studies is a challenging and subjective task. In this paper, we investigate variants of the well known 3D U-Net, a type of convolution neural network (CNN) for semantic segmentation tasks. We propose an alternative form of the 3D U-Net, which uses dilated convolutions and deep supervision to incorporate multi-scale information into the model. The proposed method is evaluated on the task of hippocampus head and body segmentation in an MRI dataset, provided as part of the MICCAI 2018 segmentation decathlon challenge. The experimental results show that our approach outperforms other conventional methods in terms of different segmentation accuracy metrics.},
archivePrefix = {arXiv},
arxivId = {1903.09097},
author = {Folle, Lukas and Vesal, Sulaiman and Ravikumar, Nishant and Maier, Andreas},
booktitle = {Informatik aktuell},
doi = {10.1007/978-3-658-25326-4_18},
eprint = {1903.09097},
file = {::},
isbn = {9783658253257},
issn = {1431472X},
month = {mar},
pages = {68--73},
title = {{Dilated deeply supervised networks for hippocampus segmentation in MRI}},
url = {http://arxiv.org/abs/1903.09097},
year = {2019}
}
@inproceedings{Xu2019b,
author = {Xu, Gang and Song, Zhigang and Sun, Zhuo and Ku, Calvin and Yang, Zhe and Liu, Cancheng and Wang, Shuhao and Ma, Jianpeng and Xu, Wei},
booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2019.01078},
isbn = {978-1-7281-4803-8},
month = {oct},
pages = {10681--10690},
publisher = {IEEE},
title = {{CAMEL: A Weakly Supervised Learning Framework for Histopathology Image Segmentation}},
url = {https://ieeexplore.ieee.org/document/9008367/},
year = {2019}
}
@article{Swiderska-Chadaj2020,
abstract = {Algorithms can improve the objectivity and efficiency of histopathologic slide analysis. In this paper, we investigated the impact of scanning systems (scanners) and cycle-GAN-based normalization on algorithm performance, by comparing different deep learning models to automatically detect prostate cancer in whole-slide images. Specifically, we compare U-Net, DenseNet and EfficientNet. Models were developed on a multi-center cohort with 582 WSIs and subsequently evaluated on two independent test sets including 85 and 50 WSIs, respectively, to show the robustness of the proposed method to differing staining protocols and scanner types. We also investigated the application of normalization as a pre-processing step by two techniques, the whole-slide image color standardizer (WSICS) algorithm, and a cycle-GAN based method. For the two independent datasets we obtained an AUC of 0.92 and 0.83 respectively. After rescanning the AUC improves to 0.91/0.88 and after style normalization to 0.98/0.97. In the future our algorithm could be used to automatically pre-screen prostate biopsies to alleviate the workload of pathologists.},
author = {Swiderska-Chadaj, Zaneta and de Bel, Thomas and Blanchet, Lionel and Baidoshvili, Alexi and Vossen, Dirk and van der Laak, Jeroen and Litjens, Geert},
doi = {10.1038/s41598-020-71420-0},
issn = {2045-2322},
journal = {Scientific Reports},
number = {1},
pages = {14398},
title = {{Impact of rescanning and normalization on convolutional neural network performance in multi-center, whole-slide classification of prostate cancer}},
url = {https://doi.org/10.1038/s41598-020-71420-0},
volume = {10},
year = {2020}
}
@misc{Litjens2017,
abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
archivePrefix = {arXiv},
arxivId = {1702.05747},
author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and S{\'{a}}nchez, Clara I.},
booktitle = {Medical Image Analysis},
doi = {10.1016/j.media.2017.07.005},
eprint = {1702.05747},
file = {::},
isbn = {978-1-5386-3220-8},
issn = {13618423},
keywords = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
pages = {60--88},
pmid = {28778026},
title = {{A survey on deep learning in medical image analysis}},
volume = {42},
year = {2017}
}
@article{Bulten2020a,
abstract = {BACKGROUND The Gleason score is the strongest correlating predictor of recurrence for prostate cancer, but has substantial inter-observer variability, limiting its usefulness for individual patients. Specialised urological pathologists have greater concordance; however, such expertise is not widely available. Prostate cancer diagnostics could thus benefit from robust, reproducible Gleason grading. We aimed to investigate the potential of deep learning to perform automated Gleason grading of prostate biopsies. METHODS In this retrospective study, we developed a deep-learning system to grade prostate biopsies following the Gleason grading standard. The system was developed using randomly selected biopsies, sampled by the biopsy Gleason score, from patients at the Radboud University Medical Center (pathology report dated between Jan 1, 2012, and Dec 31, 2017). A semi-automatic labelling technique was used to circumvent the need for manual annotations by pathologists, using pathologists' reports as the reference standard during training. The system was developed to delineate individual glands, assign Gleason growth patterns, and determine the biopsy-level grade. For validation of the method, a consensus reference standard was set by three expert urological pathologists on an independent test set of 550 biopsies. Of these 550, 100 were used in an observer experiment, in which the system, 13 pathologists, and two pathologists in training were compared with respect to the reference standard. The system was also compared to an external test dataset of 886 cores, which contained 245 cores from a different centre that were independently graded by two pathologists. FINDINGS We collected 5759 biopsies from 1243 patients. The developed system achieved a high agreement with the reference standard (quadratic Cohen's kappa 0{\textperiodcentered}918, 95{\%} CI 0{\textperiodcentered}891–0{\textperiodcentered}941) and scored highly at clinical decision thresholds: benign versus malignant (area under the curve 0{\textperiodcentered}990, 95{\%} CI 0{\textperiodcentered}982–0{\textperiodcentered}996), grade group of 2 or more (0{\textperiodcentered}978, 0{\textperiodcentered}966–0{\textperiodcentered}988), and grade group of 3 or more (0{\textperiodcentered}974, 0{\textperiodcentered}962–0{\textperiodcentered}984). In an observer experiment, the deep-learning system scored higher (kappa 0{\textperiodcentered}854) than the panel (median kappa 0{\textperiodcentered}819), outperforming 10 of 15 pathologist observers. On the external test dataset, the system obtained a high agreement with the reference standard set independently by two pathologists (quadratic Cohen's kappa 0{\textperiodcentered}723 and 0{\textperiodcentered}707) and within inter-observer variability (kappa 0{\textperiodcentered}71). INTERPRETATION Our automated deep-learning system achieved a performance similar to pathologists for Gleason grading and could potentially contribute to prostate cancer diagnosis. The system could potentially assist pathologists by screening biopsies, providing second opinions on grade group, and presenting quantitative measurements of volume percentages. FUNDING Dutch Cancer Society.},
author = {Bulten, Wouter and Pinckaers, Hans and van Boven, Hester and Vink, Robert and de Bel, Thomas and van Ginneken, Bram and van der Laak, Jeroen and {Hulsbergen-van de Kaa}, Christina and Litjens, Geert},
doi = {10.1016/S1470-2045(19)30739-9},
file = {::},
issn = {1470-2045},
journal = {The Lancet Oncology},
month = {feb},
number = {2},
pages = {233--241},
publisher = {Elsevier},
title = {{Automated deep-learning system for Gleason grading of prostate cancer using biopsies: a diagnostic study}},
url = {https://www.sciencedirect.com/science/article/pii/S1470204519307399},
volume = {21},
year = {2020}
}
@article{VIPS1996,
author = {Cupitt, John and Martinez, Kirk},
doi = {10.1117/12.233043},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
pages = {19 -- 28},
title = {{VIPS: An imaging processing system for large images}},
volume = {1663},
year = {1996}
}
@article{Santurkar,
abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called " internal covariate shift " . In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. These findings bring us closer to a true understanding of our DNN training toolkit.},
author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Mit, Adry},
file = {::},
title = {{How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)}},
url = {https://arxiv.org/pdf/1805.11604.pdf}
}
@inproceedings{Bengio2013,
abstract = {Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges.},
archivePrefix = {arXiv},
arxivId = {1305.0445},
author = {Bengio, Yoshua},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-39593-2_1},
eprint = {1305.0445},
file = {::},
isbn = {9783642395925},
issn = {03029743},
title = {{Deep learning of representations: Looking forward}},
year = {2013}
}
@article{Tellez2019,
author = {Tellez, D and Litjens, G and van der Laak, J and Ciompi, F},
doi = {10.1109/TPAMI.2019.2936841},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Gigapixel image analysis,Image analysis,Image coding,Image reconstruction,Neural networks,Task analysis,Training,Visualization,computational pathology,convolutional neural networks,representation learning},
title = {{Neural Image Compression for Gigapixel Histopathology Image Analysis}},
volume = {in press}
}
@article{Courbariaux2014,
abstract = {Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.},
archivePrefix = {arXiv},
arxivId = {1412.7024},
author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
eprint = {1412.7024},
file = {::},
month = {dec},
title = {{Training deep neural networks with low precision multiplications}},
url = {http://arxiv.org/abs/1412.7024},
year = {2014}
}
@article{Cui2019,
abstract = {With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula {\$}(1-\backslashbeta{\^{}}{\{}n{\}})/(1-\backslashbeta){\$}, where {\$}n{\$} is the number of samples and {\$}\backslashbeta \backslashin [0,1){\$} is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.},
archivePrefix = {arXiv},
arxivId = {1901.05555},
author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
eprint = {1901.05555},
file = {::},
month = {jan},
title = {{Class-Balanced Loss Based on Effective Number of Samples}},
url = {http://arxiv.org/abs/1901.05555},
year = {2019}
}
@article{Rubanova2019,
abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
archivePrefix = {arXiv},
arxivId = {1907.03907},
author = {Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David},
eprint = {1907.03907},
file = {::},
month = {jul},
title = {{Latent ODEs for Irregularly-Sampled Time Series}},
url = {http://arxiv.org/abs/1907.03907},
year = {2019}
}
@incollection{NIPS2014_5597,
author = {Li, Mu and Andersen, David G and Smola, Alexander J and Yu, Kai},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and Weinberger, K Q},
pages = {19--27},
publisher = {Curran Associates, Inc.},
title = {{Communication Efficient Distributed Machine Learning with the Parameter Server}},
url = {http://papers.nips.cc/paper/5597-communication-efficient-distributed-machine-learning-with-the-parameter-server.pdf},
year = {2014}
}
@article{Bejnordi2016,
abstract = {Variations in the color and intensity of hematoxylin and eosin (H{\&}E) stained histological slides can potentially hamper the effectiveness of quantitative image analysis. This paper presents a fully automated algorithm for standardization of whole-slide histopathological images to reduce the effect of these variations. The proposed algorithm, called whole-slide image color standardizer (WSICS), utilizes color and spatial information to classify the image pixels into different stain components. The chromatic and density distributions for each of the stain components in the hue-saturation-density color model are aligned to match the corresponding distributions from a template wholeslide image (WSI). The performance of the WSICS algorithm was evaluated on two datasets. The first originated from 125 H{\&}E stained WSIs of lymph nodes, sampled from 3 patients, and stained in 5 different laboratories on different days of the week. The second comprised 30 H{\&}E stained WSIs of rat liver sections. The result of qualitative and quantitative evaluations using the first dataset demonstrate that the WSICS algorithm outperforms competing methods in terms of achieving color constancy. The WSICS algorithm consistently yields the smallest standard deviation and coefficient of variation of the normalized median intensity measure. Using the second dataset, we evaluated the impact of our algorithm on the performance of an already published necrosis quantification system. The performance of this system was significantly improved by utilizing the WSICS algorithm. The results of the empirical evaluations collectively demonstrate the potential contribution of the proposed standardization algorithm to improved diagnostic accuracy and consistency in computer-aided diagnosis for histopathology data.},
author = {Bejnordi, Babak Ehteshami and Litjens, Geert and Timofeeva, Nadya and Otte-H{\"{o}}ller, Irene and Homeyer, Andr{\'{e}} and Karssemeijer, Nico and {Van Der Laak}, Jeroen A.W.M.},
doi = {10.1109/TMI.2015.2476509},
file = {::},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Computer-aided diagnosis,H{\&}E staining,Huesaturation-density,Standardization,Whole-slide image color standardizer (WSICS),Whole-slide imaging},
number = {2},
pages = {404--415},
pmid = {26353368},
title = {{Stain specific standardization of whole-slide histopathological images}},
volume = {35},
year = {2016}
}
@article{Becker2000,
author = {Becker, Stefan and Miron-Shatz, Talya and Schumacher, Nikolaus and Krocza, Johann and Diamantidis, Clarissa and Albrecht, Urs-Vito},
journal = {JMIR mHealth uHealth},
month = {jan},
pages = {e24},
title = {{mHealth 2.0: Experiences, Possibilities, and Perspectives}},
url = {http://readerific.com/exported/?a=25ff1d papers3://publication/uuid/AFBE7A87-01B6-4EBE-BBA0-9168F41F8F30},
volume = {2},
year = {2000}
}
@article{Carroll2004c,
abstract = {BACKGROUND: Patients are commonly nonadherent to medication regimens. In dermatology, there has been little study of the effect of nonadherence on outcomes. OBJECTIVES: To test the association between adherence behaviour and changes in severity of psoriasis. METHODS: Twenty-four subjects with psoriasis were enrolled in an 8-week, left/right, controlled trial of salicylic acid plus topical tacrolimus ointment vs. salicylic acid plus placebo. Subjects were given salicylic acid to apply to all lesions. The salicylic acid was supplied in a bottle with a medication event monitoring system cap in order to assess adherence to the salicylic acid. The primary outcome for this study was the relationship between the change in the disease severity (change in sum score of erythema, scale and thickness scores for a target plaque) and medication adherence. RESULTS: The mean initial disease severity was 5.8 on a nine-point sum score scale. For the topical tacrolimus-treated side, a decrease in adherence rate of 10{\%} was associated with a 1-point increase in severity (P {\textless} 0.05). For the placebo-treated side, adherence was not significantly correlated with changes in severity. CONCLUSIONS: Nonadherence may have a significant role in altering clinical trial data, skewing it towards ineffectiveness. Improved outcomes in psoriasis may be achievable through interventions that improve patients' adherence to treatment.},
address = {Department of Dermatology, Wake Forest University School of Medicine, Winston-Salem, NC, USA.},
author = {Carroll, C L and Feldman, S R and Camacho, F T and Balkrishnan, R},
journal = {The British journal of dermatology},
month = {oct},
number = {4},
pages = {895--897},
title = {{Better medication adherence results in greater improvement in severity of psoriasis.}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=15491434{\&}retmode=ref{\&}cmd=prlinks papers3://publication/doi/10.1111/j.1365-2133.2004.06174.x},
volume = {151},
year = {2004}
}
@inproceedings{Pinckaers2018,
abstract = {To train deep convolutional neural networks, the input data and the intermediate activations need to be kept in memory to calculate the gradient descent step. Given the limited memory available in the current generation accelerator cards, this limits the maximum dimensions of the input data. We demonstrate a method to train convolutional neural networks holding only parts of the image in memory while giving equivalent results. We quantitatively compare this new way of training convolutional neural networks with conventional training. In addition, as a proof of concept, we train a convolutional neural network with 64 megapixel images, which requires 97{\%} less memory than the conventional approach.},
archivePrefix = {arXiv},
arxivId = {1804.05712},
author = {Pinckaers, Hans and Litjens, Geert},
booktitle = {1st international conference on Medical Imaging with Deep Learning},
eprint = {1804.05712},
file = {::},
title = {{Training convolutional neural networks with megapixel images}},
url = {http://arxiv.org/abs/1804.05712},
year = {2018}
}
@article{Ianni2020a,
abstract = {Standard of care diagnostic procedure for suspected skin cancer is microscopic examination of hematoxylin {\&} eosin stained tissue by a pathologist. Areas of high inter-pathologist discordance and rising biopsy rates necessitate higher efficiency and diagnostic reproducibility. We present and validate a deep learning system which classifies digitized dermatopathology slides into 4 categories. The system is developed using 5,070 images from a single lab, and tested on an uncurated set of 13,537 images from 3 test labs, using whole slide scanners manufactured by 3 different vendors. The system's use of deep-learning-based confidence scoring as a criterion to consider the result as accurate yields an accuracy of up to 98{\%}, and makes it adoptable in a real-world setting. Without confidence scoring, the system achieved an accuracy of 78{\%}. We anticipate that our deep learning system will serve as a foundation enabling faster diagnosis of skin cancer, identification of cases for specialist review, and targeted diagnostic classifications.},
author = {Ianni, Julianna D. and Soans, Rajath E. and Sankarapandian, Sivaramakrishnan and Chamarthi, Ramachandra Vikas and Ayyagari, Devi and Olsen, Thomas G. and Bonham, Michael J. and Stavish, Coleman C. and Motaparthi, Kiran and Cockerell, Clay J. and Feeser, Theresa A. and Lee, Jason B.},
doi = {10.1038/s41598-020-59985-2},
issn = {2045-2322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {3217},
pmid = {32081956},
title = {{Tailored for Real-World: A Whole Slide Image Classification System Validated on Uncurated Multi-Site Data Emulating the Prospective Pathology Workload}},
url = {http://www.nature.com/articles/s41598-020-59985-2},
volume = {10},
year = {2020}
}
@article{Lane2005,
author = {Lane, Shannon J and Heddle, Nancy M and Arnold, Emmy and Walker, Irwin T I},
journal = {BMC Med Inform Decis Mak},
month = {jan},
number = {1},
pages = {23},
title = {{A review of randomized controlled trials comparing the effectiveness of hand held computers with paper methods for data collection}},
url = {http://readerific.com/exported/?a=342f2e papers3://publication/doi/10.1186/1472-6947-6-23},
volume = {6},
year = {2005}
}
@article{Hosni2017,
archivePrefix = {arXiv},
arxivId = {1705.11186},
author = {Hosni, Hykel and Vulpiani, Angelo},
doi = {10.1007/S13347-017-0265-3},
eprint = {1705.11186},
journal = {Philosophy {\&} Technology},
title = {{Forecasting in the light of Big Data}},
url = {http://link.springer.com/10.1007/s13347-017-0265-3},
year = {2017}
}
@inproceedings{Bulo2018,
abstract = {In this work we present In-Place Activated Batch Normalization (InPlace-ABN) - a novel approach to drastically reduce the training memory footprint of modern deep neural networks in a computationally efficient way. Our solution substitutes the conventionally used succession of BatchNorm + Activation layers with a single plugin layer, hence avoiding invasive framework surgery while providing straightforward applicability for existing deep learning frameworks. We obtain memory savings of up to 50{\%} by dropping intermediate results and by recovering required information during the backward pass through the inversion of stored forward results, with only minor increase (0.8-2{\%}) in computation time. Also, we demonstrate how frequently used checkpointing approaches can be made computationally as efficient as InPlace-ABN. In our experiments on image classification, we demonstrate on-par results on ImageNet-1k with state-of-the-art approaches. On the memory-demanding task of semantic segmentation, we report results for COCO-Stuff, Cityscapes and Mapillary Vistas, obtaining new state-of-the-art results on the latter without additional training data but in a single-scale and -model scenario. Code can be found at https://github.com/mapillary/inplace{\_}abn .},
author = {Bulo, Samuel Rota and Porzi, Lorenzo and Kontschieder, Peter},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00591},
isbn = {9781538664209},
issn = {10636919},
title = {{In-place Activated BatchNorm for Memory-Optimized Training of DNNs}},
year = {2018}
}
@misc{Ma2019,
abstract = {Deep learning (DL)algorithms have seen a massive rise in popularity for remote-sensing image analysis over the past few years. In this study, the major DL concepts pertinent to remote-sensing are introduced, and more than 200 publications in this field, most of which were published during the last two years, are reviewed and analyzed. Initially, a meta-analysis was conducted to analyze the status of remote sensing DL studies in terms of the study targets, DL model(s)used, image spatial resolution(s), type of study area, and level of classification accuracy achieved. Subsequently, a detailed review is conducted to describe/discuss how DL has been applied for remote sensing image analysis tasks including image fusion, image registration, scene classification, object detection, land use and land cover (LULC)classification, segmentation, and object-based image analysis (OBIA). This review covers nearly every application and technology in the field of remote sensing, ranging from preprocessing to mapping. Finally, a conclusion regarding the current state-of-the art methods, a critical conclusion on open challenges, and directions for future research are presented.},
author = {Ma, Lei and Liu, Yu and Zhang, Xueliang and Ye, Yuanxin and Yin, Gaofei and Johnson, Brian Alan},
booktitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
doi = {10.1016/j.isprsjprs.2019.04.015},
issn = {09242716},
keywords = {Deep learning (DL),LULC classification,Object detection,Remote sensing,Scene classification},
month = {jun},
pages = {166--177},
publisher = {Elsevier B.V.},
title = {{Deep learning in remote sensing applications: A meta-analysis and review}},
volume = {152},
year = {2019}
}
@inproceedings{Girshick:2015:FR:2919332.2920125,
address = {Washington, DC, USA},
author = {Girshick, Ross},
booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.169},
isbn = {978-1-4673-8391-2},
pages = {1440--1448},
publisher = {IEEE Computer Society},
series = {ICCV '15},
title = {{Fast R-CNN}},
url = {http://dx.doi.org/10.1109/ICCV.2015.169},
year = {2015}
}
@article{West2013,
abstract = {Background The chronic and relapsing course of psoriasis is often associated with poor adherence to treatment. Adherence to topical treatment is abysmal. Adherence to systemic treatments also decreases over time, with an overall adherence rate of 67{\%} for injectable biologic medications. Whereas overall trends in poor adherence have been documented, the fine details of adherence in individual patients is not well characterized. Purpose To assess adherence to adalimumab in patients with moderate to severe psoriasis. Methods Data on adherence were obtained from a 1-year open label trial including seven patients with moderate to severe psoriasis who agreed to participate in a randomized trial of standard physician education materials plus extended nurse education versus standard physician education materials alone. Adherence to treatment was recorded with electronic monitoring via Medication Event Monitoring System (MEMS) caps undisclosed to the patients. Patients were also instructed to note the time and date they used treatment in a journal. Results The subjects exhibited a broad range of adherence behaviors. Conclusions Adherence to adalimumab therapy for moderate-to-severe psoriasis is variable and can be very poor. The clinical impact of poor adherence to injectable biologic medications is not yet well characterized.},
author = {West, Cameron and Narahari, Swetha and O'Neill, Jenna and Davis, Scott and Huynh, Monica and Clark, Adele and Boles, Ann and Feldman, Steven R},
journal = {Dermatology online journal},
month = {may},
number = {5},
pages = {18182},
title = {{Adherence to adalimumab in patients with moderate to severe psoriasis.}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=24011280{\&}retmode=ref{\&}cmd=prlinks papers3://publication/uuid/43E8BF21-9A3A-4552-97E5-DFDA68E344F1},
volume = {19},
year = {2013}
}
@article{Zhang2019,
abstract = {GPU (graphics processing unit) has been used for many data-intensive applications. Among them, deep learning systems are one of the most important consumer systems for GPU nowadays. As deep learning applications impose deeper and larger models in order to achieve higher accuracy, memory management becomes an important research topic for deep learning systems, given that GPU has limited memory size. Many approaches have been proposed towards this issue, e.g., model compression and memory swapping. However, they either degrade the model accuracy or require a lot of manual intervention. In this paper, we propose two orthogonal approaches to reduce the memory cost from the system perspective. Our approaches are transparent to the models, and thus do not affect the model accuracy. They are achieved by exploiting the iterative nature of the training algorithm of deep learning to derive the lifetime and read/write order of all variables. With the lifetime semantics, we are able to implement a memory pool with minimal fragments. However, the optimization problem is NP-complete. We propose a heuristic algorithm that reduces up to 13.3{\%} of memory compared with Nvidia's default memory pool with equal time complexity. With the read/write semantics, the variables that are not in use can be swapped out from GPU to CPU to reduce the memory footprint. We propose multiple swapping strategies to automatically decide which variable to swap and when to swap out (in), which reduces the memory cost by up to 34.2{\%} without communication overhead.},
archivePrefix = {arXiv},
arxivId = {1903.06631},
author = {Zhang, Junzhe and Yeung, Sai Ho and Shu, Yao and He, Bingsheng and Wang, Wei},
eprint = {1903.06631},
file = {::},
month = {feb},
title = {{Efficient Memory Management for GPU-based Deep Learning Systems}},
url = {http://arxiv.org/abs/1903.06631},
year = {2019}
}
@inproceedings{Salimans2016,
abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
author = {Salimans, Tim and Kingma, Diederik P.},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
title = {{Weight normalization: A simple reparameterization to accelerate training of deep neural networks}},
year = {2016}
}
@inproceedings{Mishkin2015,
abstract = {Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.},
archivePrefix = {arXiv},
arxivId = {1511.06422},
author = {Mishkin, Dmytro and Matas, Jiri},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1511.06422},
file = {::},
month = {nov},
title = {{All you need is a good init}},
url = {http://arxiv.org/abs/1511.06422},
year = {2016}
}
@article{park_faster_2016,
abstract = {Highly-performance sparse convolution outperforms dense with only 70{\%} sparsity. Performance model that guides training to find useful sparsity range, applied to AlexNet and GoogLeNet},
author = {Park, Jongsoo and Li, Sheng and Wen, Wei and Tang, Ping Tak Peter and Li, Hai and Chen, Yiran and Dubey, Pradeep},
file = {:Users/Hans/Zotero/storage/7TU2JGTS/Park et al. - 2016 - Faster CNNs with Direct Sparse Convolutions and Gu.pdf:pdf;:Users/Hans/Zotero/storage/84UXBZIB/forum.html:html},
month = {nov},
title = {{Faster CNNs with Direct Sparse Convolutions and Guided Pruning}},
url = {https://openreview.net/forum?id=rJPcZ3txx},
year = {2016}
}
@article{Ballal2012a,
abstract = {A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We de-scribe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the rela-tionship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagat-ing through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45{\%} compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attack than our baseline convolutional neural network.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Ballal, Dilip and Zelina, Joseph},
doi = {10.2514/6.2003-4412},
eprint = {1710.09829},
file = {::},
isbn = {978-1-62410-098-7},
issn = {10495258},
journal = {International Conference on Learning Representations},
pages = {1--15},
pmid = {215550},
title = {{Matrix Capsules With Em Routing}},
url = {http://arc.aiaa.org/doi/10.2514/6.2003-4412},
year = {2012}
}
@inproceedings{Vanhoucke2011,
author = {Vanhoucke, Vincent and Senior, Andrew and Mao, Mark Z},
booktitle = {Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011},
title = {{Improving the speed of neural networks on CPUs}},
year = {2011}
}
@article{Stone2003,
author = {Stone, Arthur A and Shiffman, Saul and Schwartz, Joseph E and Broderick, Joan E and Hufford, Michael R},
journal = {Controlled Clinical Trials},
month = {apr},
pages = {182--199},
title = {{Patient compliance with paper and electronic diaries}},
url = {http://readerific.com/exported/?a=58c71c papers3://publication/uuid/F8D91120-FA05-41CB-A6C6-4EEA40FB74A4},
volume = {24},
year = {2003}
}
@article{Ozkan2016,
abstract = {Objective: The aims of this study were to evaluate the reproducibility of the Gleason grading system and to compare its interobserver variability with the novel Gleason grade grouping proposal using a large sample volume. Materials and methods: In total, 407 pathology slides of prostate needle biopsies from 34 consecutive patients with prostate cancer were re-evaluated. The International Society of Urological Pathology 2005 modified Gleason grading system with Epstein's modification was used. Two pathologists, blind to each other and to the initial pathology report, performed the pathological evaluation. To determine interobserver concordance, the kappa ($\kappa$) coefficient test was used. Results: Pathologist 1 and pathologist 2 detected a tumor in 202 and 231 cores, respectively (p {\textless} 0.001). The two pathologists disagreed on the presence of a tumor in 31 cores. Of these 31 cores, 74{\%} (n = 23/31) were Gleason pattern 3. The mean length of the cancer foci in these 31 disputed cores was 1.54 ± 0.8 mm. Concordance rates between the two observers for primary and secondary Gleason patterns were 63.96{\%} ($\kappa$ = 0.34) and 63.45{\%} ($\kappa$ = 0.37), respectively. Concordance with respect to the Gleason sum was 57.9{\%} ($\kappa$ = 0.43). When the Gleason scores were classified into the novel Gleason grade grouping, concordance was found to be 51.7{\%} ($\kappa$ = 0.39). Conclusions: The agreement between observers on the Gleason sum was moderate. The novel Gleason grade grouping did not improve interobserver agreement. Further studies are needed to confirm these results on interobserver variability.},
author = {Ozkan, Tayyar A. and Eruyar, Ahmet T. and Cebeci, Oguz O. and Memik, Omur and Ozcan, Levent and Kuskonmaz, Ibrahim},
doi = {10.1080/21681805.2016.1206619},
issn = {2168-1805},
journal = {Scandinavian Journal of Urology},
keywords = {Gleason grading,neoplasm grading,observer variation,prostatic neoplasms},
month = {nov},
number = {6},
pages = {420--424},
title = {{Interobserver variability in Gleason histological grading of prostate cancer}},
url = {https://www.tandfonline.com/doi/full/10.1080/21681805.2016.1206619},
volume = {50},
year = {2016}
}
@article{Li2019a,
archivePrefix = {arXiv},
arxivId = {1905.13208},
author = {Li, Jiayun and Li, Wenyuan and Gertych, Arkadiusz and Knudsen, Beatrice S and Speier, William and Arnold, Corey W},
eprint = {1905.13208},
journal = {arXiv preprint},
title = {{An attention-based multi-resolution model for prostate whole slide imageclassification and localization}},
url = {http://arxiv.org/abs/1905.13208},
volume = {1905.13208},
year = {2019}
}
@article{Sirinukunwattana2017,
abstract = {Colorectal adenocarcinoma originating in intestinal glandular structures is the most common form of colon cancer. In clinical practice, the morphology of intestinal glands, including architectural appearance and glandular formation, is used by pathologists to inform prognosis and plan the treatment of individual patients. However, achieving good inter-observer as well as intra-observer reproducibility of cancer grading is still a major challenge in modern pathology. An automated approach which quantifies the morphology of glands is a solution to the problem. This paper provides an overview to the Gland Segmentation in Colon Histology Images Challenge Contest (GlaS) held at MICCAI'2015. Details of the challenge, including organization, dataset and evaluation criteria, are presented, along with the method descriptions and evaluation results from the top performing methods.},
author = {Sirinukunwattana, Korsuk and Pluim, Josien P.W. and Chen, Hao and Qi, Xiaojuan and Heng, Pheng-Ann and Guo, Yun Bo and Wang, Li Yang and Matuszewski, Bogdan J. and Bruni, Elia and Sanchez, Urko and B{\"{o}}hm, Anton and Ronneberger, Olaf and Cheikh, Bassem Ben and Racoceanu, Daniel and Kainz, Philipp and Pfeiffer, Michael and Urschler, Martin and Snead, David R.J. and Rajpoot, Nasir M.},
doi = {10.1016/j.media.2016.08.008},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Colon cancer,Digital pathology,Histology image analysis,Intestinal gland,Segmentation},
month = {jan},
pages = {489--502},
title = {{Gland segmentation in colon histology images: The glas challenge contest}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841516301542},
volume = {35},
year = {2017}
}
@article{Weinstein2013,
abstract = {The Cancer Genome Atlas (TCGA) Research Network has profiled and analyzed large numbers of human tumors to discover molecular aberrations at the DNA, RNA, protein and epigenetic levels. The resulting rich data provide a major opportunity to develop an integrated picture of commonalities, differences and emergent themes across tumor lineages. The Pan-Cancer initiative compares the first 12 tumor types profiled by TCGA. Analysis of the molecular aberrations and their functional roles across tumor types will teach us how to extend therapies effective in one cancer type to others with a similar genomic profile.},
author = {Weinstein, John N and Collisson, Eric A and Mills, Gordon B and Shaw, Kenna R Mills and Ozenberger, Brad A and Ellrott, Kyle and Shmulevich, Ilya and Sander, Chris and Stuart, Joshua M and Stuart, Joshua M},
doi = {10.1038/ng.2764},
issn = {1061-4036},
journal = {Nature Genetics},
month = {oct},
number = {10},
pages = {1113--1120},
pmid = {24071849},
title = {{The Cancer Genome Atlas Pan-Cancer analysis project}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24071849 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3919969 http://www.nature.com/articles/ng.2764},
volume = {45},
year = {2013}
}
@inproceedings{Recasens2018,
abstract = {We introduce a saliency-based distortion layer for convolutional neural networks that helps to improve the spatial sampling of input data for a given task. Our differentiable layer can be added as a preprocessing block to existing task networks and trained altogether in an end-to-end fashion. The effect of the layer is to efficiently estimate how to sample from the original data in order to boost task performance. For example, for an image classification task in which the original data might range in size up to several megapixels, but where the desired input images to the task network are much smaller, our layer learns how best to sample from the underlying high resolution data in a manner which preserves task-relevant information better than uniform downsampling. This has the effect of creating distorted, caricature-like intermediate images, in which idiosyncratic elements of the image that improve task performance are zoomed and exaggerated. Unlike alternative approaches such as spatial transformer networks, our proposed layer is inspired by image saliency, computed efficiently from uniformly downsampled data, and degrades gracefully to a uniform sampling strategy under uncertainty. We apply our layer to improve existing networks for the tasks of human gaze estimation and fine-grained object classification. Code for our method is available in: http://github.com/recasens/Saliency-Sampler},
archivePrefix = {arXiv},
arxivId = {1809.03355},
author = {Recasens, Adri{\`{a}} and Kellnhofer, Petr and Stent, Simon and Matusik, Wojciech and Torralba, Antonio},
booktitle = {European Conference on Computer Vision},
eprint = {1809.03355},
file = {::},
month = {sep},
title = {{Learning to Zoom: a Saliency-Based Sampling Layer for Neural Networks}},
url = {http://arxiv.org/abs/1809.03355},
year = {2018}
}
@article{Litjens2014,
abstract = {Prostate MRI image segmentation has been an area of intense research due to the increased use of MRI as a modality for the clinical workup of prostate cancer. Segmentation is useful for various tasks, e.g. to accurately localize prostate boundaries for radiotherapy or to initialize multi-modal registration algorithms. In the past, it has been difficult for research groups to evaluate prostate segmentation algorithms on multi-center, multi-vendor and multi-protocol data. Especially because we are dealing with MR images, image appearance, resolution and the presence of artifacts are affected by differences in scanners and/or protocols, which in turn can have a large influence on algorithm accuracy. The Prostate MR Image Segmentation (PROMISE12) challenge was setup to allow a fair and meaningful comparison of segmentation methods on the basis of performance and robustness. In this work we will discuss the initial results of the online PROMISE12 challenge, and the results obtained in the live challenge workshop hosted by the MICCAI2012 conference. In the challenge, 100 prostate MR cases from 4 different centers were included, with differences in scanner manufacturer, field strength and protocol. A total of 11 teams from academic research groups and industry participated. Algorithms showed a wide variety in methods and implementation, including active appearance models, atlas registration and level sets. Evaluation was performed using boundary and volume based metrics which were combined into a single score relating the metrics to human expert performance. The winners of the challenge where the algorithms by teams Imorphics and ScrAutoProstate, with scores of 85.72 and 84.29 overall. Both algorithms where significantly better than all other algorithms in the challenge (p {\textless} 0.05) and had an efficient implementation with a run time of 8. min and 3. s per case respectively. Overall, active appearance model based approaches seemed to outperform other approaches like multi-atlas registration, both on accuracy and computation time. Although average algorithm performance was good to excellent and the Imorphics algorithm outperformed the second observer on average, we showed that algorithm combination might lead to further improvement, indicating that optimal performance for prostate segmentation is not yet obtained. All results are available online at http://promise12.grand-challenge.org/. {\textcopyright} 2013 Elsevier B.V.},
author = {Litjens, Geert and Toth, Robert and van de Ven, Wendy and Hoeks, Caroline and Kerkstra, Sjoerd and van Ginneken, Bram and Vincent, Graham and Guillard, Gwenael and Birbeck, Neil and Zhang, Jindang and Strand, Robin and Malmberg, Filip and Ou, Yangming and Davatzikos, Christos and Kirschner, Matthias and Jung, Florian and Yuan, Jing and Qiu, Wu and Gao, Qinquan and Edwards, Philip Eddie and Maan, Bianca and van der Heijden, Ferdinand and Ghose, Soumya and Mitra, Jhimli and Dowling, Jason and Barratt, Dean and Huisman, Henkjan and Madabhushi, Anant},
doi = {10.1016/j.media.2013.12.002},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Challenge,MRI,Prostate,Segmentation},
month = {feb},
number = {2},
pages = {359--373},
title = {{Evaluation of prostate segmentation algorithms for MRI: The PROMISE12 challenge}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841513001734},
volume = {18},
year = {2014}
}
@article{Torre2015,
abstract = {Cancer constitutes an enormous burden on society in more and less economically developed countries alike. The occurrence of cancer is increasing because of the growth and aging of the population, as well as an increasing prevalence of established risk factors such as smoking, overweight, physical inactivity, and changing reproductive patterns associated with urbanization and economic development. Based on GLOBOCAN estimates, about 14.1 million new cancer cases and 8.2 million deaths occurred in 2012 worldwide. Over the years, the burden has shifted to less developed countries, which currently account for about 57{\%} of cases and 65{\%} of cancer deaths worldwide. Lung cancer is the leading cause of cancer death among males in both more and less developed countries, and has surpassed breast cancer as the leading cause of cancer death among females in more devel- oped countries; breast cancer remains the leading cause of cancer death among females in less developed countries. Other leading causes of cancer death in more developed countries include colorectal cancer among males and females and prostate cancer among males. In less developed countries, liver and stomach cancer among males and cervical cancer among females are also leading causes of cancer death. Although incidence rates for all cancers combined are nearly twice as high in more developed than in less developed countries in both males and females, mortality rates are only 8{\%} to 15{\%} higher in more devel- oped countries. This disparity reflects regional differences in the mix of cancers, which is affected by risk factors and detection practices, and/or the availability of treatment. Risk factors associated with the leading causes of cancer death include tobacco use (lung, colorectal, stomach, and liver cancer), overweight/obesity and physical inactivity (breast and colorectal cancer), and infection (liver, stomach, and cervical cancer). A substantial portion of cancer cases and deaths could be prevented by broadly applying effective prevention measures, such as tobacco control, vaccination, and the use of early detection tests.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Torre, Lindsey A and Bray, Freddie and Siegel, Rebecca L and Ferlay, Jacques and Lortet-tieulent, Joannie and Jemal, Ahmedin},
doi = {10.3322/caac.21262.},
eprint = {arXiv:1011.1669v3},
isbn = {00079235},
issn = {1542-4863 (Electronic)},
journal = {CA: a cancer journal of clinicians.},
keywords = {cancer,epidemiology,health disparities,incidence,survival},
number = {2},
pages = {87--108},
pmid = {25651787},
title = {{Global Cancer Statistics, 2012}},
url = {http://onlinelibrary.wiley.com/doi/10.3322/caac.21262/abstract},
volume = {65},
year = {2015}
}
@inproceedings{Smith2017,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep convolutional neural networks (i.e., a "guessing game"). This report describes a new method for setting the learning rate, named cyclical learning rates, that eliminates the need to experimentally find the best values and schedule for the learning rates. Instead of setting the learning rate to fixed values, this method lets the learning rate cyclically vary within reasonable boundary values. This report shows that training with cyclical learning rates achieves near optimal classification accuracy without tuning and often in many fewer iterations. This report also describes a simple way to estimate "reasonable bounds" - by linearly increasing the learning rate in one training run of the network for only a few epochs. In addition, cyclical learning rates are demonstrated on training with the CIFAR-10 dataset and the AlexNet and GoogLeNet architectures on the ImageNet dataset. These methods are practical tools for everyone who trains convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1506.01186},
author = {Smith, Leslie N.},
booktitle = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
doi = {10.1109/WACV.2017.58},
eprint = {1506.01186},
isbn = {9781509048229},
title = {{Cyclical learning rates for training neural networks}},
year = {2017}
}
@article{Rumelhart1986,
annote = {10.1038/323533a0},
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
journal = {Nature},
month = {oct},
number = {6088},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
url = {http://dx.doi.org/10.1038/323533a0},
volume = {323},
year = {1986}
}
@article{Oliver2018,
abstract = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.},
archivePrefix = {arXiv},
arxivId = {1804.09170},
author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D. and Goodfellow, Ian J.},
doi = {10.1016/S0090-4295(01)01062-7},
eprint = {1804.09170},
file = {::},
issn = {00904295},
title = {{Realistic Evaluation of Deep Semi-Supervised Learning Algorithms}},
year = {2018}
}
@inproceedings{Blei,
author = {Blei, David M},
file = {::},
keywords = {Alison Toon,Alison Toon Photographer,GALA 2014,GALAconf,Galata Bridge,Golden Horn,Istanbul,Turkey,all rights reserved,boats,buying,children,city,city life,city view,copyright,crowd,destination,market,men,people,street,tourism,travel,view,women},
title = {{Bayesian Deep Learning and Generic Bayesian Inference}}
}
@inproceedings{Kingma2015,
author = {Kingma, Diederick P and Ba, Jimmy},
booktitle = {International Conference on Learning Representations (ICLR)},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@techreport{Sohl-Dickstein,
abstract = {Recent work [2, 1] has noted that all bad local minima can be removed from neural network loss landscapes, by adding a single unit with a particular parameterization. We show that the core technique from these papers can be used to remove all bad local minima from any loss landscape, so long as the global minimum has a loss of zero. This procedure does not require the addition of auxiliary units, or even that the loss be associated with a neural network. The method of action involves all bad local minima being converted into bad (non-local) minima at infinity in terms of auxiliary parameters. I. ELIMINATING ALL BAD LOCAL MINIMA Take a loss function L ($\theta$), with parameters $\theta$, and with a global minimum min $\theta$ L ($\theta$) = 0. Consider the modified loss functio{\~{n}} functio{\~{n}} L ($\theta$, a, b) = L ($\theta$) 1 + (a exp (b) − 1) 2 + $\lambda$a 2 , (1) where a, b ∈ R are auxiliary parameters, and $\lambda$ ∈ R + is a regularization hyperparameter. The specific form of Equation 1 was chosen to emphasize the similarity to the approach in Liang et al. [2] and Kawaguchi and Kaelbling [1], but without involving auxiliary units. As can be seen by inspection, the gradient with respect to the auxiliary parameters a, b is only zero for finite b when L ($\theta$) = 0 and a = 0. Otherwise, a will tend to shrink towards zero to satisfy the regularizer, b will tend to grow towards infinity so that a exp (b) can remain approximately 1, and no fixed point will be achieved for finite b. Thus, all non-global local minima of L ($\theta$) are transformed into minima at b → ∞ of˜Lof˜ of˜L ($\theta$, a, b). Recall that minima at infinity do not qualify as local minima in R n. Therefore, any local minimum of˜Lof˜ of˜L ($\theta$, a, b) is a global minimum of L ($\theta$), and˜Land˜ and˜L ($\theta$, a, b) has no bad local minima. See Appendix I for a more formal derivation, and Figure 1 for a visualization. II. IS THIS SIGNIFICANT? By eliminating the auxiliary neurons which play a central role in Kawaguchi and Kaelbling [1] and Liang et al. [2] we hope to provide more clarity into the mechanism by which bad local minima are removed from the augmented loss. We leave it to the reader to judge whether removing local minima in this fashion is trivial, deep, or both. We also note that there is extensive discussion in Section 5 of Kawaguchi and Kaelbling [1] of situations in which their auxiliary variable b (which plays a qualitatively similar role to b in Section I above) diverges to infinity. So, it has been previously observed that pathologies can continue to exist in loss landscapes modified in a fashion similar to above. Fig. 1. All local minima of L ($\theta$) with L ($\theta$) {\textgreater} 0 become non-local minima at infinity of˜Lof˜ of˜L ($\theta$, a, b). Contour plots of the modified loss landscape˜Llandscape˜ landscape˜L ($\theta$, a, b) in terms of auxiliary parameters a and b, for $\lambda$ = 1. When the original loss function L ($\theta$) {\textgreater} 0, the{\~{n}} L ($\theta$, a, b) approaches a minimum in terms of a and b as b → ∞ and a → 0. When L ($\theta$) is at its global minimum, L ($\theta$) = 0, the{\~{n}} L ($\theta$, a, b) has a local minimum at a = 0, for any value of b.},
archivePrefix = {arXiv},
arxivId = {1901.00279},
author = {Sohl-Dickstein, Jascha and Kawaguchi, Kenji},
eprint = {1901.00279},
file = {::},
title = {{Eliminating All Bad Local Minima from Loss Landscapes Without Even Adding an Extra Unit}},
url = {https://arxiv.org/pdf/1901.03909.pdf}
}
@techreport{Smith2018,
abstract = {In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions. However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed , it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem. In this work we develop a novel policy gradient method for the automatic learning of policies with options. This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. The dif-ferentiable inference procedure employed yields options that can be easily interpreted. Empirical results confirm these attributes, and indicate that our algorithm has an improved sample efficiency relative to state-of-the-art in learning options end-to-end.},
author = {Smith, Matthew J A and {Van Hoof}, Herke and Pineau, Joelle},
file = {::},
title = {{An Inference-Based Policy Gradient Method for Learning Options}},
year = {2018}
}
@article{Ruifrok2001,
abstract = {OBJECTIVE: To develop a flexible method of separation and quantification of immunohistochemical staining by means of color image analysis. STUDY DESIGN: An algorithm was developed to deconvolve the color information acquired with red-green-blue (RGB) cameras and to calculate the contribution of each of the applied stains based on stain-specific RGB absorption. The algorithm was tested using different combinations of diaminobenzidine, hematoxylin and eosin at different staining levels. RESULTS: Quantification of the different stains was not significantly influenced by the combination of multiple stains in a single sample. The color deconvolution algorithm resulted in comparable quantification independent of the stain combinations as long as the histochemical procedures did not influence the amount of stain in the sample due to bleaching because of stain solubility and saturation of staining was prevented. CONCLUSION: This image analysis algorithm provides a robust and flexible method for objective immunohistochemical analysis of samples stained with up to three different stains using a laboratory microscope, standard RGB camera setup and the public domain program NIH Image.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ruifrok, A. C. and Johnston, D. A.},
doi = {10.1097/00129039-200303000-00014},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0884-6812 (Print)$\backslash$r0884-6812 (Linking)},
issn = {08846812},
journal = {Analytical and Quantitative Cytology and Histology},
keywords = {Color deconvolution,Color separation,Computer-assisted,Image processing,Immunohistochemistry},
number = {4},
pages = {291--299},
pmid = {11531144},
title = {{Quantification of histochemical staining by color deconvolution}},
volume = {23},
year = {2001}
}
@article{Simonyan2013,
abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
archivePrefix = {arXiv},
arxivId = {1312.6034},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1312.6034},
file = {::},
journal = {CoRR},
month = {dec},
title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
url = {http://arxiv.org/abs/1312.6034},
year = {2013}
}
@inproceedings{Ioffe2015,
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {448--456},
publisher = {JMLR.org},
series = {ICML'15},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
year = {2015}
}
@article{Geijs,
abstract = {Assessment of immunohistochemically sfile:///home/hans/Downloads/spie2015full{\%}20(1).pdf tained slides is often a crucial diagnostic step in clinical practice. How- ever, as this assessment is generally performed manually by pathologists it can suffer from significant inter- observer variability. The introduction of whole-slide scanners facilitates automated analysis of immunohisto- chemical slides. Color deconvolution (CD) is one of the most popular first steps in quantifying stain density in histopathological images. However, color deconvolution requires stain color vectors for accurate unmixing. Often it is assumed that these stain vectors are static. In practice, however, they are influenced by many factors. This can cause inferior CD unmixing and thus typically results in poor quantification. Some automated methods exist for color stain vector estimation, but most depend on a significant amount of each stain to be present in the whole-slide images. In this paper we propose a method for automatically finding stain color vectors and unmix- ing IHC stained whole slide images, even when some stains are sparsely expressed. We collected 16 tonsil slides and stained them for different periods of time with hematoxylin and a DAB-colored proliferative marker Ki67. RGB pixels of WSI images were converted to the hue saturation density (HSD) color domain and subsequently K-means clustering was used to separate stains and calculate the stain color vectors for each slide. Our results show that staining time affects the stain vectors and that calculating a unique stain vector for each slide results in better unmixing results than using a standard stain vector. Keywords:},
author = {Geijs, D J and {Intezar, M, J.A.W.M. van der Laak}, G.J.S. Litjens},
file = {::},
keywords = {color deconvolution,histopathology,hsd color space,whole-slide imaging},
title = {{Automatic color unmixing of IHC stained whole slide images}}
}
@article{Trahearn2015,
author = {Trahearn, Nicholas and David, Snead and Ian, Cree and Nasir, Rajpoot},
file = {::},
journal = {Proc. of SPIE 9420, Medical Imaging 2015: Digital Pathology},
keywords = {colour deconvolution,independent component analysis,stain separation},
title = {{Multi-class stain separation using independent component analysis}},
url = {http://spie.org/Publications/Proceedings/Paper/10.1117/12.2081933},
year = {2015}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
file = {::},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
month = {dec},
number = {3},
pages = {211--252},
publisher = {Springer New York LLC},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Buda2018,
abstract = {In this study, we systematically investigate the impact of class imbalance on  classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.},
author = {Buda, Mateusz and Maki, Atsuto and Mazurowski, Maciej A},
doi = {10.1016/j.neunet.2018.07.011},
issn = {1879-2782 (Electronic)},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Humans,Machine Learning,Neural Networks, Computer,Probability,ROC Curve,trends},
language = {eng},
month = {oct},
pages = {249--259},
pmid = {30092410},
title = {{A systematic study of the class imbalance problem in convolutional neural networks.}},
volume = {106},
year = {2018}
}
@article{Pathak2015,
abstract = {We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.},
archivePrefix = {arXiv},
arxivId = {1506.03648},
author = {Pathak, Deepak and Kr{\"{a}}henb{\"{u}}hl, Philipp and Darrell, Trevor},
eprint = {1506.03648},
file = {::},
month = {jun},
title = {{Constrained Convolutional Neural Networks for Weakly Supervised Segmentation}},
url = {http://arxiv.org/abs/1506.03648},
year = {2015}
}
@article{Chen2017,
abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
archivePrefix = {arXiv},
arxivId = {1706.05587},
author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
eprint = {1706.05587},
file = {::},
month = {jun},
title = {{Rethinking Atrous Convolution for Semantic Image Segmentation}},
url = {http://arxiv.org/abs/1706.05587},
year = {2017}
}
@article{Chen2018,
abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
archivePrefix = {arXiv},
arxivId = {1806.07366},
author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
eprint = {1806.07366},
file = {::},
title = {{Neural Ordinary Differential Equations}},
url = {http://arxiv.org/abs/1806.07366},
year = {2018}
}
@article{Kutta1901,
address = {Leipzig},
author = {Kutta, Wilhelm},
language = {German},
publisher = {B.G Teubner},
title = {{Beitrag zur näherungsweisen Integration totaler Differentialgleichungen}},
year = {1901}
}
@article{Brugger2019,
abstract = {One of the key drawbacks of 3D convolutional neural networks for segmentation is their memory footprint, which necessitates compromises in the network architecture in order to fit into a given memory budget. Motivated by the RevNet for image classification, we propose a partially reversible U-Net architecture that reduces memory consumption substantially. The reversible architecture allows us to exactly recover each layer's outputs from the subsequent layer's ones, eliminating the need to store activations for backpropagation. This alleviates the biggest memory bottleneck and enables very deep (theoretically infinitely deep) 3D architectures. On the BraTS challenge dataset, we demonstrate substantial memory savings. We further show that the freed memory can be used for processing the whole field-of-view (FOV) instead of patches. Increasing network depth led to higher segmentation accuracy while growing the memory footprint only by a very small fraction, thanks to the partially reversible architecture.},
archivePrefix = {arXiv},
arxivId = {1906.06148},
author = {Br{\"{u}}gger, Robin and Baumgartner, Christian F. and Konukoglu, Ender},
eprint = {1906.06148},
file = {::},
month = {jun},
title = {{A Partially Reversible U-Net for Memory-Efficient Volumetric Image Segmentation}},
url = {http://arxiv.org/abs/1906.06148},
year = {2019}
}
@misc{Howard2019,
author = {{Jeremy Howard}},
title = {{Imagenette: a smaller subset of 10 easily classified classes from Imagenet, and a little more French}},
url = {https://github.com/fastai/imagenette},
urldate = {2019-08-22}
}
@article{Finley1974,
author = {Finley, T N and Engelman, E P and Packer, B and Aronow, A and Cosentino, A M},
journal = {The American review of respiratory disease},
number = {6},
pages = {682--684},
pmid = {4835592},
title = {{Use of the RC time constant for CO in the measurement of diffusing capacity.}},
volume = {109},
year = {1974}
}
@article{Balato2013,
abstract = {BACKGROUND: Psoriasis is a chronic disease which requires long-term therapy. Therefore, adherence to therapy and patient motivation are key points in controlling the disease. Mobile-phone-based interventions, and in particular text messages (TM), have already been used effectively to motivate patients and improve treatment adherence in many different chronic diseases such as diabetes, cardiovascular disease and asthma. OBJECTIVES: To evaluate the use of TM in improving treatment adherence and several patient outcomes such as quality of life, disease severity, patient-perceived disease severity and the patient-physician relationship. PATIENTS AND METHODS: Daily TM, providing reminders and educational tools, were sent for 12 weeks to a group of 20 patients with psoriasis. At the beginning and end of the study the following assessments were performed: Psoriasis Area Severity Index (PASI), Self-Administered Psoriasis Area Severity Index (SAPASI), body surface area (BSA), Physician Global Assessment (PGA), Dermatology Life Quality Index (DLQI), evaluation of patient-physician relationship and adherence to therapy. A matched control group of 20 patients with psoriasis was used for comparison of the same outcomes. RESULTS: Both patient groups had similar scores for PASI, SAPASI, BSA, PGA and DLQI at baseline. However, after 12 weeks the intervention group reported a significantly better improvement of disease severity as well as quality of life, showing lower values of PASI, SAPASI, BSA, PGA and DLQI with respect to the control group (P{\textless}0.05). Moreover, adherence to therapy improved in a statistically significant way (P{\textless}0.001) whereas it remained stable in the control group. Similarly, TM interventions led to an optimization of patient-physician communication. CONCLUSIONS: TM interventions seem to be a very promising tool for the long-term management of patients with psoriasis, leading to an increased compliance to therapy, positive changes in self-care behaviours and better patient-physician relationship allowing improved clinical outcomes and better control of the disease.},
address = {Department of Dermatology, University of Naples Federico II, Via Pansini 5, 80131 Naples, Italy.},
author = {Balato, N and Megna, M and {Di Costanzo}, L and Balato, A and Ayala, F},
journal = {The British journal of dermatology},
month = {jan},
number = {1},
pages = {201--205},
title = {{Educational and motivational support service: a pilot study for mobile-phone-based interventions in patients with psoriasis.}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=23240729{\&}retmode=ref{\&}cmd=prlinks papers3://publication/doi/10.1111/j.1365-2133.2012.11205.x},
volume = {168},
year = {2013}
}
@inproceedings{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Proceedings of the 25th Advances in Neural Information Processing Systems},
file = {::},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://code.google.com/p/cuda-convnet/ http://papers.nips.cc/paper/4824-imagenet-classification-w{\%}5Cnpapers3://publication/uuid/1ECF396A-CEDA-45CD-9A9F-03344449DA2A},
year = {2012}
}
@inproceedings{Tan2019,
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
archivePrefix = {arXiv},
arxivId = {1905.11946},
author = {Tan, Mingxing and Le, Quoc V.},
booktitle = {Proceedings of the 36th International Conference on Machine Learning},
eprint = {1905.11946},
file = {::},
month = {may},
title = {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1905.11946},
year = {2019}
}
@inproceedings{ronneberger2015u,
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {International Conference on Medical image computing and computer-assisted intervention},
organization = {Springer},
pages = {234--241},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
year = {2015}
}
@article{deJong2014,
abstract = {Introductie: De invloed van lichaamshouding tijdens mictie op urodynamische parameters bij mannen kan een klinische significante invloed hebben bij pati{\"{e}}nten met lower urinary tract symptoms (LUTS). Er bestaat echter geen consensus over de effecten van verschillende mictiehoudingen.},
author = {de Jong, Y and Pinckaers, J.H.F.M. and ten Brinck, R M and {{\`{a}} Nijeholt}, A A B},
doi = {10.1007/s13629-014-0008-5},
issn = {2211-4718},
journal = {Tijdschrift voor Urologie},
month = {feb},
number = {1},
pages = {36--42},
title = {{Invloed van mictiehouding op urodynamische parameters bij mannen: een literatuuronderzoek}},
url = {https://doi.org/10.1007/s13629-014-0008-5},
volume = {4},
year = {2014}
}
@inproceedings{Santurkar2018,
abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
archivePrefix = {arXiv},
arxivId = {1805.11604},
author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1805.11604},
file = {::},
issn = {10495258},
pages = {2483--2493},
title = {{How does batch normalization help optimization?}},
url = {https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf},
volume = {2018-Decem},
year = {2018}
}
@inproceedings{Hou2016,
abstract = {Convolutional Neural Networks (CNN) are state-of-theart models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.},
author = {Hou, L and Samaras, D and Kurc, T M and Gao, Y and Davis, J E and Saltz, J H},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.266},
issn = {1063-6919},
keywords = {CNN training,Cancer,EM based method,Image resolution,Neural networks,Predictive models,Robustness,Training,Visualization,WSI,cancer,cancer subtype differentiation,cellular-level visual features,decision fusion model training,discriminative patch automatic location,expectation-maximization based method,glioma classification,image classification,image patch scale,learning (artificial intelligence),medical image processing,neural nets,nonsmall-cell lung carcinoma,optimisation,patch spatial relationships,patch-based CNN,patch-based convolutional neural network,patch-level CNN,patch-level classifier,patch-level prediction aggregation,whole slide tissue image classification},
pages = {2424--2433},
title = {{Patch-Based Convolutional Neural Network for Whole Slide Tissue Image Classification}},
year = {2016}
}
@inproceedings{Graham2019,
abstract = {Analysis of the shape of glands and their lumen in digitised images of Haematoxylin {\&} Eosin stained colon histology slides can provide insight into the degree of malignancy. Segmenting each glandular component is an essential prerequisite step for subsequent automatic morphological analysis. Current automated segmentation approaches typically do not take into account the inherent rotational symmetry within histology images. We incorporate this rotational symmetry into an encoder-decoder based network by utilising group equivariant convolutions, specifically using the symmetry group of rotations by multiples of 90{\$}{\$}{\^{}}$\backslash$circ {\$}{\$}. Our rotation equivariant network splits into two separate branches after the final up-sampling operation, where the output of a given branch achieves either gland or lumen segmentation. In addition, at the output of the gland branch, we use a multi-class strategy to assist with the separation of touching instances. We show that our proposed approach achieves the state-of-the-art performance on the GlaS challenge dataset.},
address = {Cham},
author = {Graham, Simon and Epstein, David and Rajpoot, Nasir},
booktitle = {Digital Pathology},
editor = {Reyes-Aldasoro, Constantino Carlos and Janowczyk, Andrew and Veta, Mitko and Bankhead, Peter and Sirinukunwattana, Korsuk},
isbn = {978-3-030-23937-4},
pages = {109--116},
publisher = {Springer International Publishing},
title = {{Rota-Net: Rotation Equivariant Network for Simultaneous Gland and Lumen Segmentation in Colon Histology Images}},
year = {2019}
}
@article{Wang2019z,
abstract = {Histopathology image analysis serves as the gold standard for cancer diagnosis. Efficient and precise diagnosis is quite critical for the subsequent therapeutic treatment of patients. So far, computer-aided diagnosis has not been widely applied in pathological field yet as currently well-addressed tasks are only the tip of the iceberg. Whole slide image (WSI) classification is a quite challenging problem. First, the scarcity of annotations heavily impedes the pace of developing effective approaches. Pixelwise delineated annotations on WSIs are time consuming and tedious, which poses difficulties in building a large-scale training dataset. In addition, a variety of heterogeneous patterns of tumor existing in high magnification field are actually the major obstacle. Furthermore, a gigapixel scale WSI cannot be directly analyzed due to the immeasurable computational cost. How to design the weakly supervised learning methods to maximize the use of available WSI-level labels that can be readily obtained in clinical practice is quite appealing. To overcome these challenges, we present a weakly supervised approach in this article for fast and effective classification on the whole slide lung cancer images. Our method first takes advantage of a patch-based fully convolutional network (FCN) to retrieve discriminative blocks and provides representative deep features with high efficiency. Then, different context-aware block selection and feature aggregation strategies are explored to generate globally holistic WSI descriptor which is ultimately fed into a random forest (RF) classifier for the image-level prediction. To the best of our knowledge, this is the first study to exploit the potential of image-level labels along with some coarse annotations for weakly supervised learning. A large-scale lung cancer WSI dataset is constructed in this article for evaluation, which validates the effectiveness and feasibility of the proposed method. Extensive experiments demonstrate the superior performance of our method that surpasses the state-of-the-art approaches by a significant margin with an accuracy of 97.3{\%} . In addition, our method also achieves the best performance on the public lung cancer WSIs dataset from The Cancer Genome Atlas (TCGA). We highlight that a small number of coarse annotations can contribute to further accuracy improvement. We believe that weakly supervised learning methods have great potential to assist pathologists in histology image diagnosis in the near future.},
author = {Wang, X and Chen, H and Gan, C and Lin, H and Dou, Q and Tsougenis, E and Huang, Q and Cai, M and Heng, P},
doi = {10.1109/TCYB.2019.2935141},
issn = {2168-2275},
journal = {IEEE Transactions on Cybernetics},
keywords = {Cancer,Deep learning,Feature extraction,Image analysis,Lung,Supervised learning,Task analysis,Tumors,histology image analysis,weakly supervised learning,whole slide images (WSIs)},
pages = {1--13},
title = {{Weakly Supervised Deep Learning for Whole Slide Lung Cancer Image Analysis}},
year = {2019}
}
@misc{Loshchilov2017,
abstract = {We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code is available at https://github.com/loshchil/AdamW-and-SGDW},
archivePrefix = {arXiv},
arxivId = {1711.05101},
author = {Loshchilov, Ilya and Hutter, Frank},
eprint = {1711.05101},
title = {{Fixing Weight Decay Regularization in Adam}},
url = {http://arxiv.org/abs/1711.05101},
year = {2017}
}
@article{Nielsen2010,
abstract = {PURPOSE To compare clinical, immunohistochemical (IHC), and gene expression models of prognosis applicable to formalin-fixed, paraffin-embedded blocks in a large series of estrogen receptor (ER)-positive breast cancers from patients uniformly treated with adjuvant tamoxifen. EXPERIMENTAL DESIGN Quantitative real-time reverse transcription-PCR (qRT-PCR) assays for 50 genes identifying intrinsic breast cancer subtypes were completed on 786 specimens linked to clinical (median follow-up, 11.7 years) and IHC [ER, progesterone receptor (PR), HER2, and Ki67] data. Performance of predefined intrinsic subtype and risk-of-relapse scores was assessed using multivariable Cox models and Kaplan-Meier analysis. Harrell's C-index was used to compare fixed models trained in independent data sets, including proliferation signatures. RESULTS Despite clinical ER positivity, 10{\%} of cases were assigned to nonluminal subtypes. qRT-PCR signatures for proliferation genes gave more prognostic information than clinical assays for hormone receptors or Ki67. In Cox models incorporating standard prognostic variables, hazard ratios for breast cancer disease-specific survival over the first 5 years of follow-up, relative to the most common luminal A subtype, are 1.99 [95{\%} confidence interval (CI), 1.09-3.64] for luminal B, 3.65 (95{\%} CI, 1.64-8.16) for HER2-enriched subtype, and 17.71 (95{\%} CI, 1.71-183.33) for the basal-like subtype. For node-negative disease, PAM50 qRT-PCR-based risk assignment weighted for tumor size and proliferation identifies a group with {\textgreater}95{\%} 10-year survival without chemotherapy. In node-positive disease, PAM50-based prognostic models were also superior. CONCLUSION The PAM50 gene expression test for intrinsic biological subtype can be applied to large series of formalin-fixed, paraffin-embedded breast cancers, and gives more prognostic information than clinical factors and IHC using standard cut points.},
author = {Nielsen, Torsten O. and Parker, Joel S. and Leung, Samuel and Voduc, David and Ebbert, Mark and Vickery, Tammi and Davies, Sherri R. and Snider, Jacqueline and Stijleman, Inge J. and Reed, Jerry and Cheang, Maggie C.U. and Mardis, Elaine R. and Perou, Charles M. and Bernard, Philip S. and Ellis, Matthew J.},
doi = {10.1158/1078-0432.CCR-10-1282},
issn = {1078-0432},
journal = {Clinical Cancer Research},
month = {nov},
number = {21},
pages = {5222--5232},
pmid = {20837693},
title = {{A Comparison of PAM50 Intrinsic Subtyping with Immunohistochemistry and Clinical Prognostic Factors in Tamoxifen-Treated Estrogen Receptor–Positive Breast Cancer}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20837693 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2970720 http://clincancerres.aacrjournals.org/lookup/doi/10.1158/1078-0432.CCR-10-1282},
volume = {16},
year = {2010}
}
@article{Feldman2007,
abstract = {BACKGROUND: The efficacy of topical medications is limited by non-adherence. Interventions to improve adherence to topical treatments are not well characterized. OBJECTIVE: To assess the impact of office visits on patients' adherence to topical treatment. METHODS: Twenty-nine subjects enrolled in a clinical trial for psoriasis and were followed for up to 8 weeks. Subjects were told to apply 6{\%} salicylic acid gel twice daily. Electronic monitors were used to assess adherence. Results were compared to adherence in clinical trials of hand dermatitis and atopic dermatitis. RESULTS: Adherence rates were significantly higher around the time of office visits (P {\textless} .05). LIMITATIONS: This is a small study in a limited patient population. The study was observational and not a randomized trial of the effect of increased office visits. CONCLUSION: Frequent follow-up visits in clinical trials increase patients' adherence to medications. The use of a follow-up visit shortly after initiating treatment may be an effective way to boost patients' use of their medication and achieve better treatment outcomes.},
address = {Center for Dermatology Research, Department of Dermatology, The Ohio State University College of Pharmacy, Columbus, USA. sfeldman@wfubmc.edu},
author = {Feldman, Steven R and Camacho, Fabian T and Krejci-Manwaring, Jennifer and Carroll, Christie L and Balkrishnan, Rajesh},
journal = {Journal of the American Academy of Dermatology},
month = {jul},
number = {1},
pages = {81--83},
title = {{Adherence to topical therapy increases around the time of office visits.}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=17498841{\&}retmode=ref{\&}cmd=prlinks papers3://publication/doi/10.1016/j.jaad.2007.04.005},
volume = {57},
year = {2007}
}
@article{Gertych2015,
abstract = {Computerized evaluation of histological preparations of prostate tissues involves identification of tissue components such as stroma (ST), benign/normal epithelium (BN) and prostate cancer (PCa). Image classification approaches have been developed to identify and classify glandular regions in digital images of prostate tissues; however their success has been limited by difficulties in cellular segmentation and tissue heterogeneity. We hypothesized that utilizing image pixels to generate intensity histograms of hematoxylin (H) and eosin (E) stains deconvoluted from H{\&}E images numerically captures the architectural difference between glands and stroma. In addition, we postulated that joint histograms of local binary patterns and local variance (LBPxVAR) can be used as sensitive textural features to differentiate benign/normal tissue from cancer. Here we utilized a machine learning approach comprising of a support vector machine (SVM) followed by a random forest (RF) classifier to digitally stratify prostate tissue into ST, BN and PCa areas. Two pathologists manually annotated 210 images of low- and high-grade tumors from slides that were selected from 20 radical prostatectomies and digitized at high-resolution. The 210 images were split into the training (n=19) and test (n=191) sets. Local intensity histograms of H and E were used to train a SVM classifier to separate ST from epithelium (BN+PCa). The performance of SVM prediction was evaluated by measuring the accuracy of delineating epithelial areas. The Jaccard J=59.5±14.6 and RandRi=62.0±7.5 indices reported a significantly better prediction when compared to a reference method (Chen et al., Clinical Proteomics 2013, 10:18) based on the averaged values from the test set. To distinguish BN from PCa we trained a RF classifier with LBPxVAR and local intensity histograms and obtained separate performance values for BN and PCa: JBN=35.2±24.9, OBN=49.6±32, JPCa=49.5±18.5, OPCa=72.7±14.8 andRi=60.6±7.6 in the test set. Our pixel-based classification does not rely on the detection of lumens, which is prone to errors and has limitations in high-grade cancers and has the potential to aid in clinical studies in which the quantification of tumor content is necessary to prognosticate the course of the disease. The image data set with ground truth annotation is available for public use to stimulate further research in this area.},
author = {Gertych, Arkadiusz and Ing, Nathan and Ma, Zhaoxuan and Fuchs, Thomas J and Salman, Sadri and Mohanty, Sambit and Bhele, Sanica and Vel{\'{a}}squez-Vacca, Adriana and Amin, Mahul B and Knudsen, Beatrice S},
doi = {10.1016/j.compmedimag.2015.08.002},
isbn = {1879-0771 (Electronic){\$}\backslash{\$}r0895-6111 (Linking)},
issn = {18790771},
journal = {Computerized Medical Imaging and Graphics},
keywords = {Image analysis,Machine learning,Prostate cancer,Tissue classification,Tissue quantification},
pages = {197--208},
pmid = {26362074},
publisher = {Elsevier Ltd},
title = {{Machine learning approaches to analyze histological images of tissues from radical prostatectomies}},
url = {http://dx.doi.org/10.1016/j.compmedimag.2015.08.002},
volume = {46},
year = {2015}
}
@article{Pugliese2016,
author = {Pugliese, Laura and Crowley, Olga and Woodriff, Molly and Lam, Vivian and Sohn, Jeremy and Bradley, Scott},
journal = {Cureus},
month = {mar},
title = {{Feasibility of the “Bring Your Own Device” Model in Clinical Research: Results from a Randomized Controlled Pilot Study of a Mobile Patient Engagement Tool}},
url = {http://readerific.com/exported/?a=67ef1c papers3://publication/uuid/69D149B0-959D-4C4F-AEF3-F3FD2BF26482},
year = {2016}
}
@article{article,
author = {Zhao, Xiaomei and Wu, Yihong and Song, Guidong and li Zhenye and Zhang, Yazhuo and Fan, Yong},
journal = {Medical Image Analysis},
title = {{A deep learning model integrating FCNNs and CRFs for brain tumor segmentation}},
volume = {43},
year = {2017}
}
@article{lu2020data,
archivePrefix = {arXiv},
arxivId = {eess.IV/2004.09666},
author = {Lu, Ming Y and Williamson, Drew F K and Chen, Tiffany Y and Chen, Richard J and Barbieri, Matteo and Mahmood, Faisal},
eprint = {2004.09666},
journal = {arXiv preprint},
primaryClass = {eess.IV},
title = {{Data Efficient and Weakly Supervised Computational Pathology on Whole Slide Images}},
volume = {2004.09666},
year = {2020}
}
@article{Izmailov2018,
abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
archivePrefix = {arXiv},
arxivId = {1803.05407},
author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
eprint = {1803.05407},
file = {::},
journal = {arXiv preprint},
title = {{Averaging Weights Leads to Wider Optima and Better Generalization}},
url = {http://arxiv.org/abs/1803.05407},
volume = {1803.05407},
year = {2018}
}
@article{Zhang2018,
abstract = {Deep learning has transformed the computer vision, natural language processing and speech recognition. However, the following two critical questions are remaining obscure: (1) why deep neural networks generalize better than shallow networks? (2) Does it always hold that a deeper network leads to better performance? Specifically, letting {\$}L{\$} be the number of convolutional and pooling layers in a deep neural network, and {\$}n{\$} be the size of the training sample, we derive the upper bound on the expected generalization error for this network, i.e., $\backslash$begin{\{}eqnarray*{\}} $\backslash$mathbb{\{}E{\}}[R(W)-R{\_}S(W)] $\backslash$leq $\backslash$exp{\{}$\backslash$left(-$\backslash$frac{\{}L{\}}{\{}2{\}}$\backslash$log{\{}$\backslash$frac{\{}1{\}}{\{}$\backslash$eta{\}}{\}}$\backslash$right){\}}$\backslash$sqrt{\{}$\backslash$frac{\{}2$\backslash$sigma{\^{}}2{\}}{\{}n{\}}I(S,W) {\}} $\backslash$end{\{}eqnarray*{\}} where {\$}\backslashsigma {\textgreater}0{\$} is a constant depending on the loss function, {\$}0{\textless}\backslasheta{\textless}1{\$} is a constant depending on the information loss for each convolutional or pooling layer, and {\$}I(S, W){\$} is the mutual information between the training sample {\$}S{\$} and the output hypothesis {\$}W{\$}. This upper bound discovers: (1) As the network increases its number of convolutional and pooling layers {\$}L{\$}, the expected generalization error will decrease exponentially to zero. Layers with strict information loss, such as the convolutional layers, reduce the generalization error of deep learning algorithms. This answers the first question. However, (2) algorithms with zero expected generalization error does not imply a small test error or {\$}\backslashmathbb{\{}E{\}}[R(W)]{\$}. This is because {\$}\backslashmathbb{\{}E{\}}[R{\_}S(W)]{\$} will be large when the information for fitting the data is lost as the number of layers increases. This suggests that the claim "the deeper the better" is conditioned on a small training error or {\$}\backslashmathbb{\{}E{\}}[R{\_}S(W)]{\$}.},
archivePrefix = {arXiv},
arxivId = {1804.09060},
author = {Zhang, Jingwei and Liu, Tongliang and Tao, Dacheng},
eprint = {1804.09060},
file = {::},
title = {{An Information-Theoretic View for Deep Learning}},
year = {2018}
}
@article{Woods2011,
author = {Woods, Craig A and Dumbleton, Kathryn and Jones, Lyndon and Fonn, Desmond},
journal = {Optometry and Vision Science},
month = {feb},
pages = {290--294},
title = {{Patient Use of Smartphones to Communicate Subjective Data in Clinical Trials}},
url = {http://readerific.com/exported/?a=76a210 papers3://publication/uuid/80135FB7-A47B-4E63-9C5B-721420BF32BD},
volume = {88},
year = {2011}
}
@article{Irvin2019,
abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
archivePrefix = {arXiv},
arxivId = {1901.07031},
author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
eprint = {1901.07031},
file = {::},
month = {jan},
title = {{CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison}},
url = {http://arxiv.org/abs/1901.07031},
year = {2019}
}
@article{nguyen2017,
author = {Nguyen, Tan Huu and Sridharan, Shamira and Macias, Virgilia and Kajdacsy-Balla, Andre and Melamed, Jonathan and Do, Minh N and Popescu, Gabriel},
journal = {Journal of biomedical optics},
number = {3},
pages = {36015},
publisher = {International Society for Optics and Photonics},
title = {{Automatic Gleason grading of prostate cancer using quantitative phase imaging and machine learning}},
volume = {22},
year = {2017}
}
@inproceedings{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2017.243},
isbn = {9781538604571},
month = {nov},
pages = {2261--2269},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Densely connected convolutional networks}},
year = {2017}
}
@article{Pinckaers2019,
archivePrefix = {arXiv},
arxivId = {cs.CV/1911.04432},
author = {Pinckaers, Hans and van Ginneken, Bram and Litjens, Geert},
eprint = {1911.04432},
journal = {arXiv preprint},
primaryClass = {cs.CV},
title = {{Streaming convolutional neural networks for end-to-end learning with multi-megapixel images}},
volume = {1911.04432},
year = {2019}
}
@article{VanderLaak2019,
abstract = {A deep-learning model for cancer detection trained on a large number of scanned pathology slides and associated diagnosis labels enables model development without the need for pixel-level annotations.},
author = {van der Laak, Jeroen and Ciompi, Francesco and Litjens, Geert},
doi = {10.1038/s41551-019-0472-6},
issn = {2157846X},
journal = {Nature Biomedical Engineering},
keywords = {Cancer,Computational models,Diagnostic markers,Medical imaging},
month = {oct},
number = {11},
pages = {855--856},
publisher = {Nature Publishing Group},
title = {{No pixel-level annotations needed}},
volume = {3},
year = {2019}
}
@article{Kindermans2017,
abstract = {DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.},
archivePrefix = {arXiv},
arxivId = {1705.05598},
author = {Kindermans, Pieter-Jan and Sch{\"{u}}tt, Kristof T. and Alber, Maximilian and M{\"{u}}ller, Klaus-Robert and Erhan, Dumitru and Kim, Been and D{\"{a}}hne, Sven},
eprint = {1705.05598},
file = {::},
journal = {arXiv preprint},
title = {{Learning how to explain neural networks: PatternNet and PatternAttribution}},
year = {2017}
}
@article{Hackel2018,
abstract = {While CNNs naturally lend themselves to densely sampled data, and sophisticated implementations are available, they lack the ability to efficiently process sparse data. In this work we introduce a suite of tools that exploit sparsity in both the feature maps and the filter weights, and thereby allow for significantly lower memory footprints and computation times than the conventional dense framework when processing data with a high degree of sparsity. Our scheme provides (i) an efficient GPU implementation of a convolution layer based on direct, sparse convolution; (ii) a filter step within the convolution layer, which we call attention, that prevents fill-in, i.e., the tendency of convolution to rapidly decrease sparsity, and guarantees an upper bound on the computational resources; and (iii) an adaptation of the back-propagation algorithm, which makes it possible to combine our approach with standard learning frameworks, while still exploiting sparsity in the data and the model.},
archivePrefix = {arXiv},
arxivId = {1801.10585},
author = {Hackel, Timo and Usvyatsov, Mikhail and Galliani, Silvano and Wegner, Jan D. and Schindler, Konrad},
eprint = {1801.10585},
title = {{Inference, Learning and Attention Mechanisms that Exploit and Preserve Sparsity in Convolutional Networks}},
url = {http://arxiv.org/abs/1801.10585},
year = {2018}
}
@article{DBLP:journals/corr/abs-1802-02212,
archivePrefix = {arXiv},
arxivId = {1802.02212},
author = {Courtiol, Pierre and Tramel, Eric W and Sanselme, Marc and Wainrib, Gilles},
eprint = {1802.02212},
journal = {CoRR},
title = {{Classification and Disease Localization in Histopathology Using Only Global Labels: {\{}A{\}} Weakly-Supervised Approach}},
url = {http://arxiv.org/abs/1802.02212},
volume = {abs/1802.0},
year = {2018}
}
@article{Zintgraf2016,
abstract = {We present a method for visualising the response of a deep neural network to a specific input. For image data for instance our method will highlight areas that provide evidence in favor of, and against choosing a certain class. The method overcomes several shortcomings of previous methods and provides great additional insight into the decision making process of convolutional networks, which is important both to improve models and to accelerate the adoption of such methods in e.g. medicine. In experiments on ImageNet data, we illustrate how the method works and can be applied in different ways to understand deep neural nets.},
archivePrefix = {arXiv},
arxivId = {1603.02518},
author = {Zintgraf, Luisa M. and Cohen, Taco S. and Welling, Max},
eprint = {1603.02518},
file = {::},
title = {{A New Method to Visualize Deep Neural Networks}},
year = {2016}
}
@article{Bandi2019,
abstract = {{\textcopyright} 2018 IEEE. Automated detection of cancer metastases in lymph nodes has the potential to improve the assessment of prognosis for patients. To enable fair comparison between the algorithms for this purpose, we set up the CAMELYON17 challenge in conjunction with the IEEE International Symposium on Biomedical Imaging 2017 Conference in Melbourne. Over 300 participants registered on the challenge website, of which 23 teams submitted a total of 37 algorithms before the initial deadline. Participants were provided with 899 whole-slide images (WSIs) for developing their algorithms. The developed algorithms were evaluated based on the test set encompassing 100 patients and 500 WSIs. The evaluation metric used was a quadratic weighted Cohen's kappa. We discuss the algorithmic details of the 10 best pre-conference and two post-conference submissions. All these participants used convolutional neural networks in combination with pre- and postprocessing steps. Algorithms differed mostly in neural network architecture, training strategy, and pre- and postprocessing methodology. Overall, the kappa metric ranged from 0.89 to -0.13 across all submissions. The best results were obtained with pre-trained architectures such as ResNet. Confusion matrix analysis revealed that all participants struggled with reliably identifying isolated tumor cells, the smallest type of metastasis, with detection rates below 40{\%}. Qualitative inspection of the results of the top participants showed categories of false positives, such as nerves or contamination, which could be targeted for further optimization. Last, we show that simple combinations of the top algorithms result in higher kappa metric values than any algorithm individually, with 0.93 for the best combination.},
author = {B{\'{a}}ndi, P{\'{e}}ter and Geessink, Oscar and Manson, Quirine and {Van Dijk}, Marcory and Balkenhol, Maschenka and Hermsen, Meyke and {Ehteshami Bejnordi}, Babak and Lee, Byungjae and Paeng, Kyunghyun and Zhong, Aoxiao and Li, Quanzheng and Zanjani, Farhad Ghazvinian and Zinger, Svitlana and Fukuta, Keisuke and Komura, Daisuke and Ovtcharov, Vlado and Cheng, Shenghua and Zeng, Shaoqun and Thagaard, Jeppe and Dahl, Anders B. and Lin, Huangjing and Chen, Hao and Jacobsson, Ludwig and Hedlund, Martin and {\c{C}}etin, Melih and Halici, Eren and Jackson, Hunter and Chen, Richard and Both, Fabian and Franke, J{\"{o}}rg and Kusters-Vandevelde, Heidi and Vreuls, Willem and Bult, Peter and {Van Ginneken}, Bram and {Van Der Laak}, Jeroen and Litjens, Geert},
doi = {10.1109/TMI.2018.2867350},
file = {::},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Breast cancer,grand challenge,lymph node metastases,sentinel lymph node,whole-slide images},
month = {feb},
number = {2},
pages = {550--560},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{From Detection of Individual Metastases to Classification of Lymph Node Status at the Patient Level: The CAMELYON17 Challenge}},
volume = {38},
year = {2019}
}
@article{Balkenhol2019,
abstract = {{\textcopyright} 2019, The Author(s), under exclusive licence to United States and Canadian Academy of Pathology. As part of routine histological grading, for every invasive breast cancer the mitotic count is assessed by counting mitoses in the (visually selected) region with the highest proliferative activity. Because this procedure is prone to subjectivity, the present study compares visual mitotic counting with deep learning based automated mitotic counting and fully automated hotspot selection. Two cohorts were used in this study. Cohort A comprised 90 prospectively included tumors which were selected based on the mitotic frequency scores given during routine glass slide diagnostics. This pathologist additionally assessed the mitotic count in these tumors in whole slide images (WSI) within a preselected hotspot. A second observer performed the same procedures on this cohort. The preselected hotspot was generated by a convolutional neural network (CNN) trained to detect all mitotic figures in digitized hematoxylin and eosin (H{\&}E) sections. The second cohort comprised a multicenter, retrospective TNBC cohort (n = 298), of which the mitotic count was assessed by three independent observers on glass slides. The same CNN was applied on this cohort and the absolute number of mitotic figures in the hotspot was compared to the averaged mitotic count of the observers. Baseline interobserver agreement for glass slide assessment in cohort A was good (kappa 0.689; 95{\%} CI 0.580–0.799). Using the CNN generated hotspot in WSI, the agreement score increased to 0.814 (95{\%} CI 0.719–0.909). Automated counting by the CNN in comparison with observers counting in the predefined hotspot region yielded an average kappa of 0.724. We conclude that manual mitotic counting is not affected by assessment modality (glass slides, WSI) and that counting mitotic figures in WSI is feasible. Using a predefined hotspot area considerably improves reproducibility. Also, fully automated assessment of mitotic score appears to be feasible without introducing additional bias or variability.},
author = {Balkenhol, M.C.A. and Tellez, D. and Vreuls, W. and Clahsen, P.C. and Pinckaers, H. and Ciompi, F. and Bult, P. and van der Laak, J.A.W.M.},
doi = {10.1038/s41374-019-0275-0},
issn = {15300307},
journal = {Laboratory Investigation},
title = {{Deep learning assisted mitotic counting for breast cancer}},
year = {2019}
}
@article{Paeng2016,
abstract = {We present a unified framework to predict tumor proliferation scores from breast histopathology whole slide images. Our system offers a fully automated solution to predicting both a molecular data-based, and a mitosis counting-based tumor proliferation score. The framework integrates three modules, each fine-tuned to maximize the overall performance: An image processing component for handling whole slide images, a deep learning based mitosis detection network, and a proliferation scores prediction module. We have achieved 0.567 quadratic weighted Cohen's kappa in mitosis counting-based score prediction and 0.652 F1-score in mitosis detection. On Spearman's correlation coefficient, which evaluates predictive accuracy on the molecular data based score, the system obtained 0.6171. Our approach won first place in all of the three tasks in Tumor Proliferation Assessment Challenge 2016 which is MICCAI grand challenge.},
archivePrefix = {arXiv},
arxivId = {1612.07180},
author = {Paeng, Kyunghyun and Hwang, Sangheum and Park, Sunggyun and Kim, Minsoo},
eprint = {1612.07180},
file = {::},
journal = {CoRR},
month = {dec},
title = {{A Unified Framework for Tumor Proliferation Score Prediction in Breast Histopathology}},
url = {http://arxiv.org/abs/1612.07180},
volume = {abs/1612.0},
year = {2016}
}
@article{Campanella2019,
abstract = {The development of decision support systems for pathology and their deployment in clinical practice have been hindered by the need for large manually annotated datasets. To overcome this problem, we present a multiple instance learning-based deep learning system that uses only the reported diagnoses as labels for training, thereby avoiding expensive and time-consuming pixel-wise manual annotations. We evaluated this framework at scale on a dataset of 44,732 whole slide images from 15,187 patients without any form of data curation. Tests on prostate cancer, basal cell carcinoma and breast cancer metastases to axillary lymph nodes resulted in areas under the curve above 0.98 for all cancer types. Its clinical application would allow pathologists to exclude 65–75{\%} of slides while retaining 100{\%} sensitivity. Our results show that this system has the ability to train accurate classification models at unprecedented scale, laying the foundation for the deployment of computational decision support systems in clinical practice.},
author = {Campanella, Gabriele and Hanna, Matthew G. and Geneslaw, Luke and Miraflor, Allen and {Werneck Krauss Silva}, Vitor and Busam, Klaus J. and Brogi, Edi and Reuter, Victor E. and Klimstra, David S. and Fuchs, Thomas J.},
doi = {10.1038/s41591-019-0508-1},
issn = {1078-8956},
journal = {Nature Medicine},
month = {aug},
number = {8},
pages = {1301--1309},
title = {{Clinical-grade computational pathology using weakly supervised deep learning on whole slide images}},
url = {http://www.nature.com/articles/s41591-019-0508-1},
volume = {25},
year = {2019}
}
@article{Of2018,
abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of ex-ponential moving averages of squared past gradients. It has been empirically ob-served that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous anal-ysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with " long-term memory " of past gradi-ents, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
author = {Of, T H E Convergence},
file = {::},
pages = {1--23},
title = {{On the Convergence of Adam and Beyond}},
year = {2018}
}
@article{Kutta1901a,
author = {Kutta, Wilhelm},
journal = {Zeitschrift f{\"{u}}r Mathematik und Physik},
pages = {435--453},
title = {{Beitrag zur n{\"{a}}herungsweisen Integration totaler Differentialgleichungen}},
volume = {46},
year = {1901}
}
@inproceedings{chen2018encoder,
author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
booktitle = {Proceedings of the European conference on computer vision (ECCV)},
pages = {801--818},
title = {{Encoder-decoder with atrous separable convolution for semantic image segmentation}},
year = {2018}
}
@inproceedings{Gomez2017,
abstract = {Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.},
archivePrefix = {arXiv},
arxivId = {1707.04585},
author = {Gomez, Aidan N. and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
eprint = {1707.04585},
file = {::},
month = {jul},
pages = {2211--2221},
title = {{The Reversible Residual Network: Backpropagation Without Storing Activations}},
url = {http://arxiv.org/abs/1707.04585},
year = {2017}
}
@article{article,
author = {Rissmann, Robert and {H. F. M. Pinckaers}, J and de Man, Andries and Cohen, Adam and {A. Dubois}, E},
journal = {British Journal of Clinical Pharmacology},
pages = {772},
title = {{PHARMACOLOGY E-LEARNING 2.0-WEB VS. APP}},
volume = {78},
year = {2014}
}
@article{Litjens2016,
abstract = {Pathologists face a substantial increase in workload and complexity of histopathologic cancer diagnosis due to the advent of personalized medicine. Therefore, diagnostic protocols have to focus equally on efficiency and accuracy. In this paper we introduce 'deep learning' as a technique to improve the objectivity and efficiency of histopathologic slide analysis. Through two examples, prostate cancer identification in biopsy specimens and breast cancer metastasis detection in sentinel lymph nodes, we show the potential of this new methodology to reduce the workload for pathologists, while at the same time increasing objectivity of diagnoses. We found that all slides containing prostate cancer and micro- and macro-metastases of breast cancer could be identified automatically while 30-40{\%} of the slides containing benign and normal tissue could be excluded without the use of any additional immunohistochemical markers or human intervention. We conclude that 'deep learning' holds great promise to improve the efficacy of prostate cancer diagnosis and breast cancer staging.},
author = {Litjens, Geert and S{\'{a}}nchez, Clara I. and Timofeeva, Nadya and Hermsen, Meyke and Nagtegaal, Iris and Kovacs, Iringo and {Hulsbergen - van de Kaa}, Christina and Bult, Peter and van Ginneken, Bram and van der Laak, Jeroen},
doi = {10.1038/srep26286},
issn = {2045-2322},
journal = {Scientific Reports},
month = {sep},
number = {1},
pages = {26286},
pmid = {27212078},
title = {{Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis}},
url = {http://www.nature.com/articles/srep26286},
volume = {6},
year = {2016}
}
@article{Ianni2020,
abstract = {Standard of care diagnostic procedure for suspected skin cancer is microscopic examination of hematoxylin {\&} eosin stained tissue by a pathologist. Areas of high inter-pathologist discordance and rising biopsy rates necessitate higher efficiency and diagnostic reproducibility. We present and validate a deep learning system which classifies digitized dermatopathology slides into 4 categories. The system is developed using 5,070 images from a single lab, and tested on an uncurated set of 13,537 images from 3 test labs, using whole slide scanners manufactured by 3 different vendors. The system's use of deep-learning-based confidence scoring as a criterion to consider the result as accurate yields an accuracy of up to 98{\%}, and makes it adoptable in a real-world setting. Without confidence scoring, the system achieved an accuracy of 78{\%}. We anticipate that our deep learning system will serve as a foundation enabling faster diagnosis of skin cancer, identification of cases for specialist review, and targeted diagnostic classifications.},
author = {Ianni, Julianna D and Soans, Rajath E and Sankarapandian, Sivaramakrishnan and Chamarthi, Ramachandra Vikas and Ayyagari, Devi and Olsen, Thomas G and Bonham, Michael J and Stavish, Coleman C and Motaparthi, Kiran and Cockerell, Clay J and Feeser, Theresa A and Lee, Jason B},
doi = {10.1038/s41598-020-59985-2},
issn = {2045-2322},
journal = {Scientific Reports},
number = {1},
pages = {3217},
title = {{Tailored for Real-World: A Whole Slide Image Classification System Validated on Uncurated Multi-Site Data Emulating the Prospective Pathology Workload}},
url = {https://doi.org/10.1038/s41598-020-59985-2},
volume = {10},
year = {2020}
}
@article{Zhou2017,
abstract = {In this paper, we propose gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train. Actually, even when gcForest is applied to different data from different domains, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient and scalable. In our experiments its training time running on a PC is comparable to that of deep neural networks running with GPU facilities, and the efficiency advantage may be more apparent because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require large-scale training data, gcForest can work well even when there are only small-scale training data. Moreover, as a tree-based approach, gcForest should be easier for theoretical analysis than deep neural networks.},
archivePrefix = {arXiv},
arxivId = {1702.08835},
author = {Zhou, Zhi-Hua and Feng, Ji},
eprint = {1702.08835},
file = {::},
title = {{Deep Forest: Towards An Alternative to Deep Neural Networks}},
year = {2017}
}
@inproceedings{TanSKZYL18,
author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
booktitle = {Artificial Neural Networks and Machine Learning - {\{}ICANN{\}} 2018 - 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part {\{}III{\}}},
doi = {10.1007/978-3-030-01424-7_27},
pages = {270--279},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{A Survey on Deep Transfer Learning}},
url = {https://doi.org/10.1007/978-3-030-01424-7{\_}27},
volume = {11141},
year = {2018}
}
@article{Masse2018,
abstract = {Humans and most animals can learn new tasks without forgetting old ones. However, training artificial neural networks (ANNs) on new tasks typically cause it to forget previously learned tasks. This phenomenon is the result of "catastrophic forgetting", in which training an ANN disrupts connection weights that were important for solving previous tasks, degrading task performance. Several recent studies have proposed methods to stabilize connection weights of ANNs that are deemed most important for solving a task, which helps alleviate catastrophic forgetting. Here, drawing inspiration from algorithms that are believed to be implemented in vivo, we propose a complementary method: adding a context-dependent gating signal, such that only sparse, mostly non-overlapping patterns of units are active for any one task. This method is easy to implement, requires little computational overhead, and allows ANNs to maintain high performance across large numbers of sequentially presented tasks when combined with weight stabilization. This work provides another example of how neuroscience-inspired algorithms can benefit ANN design and capability.},
archivePrefix = {arXiv},
arxivId = {1802.01569},
author = {Masse, Nicolas Y. and Grant, Gregory D. and Freedman, David J.},
eprint = {1802.01569},
file = {::},
title = {{Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization}},
url = {http://arxiv.org/abs/1802.01569},
year = {2018}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demon-strated impressive classification performance on the ImageNet bench-mark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the oper-ation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
author = {Zeiler, Matthew D and Fergus, Rob},
file = {::},
title = {{LNCS 8689 - Visualizing and Understanding Convolutional Networks}},
year = {2014}
}
@article{Ilse2018,
abstract = {Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.},
archivePrefix = {arXiv},
arxivId = {1802.04712},
author = {Ilse, Maximilian and Tomczak, Jakub M. and Welling, Max},
eprint = {1802.04712},
file = {::},
journal = {arXiv preprint},
title = {{Attention-based Deep Multiple Instance Learning}},
url = {http://arxiv.org/abs/1802.04712},
volume = {1802.04712},
year = {2018}
}
@article{Dong2018,
abstract = {Convolutional neural networks have led to significant breakthroughs in the domain of medical image analysis. However, the task of breast cancer segmentation in whole-slide images (WSIs) is still underexplored. WSIs are large histopathological images with extremely high resolution. Constrained by the hardware and field of view, using high-magnification patches can slow down the inference process and using low-magnification patches can cause the loss of information. In this paper, we aim to achieve two seemingly conflicting goals for breast cancer segmentation: accurate and fast prediction. We propose a simple yet efficient framework Reinforced Auto-Zoom Net (RAZN) to tackle this task. Motivated by the zoom-in operation of a pathologist using a digital microscope, RAZN learns a policy network to decide whether zooming is required in a given region of interest. Because the zoom-in action is selective, RAZN is robust to unbalanced and noisy ground truth labels and can efficiently reduce overfitting. We evaluate our method on a public breast cancer dataset. RAZN outperforms both single-scale and multi-scale baseline approaches, achieving better accuracy at low inference cost.},
archivePrefix = {arXiv},
arxivId = {1807.11113},
author = {Dong, Nanqing and Kampffmeyer, Michael and Liang, Xiaodan and Wang, Zeya and Dai, Wei and Xing, Eric P.},
eprint = {1807.11113},
file = {::},
journal = {CoRR},
title = {{Reinforced Auto-Zoom Net: Towards Accurate and Fast Breast Cancer Segmentation in Whole-slide Images}},
url = {http://arxiv.org/abs/1807.11113},
volume = {abs/1807.1},
year = {2018}
}
@misc{Litjens2018,
abstract = {Background: The presence of lymph node metastases is one of the most important factors in breast cancer prognosis. The most common way to assess regional lymph node status is the sentinel lymph node procedure. The sentinel lymph node is the most likely lymph node to contain metastasized cancer cells and is excised, histopathologically processed, and examined by a pathologist. This tedious examination process is time-consuming and can lead to small metastases being missed. However, recent advances in whole-slide imaging and machine learning have opened an avenue for analysis of digitized lymph node sections with computer algorithms. For example, convolutional neural networks, a type of machine-learning algorithm, can be used to automatically detect cancer metastases in lymph nodes with high accuracy. To train machine-learning models, large, well-curated datasets are needed. Results: We released a dataset of 1,399 annotated whole-slide images (WSIs) of lymph nodes, both with and without metastases, in 3 terabytes of data in the context of the CAMELYON16 and CAMELYON17 Grand Challenges. Slides were collected from five medical centers to cover a broad range of image appearance and staining variations. Each WSI has a slide-level label indicating whether it contains no metastases, macro-metastases, micro-metastases, or isolated tumor cells. Furthermore, for 209 WSIs, detailed hand-drawn contours for all metastases are provided. Last, open-source software tools to visualize and interact with the data have been made available. Conclusions: A unique dataset of annotated, whole-slide digital histopathology images has been provided with high potential for re-use.},
author = {Litjens, Geert and Bandi, Peter and Bejnordi, Babak Ehteshami and Geessink, Oscar and Balkenhol, Maschenka and Bult, Peter and Halilovic, Altuna and Hermsen, Meyke and van de Loo, Rob and Vogels, Rob and Manson, Quirine F. and Stathonikos, Nikolas and Baidoshvili, Alexi and van Diest, Paul and Wauters, Carla and van Dijk, Marcory and van der Laak, Jeroen},
booktitle = {GigaScience},
doi = {10.1093/gigascience/giy065},
file = {::},
issn = {2047217X},
keywords = {Breast cancer,Grand challenge,Lymph node metastases,Sentinel node,Whole-slide images},
month = {jun},
number = {6},
publisher = {Oxford University Press},
title = {{1399 H{\&}E-stained sentinel lymph node sections of breast cancer patients: The CAMELYON dataset}},
volume = {7},
year = {2018}
}
@inproceedings{Katharopoulos2019,
abstract = {Existing deep architectures cannot operate on very large signals such as megapixel images due to computational and memory constraints. To tackle this limitation, we propose a fully differentiable end-to-end trainable model that samples and processes only a fraction of the full resolution input image. The locations to process are sampled from an attention distribution computed from a low resolution view of the input. We refer to our method as attention sampling and it can process images of several megapixels with a standard single GPU setup. We show that sampling from the attention distribution results in an unbiased estimator of the full model with minimal variance, and we derive an unbiased estimator of the gradient that we use to train our model end-to-end with a normal SGD procedure. This new method is evaluated on three classification tasks, where we show that it allows to reduce computation and memory footprint by an order of magnitude for the same accuracy as classical architectures. We also show the consistency of the sampling that indeed focuses on informative parts of the input images.},
archivePrefix = {arXiv},
arxivId = {1905.03711},
author = {Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
booktitle = {Proceedings of the 36th International Conference on Machine Learning},
eprint = {1905.03711},
file = {::},
month = {may},
title = {{Processing Megapixel Images with Deep Attention-Sampling Models}},
url = {http://arxiv.org/abs/1905.03711},
year = {2019}
}
@misc{TheMendeleySupportTeam2010,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2010}
}
@article{Wilson2018,
abstract = {As global efforts accelerate to implement the Sustainable Development Goals and, in particular, universal health coverage, access to high-quality and timely pathology and laboratory medicine (PALM) services will be needed to support health-care systems that are tasked with achieving these goals. This access will be most challenging to achieve in low-income and middle-income countries (LMICs), which have a disproportionately large share of the global burden of disease but a disproportionately low share of global healthWilson, M. L., Fleming, K. A., Kuti, M. A., Looi, L. M., Lago, N., {\&} Ru, K. (2018). Access to pathology and laboratory medicine services: a crucial gap. Lancet (London, England), 391(10133), 1927–1938. https://doi.org/10.1016/S0140-6736(18)30458-6-care resources, particularly PALM services. In this first in a Series of three papers on PALM in LMICs, we describe the crucial and central roles of PALM services in the accurate diagnosis and detection of disease, informing prognosis and guiding treatment, contributing to disease screening, public health surveillance and disease registries, and supporting medical-legal systems. We also describe how, even though data are sparse, these services are of both insufficient scope and inadequate quality to play their key role in health-care systems in LMICs. Lastly, we identify four key barriers to the provision of optimal PALM services in resource-limited settings: insufficient human resources or workforce capacity, inadequate education and training, inadequate infrastructure, and insufficient quality, standards, and accreditation.},
author = {Wilson, Michael L and Fleming, Kenneth A and Kuti, Modupe A and Looi, Lai Meng and Lago, Nestor and Ru, Kun},
doi = {10.1016/S0140-6736(18)30458-6},
file = {::},
issn = {1474-547X},
journal = {Lancet (London, England)},
month = {may},
number = {10133},
pages = {1927--1938},
pmid = {29550029},
publisher = {Elsevier},
title = {{Access to pathology and laboratory medicine services: a crucial gap.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29550029},
volume = {391},
year = {2018}
}
@article{Lucas2019,
abstract = {Histopathologic grading of prostate cancer using Gleason patterns (GPs) is subject to a large inter-observer variability, which may result in suboptimal treatment of patients. With the introduction of digitization and whole-slide images of prostate biopsies, computer-aided grading becomes feasible. Computer-aided grading has the potential to improve histopathological grading and treatment selection for prostate cancer. Automated detection of GPs and determination of the grade groups (GG) using a convolutional neural network. In total, 96 prostate biopsies from 38 patients are annotated on pixel-level. Automated detection of GP 3 and GP ≥ 4 in digitized prostate biopsies is performed by re-training the Inception-v3 convolutional neural network (CNN). The outcome of the CNN is subsequently converted into probability maps of GP ≥ 3 and GP ≥ 4, and the GG of the whole biopsy is obtained according to these probability maps. Differentiation between non-atypical and malignant (GP ≥ 3) areas resulted in an accuracy of 92{\%} with a sensitivity and specificity of 90 and 93{\%}, respectively. The differentiation between GP ≥ 4 and GP ≤ 3 was accurate for 90{\%}, with a sensitivity and specificity of 77 and 94{\%}, respectively. Concordance of our automated GG determination method with a genitourinary pathologist was obtained in 65{\%} ($\kappa$ = 0.70), indicating substantial agreement. A CNN allows for accurate differentiation between non-atypical and malignant areas as defined by GPs, leading to a substantial agreement with the pathologist in defining the GG.},
author = {Lucas, Marit and Jansen, Ilaria and Savci-Heijink, C. Dilara and Meijer, Sybren L. and de Boer, Onno J. and van Leeuwen, Ton G. and de Bruin, Daniel M. and Marquering, Henk A.},
doi = {10.1007/s00428-019-02577-x},
issn = {0945-6317},
journal = {Virchows Archiv},
keywords = {Convolutional neural network,Gleason patterns,Grade groups,Prostate},
month = {jul},
number = {1},
pages = {77--83},
title = {{Deep learning for automatic Gleason pattern classification for grade group determination of prostate biopsies}},
url = {http://link.springer.com/10.1007/s00428-019-02577-x},
volume = {475},
year = {2019}
}
@article{Strom2020,
abstract = {BACKGROUND
An increasing volume of prostate biopsies and a worldwide shortage of urological pathologists puts a strain on pathology departments. Additionally, the high intra-observer and inter-observer variability in grading can result in overtreatment and undertreatment of prostate cancer. To alleviate these problems, we aimed to develop an artificial intelligence (AI) system with clinically acceptable accuracy for prostate cancer detection, localisation, and Gleason grading. 

METHODS
We digitised 6682 slides from needle core biopsies from 976 randomly selected participants aged 50–69 in the Swedish prospective and population-based STHLM3 diagnostic study done between May 28, 2012, and Dec 30, 2014 (ISRCTN84445406), and another 271 from 93 men from outside the study. The resulting images were used to train deep neural networks for assessment of prostate biopsies. The networks were evaluated by predicting the presence, extent, and Gleason grade of malignant tissue for an independent test dataset comprising 1631 biopsies from 246 men from STHLM3 and an external validation dataset of 330 biopsies from 73 men. We also evaluated grading performance on 87 biopsies individually graded by 23 experienced urological pathologists from the International Society of Urological Pathology. We assessed discriminatory performance by receiver operating characteristics and tumour extent predictions by correlating predicted cancer length against measurements by the reporting pathologist. We quantified the concordance between grades assigned by the AI system and the expert urological pathologists using Cohen's kappa. 

FINDINGS
The AI achieved an area under the receiver operating characteristics curve of 0{\textperiodcentered}997 (95{\%} CI 0{\textperiodcentered}994–0{\textperiodcentered}999) for distinguishing between benign (n=910) and malignant (n=721) biopsy cores on the independent test dataset and 0{\textperiodcentered}986 (0{\textperiodcentered}972–0{\textperiodcentered}996) on the external validation dataset (benign n=108, malignant n=222). The correlation between cancer length predicted by the AI and assigned by the reporting pathologist was 0{\textperiodcentered}96 (95{\%} CI 0{\textperiodcentered}95–0{\textperiodcentered}97) for the independent test dataset and 0{\textperiodcentered}87 (0{\textperiodcentered}84–0{\textperiodcentered}90) for the external validation dataset. For assigning Gleason grades, the AI achieved a mean pairwise kappa of 0{\textperiodcentered}62, which was within the range of the corresponding values for the expert pathologists (0{\textperiodcentered}60–0{\textperiodcentered}73). 

INTERPRETATION
An AI system can be trained to detect and grade cancer in prostate needle biopsy samples at a ranking comparable to that of international experts in prostate pathology. Clinical application could reduce pathology workload by reducing the assessment of benign biopsies and by automating the task of measuring cancer length in positive biopsy cores. An AI system with expert-level grading performance might contribute a second opinion, aid in standardising grading, and provide pathology expertise in parts of the world where it does not exist. 

FUNDING
Swedish Research Council, Swedish Cancer Society, Swedish eScience Research Center, EIT Health.},
author = {Str{\"{o}}m, Peter and Kartasalo, Kimmo and Olsson, Henrik and Solorzano, Leslie and Delahunt, Brett and Berney, Daniel M and Bostwick, David G and Evans, Andrew J and Grignon, David J and Humphrey, Peter A and Iczkowski, Kenneth A and Kench, James G and Kristiansen, Glen and van der Kwast, Theodorus H and Leite, Katia R M and McKenney, Jesse K and Oxley, Jon and Pan, Chin-Chen and Samaratunga, Hemamali and Srigley, John R and Takahashi, Hiroyuki and Tsuzuki, Toyonori and Varma, Murali and Zhou, Ming and Lindberg, Johan and Lindskog, Cecilia and Ruusuvuori, Pekka and W{\"{a}}hlby, Carolina and Gr{\"{o}}nberg, Henrik and Rantalainen, Mattias and Egevad, Lars and Eklund, Martin},
doi = {10.1016/S1470-2045(19)30738-7},
file = {::},
issn = {1470-2045},
journal = {The Lancet Oncology},
month = {feb},
number = {2},
pages = {222--232},
publisher = {Elsevier},
title = {{Artificial intelligence for diagnosis and grading of prostate cancer in biopsies: a population-based, diagnostic study}},
url = {https://www.sciencedirect.com/science/article/pii/S1470204519307387?via{\%}3Dihub},
volume = {21},
year = {2020}
}
@article{Hestness,
abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents—the "steepness" of the learning curve—yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and {Mostofa Ali Patwary}, Md and Yang, Yang and Zhou, Yanqi and Research, Baidu},
file = {::},
title = {{DEEP LEARNING SCALING IS PREDICTABLE, EMPIRICALLY}}
}
@inproceedings{Zhang2019a,
abstract = {Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and average-pooling, ignore the sampling theorem. The well-known signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks leads to performance degradation; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling. The technique is general and can be incorporated across layer types and applications, such as image classification and conditional image generation. In addition to increased shift-invariance, we also observe, surprisingly, that anti-aliasing boosts accuracy in ImageNet classification, across several commonly-used architectures. This indicates that anti-aliasing serves as effective regularization. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks. Code and anti-aliased versions of popular networks will be made available at $\backslash$url{\{}https://richzhang.github.io/antialiased-cnns/{\}} .},
archivePrefix = {arXiv},
arxivId = {1904.11486},
author = {Zhang, Richard},
booktitle = {Proceedings of the 36th International Conference on Machine Learning},
eprint = {1904.11486},
file = {::},
month = {apr},
title = {{Making Convolutional Networks Shift-Invariant Again}},
url = {http://arxiv.org/abs/1904.11486},
year = {2019}
}
@inproceedings{Mahendran2015,
abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
archivePrefix = {arXiv},
arxivId = {1412.0035},
author = {Mahendran, Aravindh and Vedaldi, Andrea},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7299155},
eprint = {1412.0035},
file = {::},
isbn = {9781467369640},
issn = {10636919},
pmid = {903},
title = {{Understanding deep image representations by inverting them}},
year = {2015}
}
@article{Lakshminarayanan2016,
abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1612.01474},
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
eprint = {1612.01474},
file = {::},
number = {Nips},
title = {{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}},
url = {http://arxiv.org/abs/1612.01474},
year = {2016}
}
@inproceedings{Krahenbuhl2015,
abstract = {Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.},
archivePrefix = {arXiv},
arxivId = {1511.06856},
author = {Kr{\"{a}}henb{\"{u}}hl, Philipp and Doersch, Carl and Donahue, Jeff and Darrell, Trevor},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1511.06856},
file = {::},
month = {nov},
title = {{Data-dependent Initializations of Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1511.06856},
year = {2016}
}
@article{Pinheiro2014,
abstract = {We are interested in inferring object segmentation by leveraging only object class information, and by considering only minimal priors on the object segmentation task. This problem could be viewed as a kind of weakly supervised segmentation task, and naturally fits the Multiple Instance Learning (MIL) framework: every training image is known to have (or not) at least one pixel corresponding to the image class label, and the segmentation task can be rewritten as inferring the pixels belonging to the class of the object (given one image, and its object class). We propose a Convolutional Neural Network-based model, which is constrained during training to put more weight on pixels which are important for classifying the image. We show that at test time, the model has learned to discriminate the right pixels well enough, such that it performs very well on an existing segmentation benchmark, by adding only few smoothing priors. Our system is trained using a subset of the Imagenet dataset and the segmentation experiments are performed on the challenging Pascal VOC dataset (with no fine-tuning of the model on Pascal VOC). Our model beats the state of the art results in weakly supervised object segmentation task by a large margin. We also compare the performance of our model with state of the art fully-supervised segmentation approaches.},
archivePrefix = {arXiv},
arxivId = {1411.6228},
author = {Pinheiro, Pedro O. and Collobert, Ronan},
eprint = {1411.6228},
file = {::},
month = {nov},
title = {{From Image-level to Pixel-level Labeling with Convolutional Networks}},
url = {http://arxiv.org/abs/1411.6228},
year = {2014}
}
@article{Epstein2010,
abstract = {Purpose: An update is provided of the Gleason grading system, which has evolved significantly since its initial description. Materials and Methods: A search was performed using the MEDLINE{\textregistered} database and referenced lists of relevant studies to obtain articles concerning changes to the Gleason grading system. Results: Since the introduction of the Gleason grading system more than 40 years ago many aspects of prostate cancer have changed, including prostate specific antigen testing, transrectal ultrasound guided prostate needle biopsy with greater sampling, immunohistochemistry for basal cells that changed the classification of prostate cancer and new prostate cancer variants. The system was updated at a 2005 consensus conference of international experts in urological pathology, under the auspices of the International Society of Urological Pathology. Gleason score 2-4 should rarely if ever be diagnosed on needle biopsy, certain patterns (ie poorly formed glands) originally considered Gleason pattern 3 are now considered Gleason pattern 4 and all cribriform cancer should be graded pattern 4. The grading of variants and subtypes of acinar adenocarcinoma of the prostate, including cancer with vacuoles, foamy gland carcinoma, ductal adenocarcinoma, pseudohyperplastic carcinoma and small cell carcinoma have also been modified. Other recent issues include reporting secondary patterns of lower and higher grades when present to a limited extent, and commenting on tertiary grade patterns which differ depending on whether the specimen is from needle biopsy or radical prostatectomy. Whereas there is little debate on the definition of tertiary pattern on needle biopsy, this issue is controversial in radical prostatectomy specimens. Although tertiary Gleason patterns are typically added to pathology reports, they are routinely omitted in practice since there is no simple way to incorporate them in predictive nomograms/tables, research studies and patient counseling. Thus, a modified radical prostatectomy Gleason scoring system was recently proposed to incorporate tertiary Gleason patterns in an intuitive fashion. For needle biopsy with different cores showing different grades, the current recommendation is to report the grades of each core separately, whereby the highest grade tumor is selected as the grade of the entire case to determine treatment, regardless of the percent involvement. After the 2005 consensus conference several studies confirmed the superiority of the modified Gleason system as well as its impact on urological practice. Conclusions: It is remarkable that nearly 40 years after its inception the Gleason grading system remains one of the most powerful prognostic factors for prostate cancer. This system has remained timely because of gradual adaptations by urological pathologists to accommodate the changing practice of medicine. {\textcopyright} 2010 American Urological Association.},
author = {Epstein, Jonathan I.},
doi = {10.1016/j.juro.2009.10.046},
issn = {0022-5347},
journal = {Journal of Urology},
keywords = {adenocarcinoma,carcinoma,prostate,prostatic neoplasms,small cell,surgical pathology},
month = {feb},
number = {2},
pages = {433--440},
title = {{An Update of the Gleason Grading System}},
url = {http://www.jurology.com/doi/10.1016/j.juro.2009.10.046},
volume = {183},
year = {2010}
}
@article{Amores2013,
abstract = {Multiple Instance Learning (MIL) has become an important topic in the pattern recognition community, and many solutions to this problem have been proposed until now. Despite this fact, there is a lack of comparative studies that shed light into the characteristics and behavior of the different methods. In this work we provide such an analysis focused on the classification task (i.e., leaving out other learning tasks such as regression). In order to perform our study, we implemented fourteen methods grouped into three different families. We analyze the performance of the approaches across a variety of well-known databases, and we also study their behavior in synthetic scenarios in order to highlight their characteristics. As a result of this analysis, we conclude that methods that extract global bag-level information show a clearly superior performance in general. In this sense, the analysis permits us to understand why some types of methods are more successful than others, and it permits us to establish guidelines in the design of new MIL methods. {\textcopyright} 2013 Elsevier B.V.},
author = {Amores, Jaume},
doi = {10.1016/j.artint.2013.06.003},
file = {::},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Bag-of-Words,Codebook,Multi-instance learning},
month = {aug},
pages = {81--105},
publisher = {Elsevier},
title = {{Multiple instance classification: Review, taxonomy and comparative study}},
volume = {201},
year = {2013}
}
@article{Eidnes,
abstract = {We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.},
author = {Eidnes, Lars H and Norway, Trondheim and N{\o}kland, Arild},
file = {::},
title = {{Shifting Mean Activation Towards Zero with Bipolar Activation Functions}},
url = {https://arxiv.org/pdf/1709.04054.pdf}
}
@article{Courtiol2018,
abstract = {Analysis of histopathology slides is a critical step for many diagnoses, and in particular in oncology where it defines the gold standard. In the case of digital histopathological analysis, highly trained pathologists must review vast whole-slide-images of extreme digital resolution ({\$}100,000{\^{}}2{\$} pixels) across multiple zoom levels in order to locate abnormal regions of cells, or in some cases single cells, out of millions. The application of deep learning to this problem is hampered not only by small sample sizes, as typical datasets contain only a few hundred samples, but also by the generation of ground-truth localized annotations for training interpretable classification and segmentation models. We propose a method for disease localization in the context of weakly supervised learning, where only image-level labels are available during training. Even without pixel-level annotations, we are able to demonstrate performance comparable with models trained with strong annotations on the Camelyon-16 lymph node metastases detection challenge. We accomplish this through the use of pre-trained deep convolutional networks, feature embedding, as well as learning via top instances and negative evidence, a multiple instance learning technique from the field of semantic segmentation and object detection.},
archivePrefix = {arXiv},
arxivId = {1802.02212},
author = {Courtiol, Pierre and Tramel, Eric W. and Sanselme, Marc and Wainrib, Gilles},
eprint = {1802.02212},
file = {::},
journal = {arXiv preprint},
title = {{Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach}},
url = {http://arxiv.org/abs/1802.02212},
volume = {1802.02212},
year = {2018}
}
@article{Runge1895,
author = {Runge, C},
journal = {Mathematische Annalen},
pages = {167--178},
title = {{Ueber die numerische Aufl{\"{o}}sung von Differentialgleichungen.}},
url = {http://eudml.org/doc/157756},
volume = {46},
year = {1895}
}
@inproceedings{Szegedy2015,
author = {Szegedy, C and {Wei Liu} and {Yangqing Jia} and Sermanet, P and Reed, S and Anguelov, D and Erhan, D and Vanhoucke, V and Rabinovich, A},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7298594},
keywords = {Computer architecture,Computer vision,Convolutional codes,Hebbian learning,Hebbian principle,Neural networks,Object detection,Sparse matrices,Visualization,architectural decision,convolution,convolutional neural network architecture,decision making,feature extraction,image classification,neural net architecture,object classification,object detection,resource allocation,resource utilization},
pages = {1--9},
title = {{Going deeper with convolutions}},
year = {2015}
}
@article{Fallis2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
@article{Xu2016,
abstract = {Epithelial (EP) and stromal (ST) are two types of tissues in histological images. Automated segmentation or classification of EP and ST tissues is important when developing computerized system for analyzing the tumor microenvironment. In this paper, a Deep Convolutional Neural Networks (DCNN) based feature learning is presented to automatically segment or classify EP and ST regions from digitized tumor tissue microarrays (TMAs). Current approaches are based on handcraft feature representation, such as color, texture, and Local Binary Patterns (LBP) in classifying two regions. Compared to handcrafted feature based approaches, which involve task dependent representation, DCNN is an end-to-end feature extractor that may be directly learned from the raw pixel intensity value of EP and ST tissues in a data driven fashion. These high-level features contribute to the construction of a supervised classifier for discriminating the two types of tissues. In this work we compare DCNN based models with three handcraft feature extraction based approaches on two different datasets which consist of 157 Hematoxylin and Eosin (H{\&}E) stained images of breast cancer and 1376 immunohistological (IHC) stained images of colorectal cancer, respectively. The DCNN based feature learning approach was shown to have a F1 classification score of 85{\%}, 89{\%}, and 100{\%}, accuracy (ACC) of 84{\%}, 88{\%}, and 100{\%}, and Matthews Correlation Coefficient (MCC) of 86{\%}, 77{\%}, and 100{\%} on two H{\&}E stained (NKI and VGH) and IHC stained data, respectively. Our DNN based approach was shown to outperform three handcraft feature extraction based approaches in terms of the classification of EP and ST regions.},
author = {Xu, Jun and Luo, Xiaofei and Wang, Guanhao and Gilmore, Hannah and Madabhushi, Anant},
doi = {10.1016/j.neucom.2016.01.034},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Breast histopathology,Colorectal cancer,Deep Convolutional Neural Networks,Feature representation,Regions,The classification of epithelial and stromal},
pages = {214--223},
pmid = {28154470},
publisher = {Elsevier},
title = {{A Deep Convolutional Neural Network for segmenting and classifying epithelial and stromal regions in histopathological images}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.01.034},
volume = {191},
year = {2016}
}
@article{Zagoruyko2016,
abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
archivePrefix = {arXiv},
arxivId = {1605.07146},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
eprint = {1605.07146},
file = {::},
month = {may},
title = {{Wide Residual Networks}},
url = {http://arxiv.org/abs/1605.07146},
year = {2016}
}
@inproceedings{Marneffe2014,
abstract = {Revisiting the now de facto standard Stanford dependency representation, we propose an improved taxonomy to capture grammatical relations across languages, including morphologically rich ones. We suggest a two-layered taxonomy: a set of broadly attested universal grammatical relations, to which language-specific relations can be added. We emphasize the lexicalist stance of the Stanford Dependencies, which leads to a particular, partially new treatment of compounding, prepositions, and morphology. We show how existing dependency schemes for several languages map onto the universal taxonomy proposed here and close with consideration of practical implications of dependency representation choices for NLP applications, in particular parsing.},
author = {Marneffe, MC de Marie-catherine De and Dozat, Timothy and Silveira, Natalia and Haverinen, Katri and Ginter, Filip and Nivre, Joakim and Manning, Christopher D},
booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)},
isbn = {978-2-9517408-8-4},
keywords = {dependency grammar,grammatical taxonomy,stanford dependencies},
title = {{Universal Stanford Dependencies: a Cross-Linguistic Typology}},
year = {2014}
}
@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
file = {::},
isbn = {9781467388504},
issn = {10636919},
month = {dec},
pages = {770--778},
publisher = {IEEE Computer Society},
title = {{Deep residual learning for image recognition}},
volume = {2016},
year = {2016}
}
@article{Bulten2020,
abstract = {Background: The Gleason score is the strongest correlating predictor of recurrence for prostate cancer, but has substantial inter-observer variability, limiting its usefulness for individual patients. Specialised urological pathologists have greater concordance; however, such expertise is not widely available. Prostate cancer diagnostics could thus benefit from robust, reproducible Gleason grading. We aimed to investigate the potential of deep learning to perform automated Gleason grading of prostate biopsies. Methods: In this retrospective study, we developed a deep-learning system to grade prostate biopsies following the Gleason grading standard. The system was developed using randomly selected biopsies, sampled by the biopsy Gleason score, from patients at the Radboud University Medical Center (pathology report dated between Jan 1, 2012, and Dec 31, 2017). A semi-automatic labelling technique was used to circumvent the need for manual annotations by pathologists, using pathologists' reports as the reference standard during training. The system was developed to delineate individual glands, assign Gleason growth patterns, and determine the biopsy-level grade. For validation of the method, a consensus reference standard was set by three expert urological pathologists on an independent test set of 550 biopsies. Of these 550, 100 were used in an observer experiment, in which the system, 13 pathologists, and two pathologists in training were compared with respect to the reference standard. The system was also compared to an external test dataset of 886 cores, which contained 245 cores from a different centre that were independently graded by two pathologists. Findings: We collected 5759 biopsies from 1243 patients. The developed system achieved a high agreement with the reference standard (quadratic Cohen's kappa 0{\textperiodcentered}918, 95{\%} CI 0{\textperiodcentered}891–0{\textperiodcentered}941) and scored highly at clinical decision thresholds: benign versus malignant (area under the curve 0{\textperiodcentered}990, 95{\%} CI 0{\textperiodcentered}982–0{\textperiodcentered}996), grade group of 2 or more (0{\textperiodcentered}978, 0{\textperiodcentered}966–0{\textperiodcentered}988), and grade group of 3 or more (0{\textperiodcentered}974, 0{\textperiodcentered}962–0{\textperiodcentered}984). In an observer experiment, the deep-learning system scored higher (kappa 0{\textperiodcentered}854) than the panel (median kappa 0{\textperiodcentered}819), outperforming 10 of 15 pathologist observers. On the external test dataset, the system obtained a high agreement with the reference standard set independently by two pathologists (quadratic Cohen's kappa 0{\textperiodcentered}723 and 0{\textperiodcentered}707) and within inter-observer variability (kappa 0{\textperiodcentered}71). Interpretation: Our automated deep-learning system achieved a performance similar to pathologists for Gleason grading and could potentially contribute to prostate cancer diagnosis. The system could potentially assist pathologists by screening biopsies, providing second opinions on grade group, and presenting quantitative measurements of volume percentages. Funding: Dutch Cancer Society.},
author = {Bulten, Wouter and Pinckaers, Hans and van Boven, Hester and Vink, Robert and de Bel, Thomas and van Ginneken, Bram and van der Laak, Jeroen and {Hulsbergen-van de Kaa}, Christina and Litjens, Geert},
doi = {10.1016/S1470-2045(19)30739-9},
issn = {14702045},
journal = {The Lancet Oncology},
month = {feb},
number = {2},
pages = {233--241},
title = {{Automated deep-learning system for Gleason grading of prostate cancer using biopsies: a diagnostic study}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1470204519307399},
volume = {21},
year = {2020}
}
@article{Sabour,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
file = {::},
title = {{Dynamic Routing Between Capsules}}
}
@incollection{NIPS2019_9427,
author = {Dauphin, Yann N and Schoenholz, Samuel},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
pages = {12645--12657},
publisher = {Curran Associates, Inc.},
title = {{MetaInit: Initializing learning by learning to initialize}},
url = {http://papers.nips.cc/paper/9427-metainit-initializing-learning-by-learning-to-initialize.pdf},
year = {2019}
}
@article{Campanella2018,
abstract = {In the field of computational pathology, the use of decision support systems powered by state-of-the-art deep learning solutions has been hampered by the lack of large labeled datasets. Until recently, studies relied on datasets in the order of few hundreds of slides which are not enough to train a model that can work at scale in the clinic. Here, we have gathered a dataset consisting of 12,160 slides, two orders of magnitude larger than previous datasets in pathology and equivalent to 25 times the pixel count of the entire ImageNet dataset. Given the size of our dataset it is possible for us to train a deep learning model under the Multiple Instance Learning (MIL) assumption where only the overall slide diagnosis is necessary for training, avoiding all the expensive pixel-wise annotations that are usually part of supervised learning approaches. We test our framework on a complex task, that of prostate cancer diagnosis on needle biopsies. We performed a thorough evaluation of the performance of our MIL pipeline under several conditions achieving an AUC of 0.98 on a held-out test set of 1,824 slides. These results open the way for training accurate diagnosis prediction models at scale, laying the foundation for decision support system deployment in the clinic.},
archivePrefix = {arXiv},
arxivId = {1805.06983},
author = {Campanella, Gabriele and Silva, Vitor Werneck Krauss and Fuchs, Thomas J.},
eprint = {1805.06983},
file = {::},
month = {may},
title = {{Terabyte-scale Deep Multiple Instance Learning for Classification and Localization in Pathology}},
url = {http://arxiv.org/abs/1805.06983},
year = {2018}
}
@inproceedings{Couture2018,
abstract = {Multiple instance (MI) learning with a convolutional neural network enables end-to-end training in the presence of weak image-level labels. We propose a new method for aggregating predictions from smaller regions of the image into an image-level classification by using the quantile function. The quantile function provides a more complete description of the heterogeneity within each image, improving image-level classification. We also adapt image augmentation to the MI framework by randomly selecting cropped regions on which to apply MI aggregation during each epoch of training. This provides a mechanism to study the importance of MI learning. We validate our method on five different classification tasks for breast tumor histology and provide a visualization method for interpreting local image classifications that could lead to future insights into tumor heterogeneity.},
address = {Cham},
author = {Couture, Heather D and Marron, J S and Perou, Charles M and Troester, Melissa A and Niethammer, Marc},
booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018},
editor = {Frangi, Alejandro F and Schnabel, Julia A and Davatzikos, Christos and Alberola-L{\'{o}}pez, Carlos and Fichtinger, Gabor},
isbn = {978-3-030-00934-2},
pages = {254--262},
publisher = {Springer International Publishing},
title = {{Multiple Instance Learning for Heterogeneous Images: Training a CNN for Histopathology}},
year = {2018}
}
@article{Loshchilov2016,
abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14{\%} and 16.21{\%}, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
archivePrefix = {arXiv},
arxivId = {1608.03983},
author = {Loshchilov, Ilya and Hutter, Frank},
doi = {10.1002/fut},
eprint = {1608.03983},
file = {::},
title = {{SGDR: Stochastic Gradient Descent with Warm Restarts}},
year = {2016}
}
@article{Shi2019,
author = {Shi, Jun and Zheng, Xiao and Wu, Jinjie and Gong, Bangming and Zhang, Qi and Ying, Shihui},
doi = {10.1016/j.patcog.2018.12.013},
issn = {00313203},
journal = {Pattern Recognition},
month = {may},
pages = {67--76},
title = {{Quaternion Grassmann average network for learning representation of histopathological image}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320318304345},
volume = {89},
year = {2019}
}
@article{Devalla2018,
abstract = {Given that the neural and connective tissues of the optic nerve head (ONH) exhibit complex morphological changes with the development and progression of glaucoma, their simultaneous isolation from optical coherence tomography (OCT) images may be of great interest for the clinical diagnosis and management of this pathology. A deep learning algorithm was designed and trained to digitally stain (i.e. highlight) 6 ONH tissue layers by capturing both the local (tissue texture) and contextual information (spatial arrangement of tissues). The overall dice coefficient (mean of all tissues) was {\$}0.91 \backslashpm 0.05{\$} when assessed against manual segmentations performed by an expert observer. We offer here a robust segmentation framework that could be extended for the automated parametric study of the ONH tissues.},
archivePrefix = {arXiv},
arxivId = {1803.00232},
author = {Devalla, Sripad Krishna and Renukanand, Prajwal K. and Sreedhar, Bharathwaj K. and Perera, Shamira and Mari, Jean-Martial and Chin, Khai Sing and Tun, Tin A. and Strouthidis, Nicholas G. and Aung, Tin and Thiery, Alexandre H. and Girard, Michael J. A.},
doi = {10.1167/iovs.17-22617},
eprint = {1803.00232},
file = {::},
isbn = {1803.00232},
issn = {1552-5783},
journal = {Invest. Ophthalmol. Vis. Sci.},
keywords = {body tissue,glaucoma,learning,optic disk,optical coherence tomography,training},
number = {1},
pages = {63--74},
pmid = {29313052},
publisher = {The Association for Research in Vision and Ophthalmology},
title = {{DRUNET: A Dilated-Residual U-Net Deep Learning Network to Digitally Stain Optic Nerve Head Tissues in Optical Coherence Tomography Images}},
url = {http://arxiv.org/abs/1803.00232},
volume = {59},
year = {2018}
}
@article{Chen2016a,
abstract = {We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences.},
archivePrefix = {arXiv},
arxivId = {1604.06174},
author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
eprint = {1604.06174},
file = {::},
journal = {arXiv preprint},
month = {apr},
title = {{Training Deep Nets with Sublinear Memory Cost}},
url = {http://arxiv.org/abs/1604.06174},
volume = {1604.06174},
year = {2016}
}
@incollection{Chen2014,
abstract = {Immunohistochemistry (IHC) staining is a widely used tech-nique in the diagnosis of abnormal cells such as cancer. For instance, it can be used to determine the distribution and localization of the differen-tially expressed biomarkers of immune cells (such as T-cells or B-cells) in cancerous tissue for an immune response study. Typically, the immuno-logical data of interest includes the type, density and location of the immune cells within the tumor samples; this data is of particular inter-est to pathologists for accurate patient survival prediction. However, to manually count each subset of immune cells under a bright-field micro-scope for each piece of IHC stained tissue is usually extremely tedious and time consuming. This makes automatic detection very attractive, but it can be very challenging due to the wide variety of cell appear-ances resulting from different tissue types, block cuttings, and staining processes. This paper presents a novel method for automatic immune cell counting on digitally scanned images of IHC stained slides. The method first uses a sparse color unmixing technique to separate the IHC image into multiple color channels that correspond to different cell structures. Since the immune cell biomarkers that we are interested in are mem-brane markers, the detection problem is formulated into a deep learning framework using the membrane image channel. The algorithm is evalu-ated on a clinical data set containing a large number of IHC slides and demonstrates more effective detection than the existing technique and the result is also in accordance with the human observer's output.},
annote = {Interesting, they use unmixing of the stains before training neural networkk
C.},
author = {Chen, Ting and Chefd'hotel, Christophe},
doi = {10.1007/978-3-319-10581-9_3},
file = {::},
isbn = {978-3-319-10581-9},
pages = {17--24},
title = {{Deep Learning Based Automatic Immune Cell Detection for Immunohistochemistry Images}},
url = {https://pdfs.semanticscholar.org/e47c/3012e443d3bd7b0e5630427595e765c4b21f.pdf http://link.springer.com/10.1007/978-3-319-10581-9{\_}3},
year = {2014}
}
@inproceedings{Naik2007,
abstract = {Abstract—In this paper we present a method of automatically detecting and segmenting glands in digitized images of prostate histology and to use features derived from gland morphology to distinguish between intermediate Gleason grades. Gleason grading is a method of describing prostate cancer malignancy on a numerical scale from grade 1 (early stage cancer) through grade 5 (highly infiltrative cancer). Studies have shown that gland morphology plays a significant role in discriminating Gleason grades. We present a method of automated detection and segmentation of prostate gland regions. A Bayesian classifier is used to detect candidate gland regions by utilizing low-level image features to find the lumen, epithelial cell cytoplasm, and epithelial nuclei of the tissue. False positive regions identified as glands are eliminated via use of domain-specific knowledge constraints. Following candidate region detection via low-level and empirical domain information, the lumen area is used to initialize a level-set curve, which is evolved to lie at the interior boundary of the nuclei surrounding the gland structure. Features are calculated from the boundaries that characterize the morphology of the lumen and the gland regions, including area overlap ratio, distance ratio, standard deviation and variance of distance, perimeter ratio, compactness, smoothness, and area. The feature space is reduced using a manifold learning scheme (Graph Embedding) that is used to embed objects that are adjacent to each other in the high dimensional feature space into a lower dimensional embedding space. Objects embedded in this low dimensional embedding space are then classified via a support vector machine (SVM) classifier as belonging to Gleason grade 3, grade 4 cancer, or benign epithelium. We evaluate the efficacy of the automated segmentation algorithm by comparing the classification accuracy obtained using the automated segmentation scheme to the accuracy obtained via a user assisted segmentation scheme. Using the automated scheme, the system achieves accuracies of 86.35{\%} when distinguishing Gleason grade 3 from benign epithelium, 92.90{\%} distinguishing grade 4 from benign epithelium, and 95.19{\%} distinguishing between Gleason grades 3 and 4. The manual scheme returns accuracies of 95.14{\%}, 95.35{\%}, and 80.76{\%} for the respective classification tasks, indicating that the automated segmentation algorithm and the manual scheme are comparable in terms of achieving the overall objective of grade classificatio},
author = {Naik, Shivang and Doyle, Scott and Feldman, Michael and Tomaszewski, John and Madabhushi, Anant},
booktitle = {Proceedings of 2nd Workshop on Microsopic Image Analysis with Applications in Biology},
pages = {1--8},
title = {{Gland Segmentation and Computerized Gleason Grading of Prostate Histology by Integrating Low- , High-level and Domain Specific Information.}},
year = {2007}
}
@article{micikevicius_mixed,
abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
annote = {arXiv: 1710.03740},
author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
file = {:Users/Hans/Zotero/storage/XG5WIHQ7/Micikevicius et al. - 2018 - Mixed Precision Training.pdf:pdf;:Users/Hans/Zotero/storage/ZX99KUZN/1710.html:html},
journal = {arXiv preprint},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
month = {feb},
title = {{Mixed Precision Training}},
url = {http://arxiv.org/abs/1710.03740},
volume = {1710.03740},
year = {2017}
}
