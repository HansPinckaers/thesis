<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Introduction</title>
  <style>
html {
line-height: 1.5;
font-family: Georgia, serif;
font-size: 20px;
color: #1a1a1a;
background-color: #fdfdfd;
}
body {
margin: 0 auto;
max-width: 36em;
padding-left: 50px;
padding-right: 50px;
padding-top: 50px;
padding-bottom: 50px;
hyphens: auto;
overflow-wrap: break-word;
text-rendering: optimizeLegibility;
font-kerning: normal;
}
@media (max-width: 600px) {
body {
font-size: 0.9em;
padding: 1em;
}
h1 {
font-size: 1.8em;
}
}
@media print {
body {
background-color: transparent;
color: black;
font-size: 12pt;
}
p, h2, h3 {
orphans: 3;
widows: 3;
}
h2, h3, h4 {
page-break-after: avoid;
}
}
p {
margin: 1em 0;
}
a {
color: #1a1a1a;
}
a:visited {
color: #1a1a1a;
}
img {
max-width: 100%;
}
h1, h2, h3, h4, h5, h6 {
margin-top: 1.4em;
}
h5, h6 {
font-size: 1em;
font-style: italic;
}
h6 {
font-weight: normal;
}
ol, ul {
padding-left: 1.7em;
margin-top: 1em;
}
li > ol, li > ul {
margin-top: 0;
}
blockquote {
margin: 1em 0 1em 1.7em;
padding-left: 1em;
border-left: 2px solid #e6e6e6;
color: #606060;
}
code {
font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
font-size: 85%;
margin: 0;
}
pre {
margin: 1em 0;
overflow: auto;
}
pre code {
padding: 0;
overflow: visible;
overflow-wrap: normal;
}
.sourceCode {
background-color: transparent;
overflow: visible;
}
hr {
background-color: #1a1a1a;
border: none;
height: 1px;
margin: 1em 0;
}
table {
margin: 1em 0;
border-collapse: collapse;
width: 100%;
overflow-x: auto;
display: block;
font-variant-numeric: lining-nums tabular-nums;
}
table caption {
margin-bottom: 0.75em;
}
tbody {
margin-top: 0.5em;
border-top: 1px solid #1a1a1a;
border-bottom: 1px solid #1a1a1a;
}
th {
border-top: 1px solid #1a1a1a;
padding: 0.25em 0.5em 0.25em 0.5em;
}
td {
padding: 0.125em 0.5em 0.25em 0.5em;
}
header {
margin-bottom: 4em;
text-align: center;
}
#TOC li {
list-style: none;
}
#TOC ul {
padding-left: 1.3em;
}
#TOC > ul {
padding-left: 0;
}
#TOC a:not(:hover) {
text-decoration: none;
}
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
</style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Introduction</h1>
</header>
<p>This thesis will focus on the most common form of prostate cancer,
prostatic adenocarcinoma. For simplicity, we will refer to this type as
just ‘prostate cancer.’ This disease affects one in .. men every year,
making it the most prevalent common type of cancer in men (excluding
skin cancers). Prostate cancer is a disease of the epithelial cells of
the prostate. Epithelial cells line our body cavities, our hollow
organs, and glands. They undergo rapid proliferation, primarily due to
damage. This proliferation increases the risk of genetic mutations,
ultimately increasing the risk of a cell uncontrollably dividing.
Together with enabling factors of its tissue environment, this can give
rise to cancer.</p>
<h1 id="the-gleason-grading-system">The Gleason grading system</h1>
<p>In general, the more aggressive cancerous cells are, the less they
will behave and morphologically appear like their original function. The
prostate is a gland that produces prostatic fluid. The fluid is
transported to the urethra by small tubes. These tubes, called prostatic
glands, are lined with epithelium. Low-grade cancer will thus mimic
those gland structures. High-grade prostate cancer loses its structural
morphology, forming sheets of cells or even quasi-randomly dispersed
individual cancerous cells.</p>
<p>American pathologist Donald Floyd Gleason systematically wrote down
the correlation between growth patterns and prognosis in prostate cancer
in the 1960s<span class="citation" data-cites="cite">[@cite]</span>.
Pathologists still use this Gleason grading, albeit several revisions
later<span class="citation" data-cites="cite">[@cite]</span>, to
classify prostate cancer.</p>
<p>[Insert image Gleason grown patterns]</p>
<h1 id="prognostic-biomarkers">Prognostic biomarkers</h1>
<p>The Gleason grading system can be considered a biomarker as it
indicates the prognosis of a patient with prostate cancer<span class="citation" data-cites="chen2011">[@chen2011]</span>. Traditionally
clinicians use nomograms to decide treatment protocols, which combine
the clinical characteristics of the patients with other diagnostic
tests, such as histopathological, radiological assessment, and lab
assessments. The more precise these assessments are, the better we can
tailor the treatment to the specific patient, this is known as
personalized medicine.<span class="citation" data-cites="cite">[@cite]</span></p>
<h2 id="genomic-biomarkers-mutations-prostatedx-etc">Genomic biomarkers
(mutations &amp; prostateDx etc)</h2>
<ul>
<li>trying to leverage machine learning to make radiology/pathology
reports more precise</li>
</ul>
<h2 id="visual-biomarkers">“Visual” biomarkers</h2>
<h1 id="convolutional-neural-networks">Convolutional neural
networks</h1>
<p>Convolutional neural networks (CNNs) have emerged among the
state-of-the-art machine learning algorithms for various computer vision
tasks, such as image classification and segmentation.</p>
<p>The central component of a convolutional neural network is often
represented as a sliding kernel (or filter) over an input matrix,
producing an output matrix. This has several advantages; we can use a
smaller kernel than the whole making the network less complex while
exploiting the fact that objects in the image are translation invariant.
A cat in the upper-left corner is still a cat in the lower right corner.
We introduce this inductive bias to the network by using
convolutions.</p>
<p>Most convolutional neural network architectures have alternating
blocks of layers consisting of a convolutional operation, a non-linear
activation function, and often a normalization operation. The
non-linearities are essential, as they make the networks able to
represent more complex (non-linear) functions. Normalization layers
bound the output of the block to be within a specific range which helps
during the optimization of the network.</p>
<p>Even though sliding kernels are less complex than having one
parameter per input value, the network architectures have evolved to
become deeper and wider to enhance their accuracy further. Training
larger CNNs demands larger amounts of computer memory, which increases
exponentially with the size of input images. Consequently, most natural
image datasets in computer vision, such as ImageNet and CIFAR-10,
contain sub-megapixel images to circumvent memory limitations.</p>
<p>In specific domains like remote sensing and medical imaging, there is
a need to train CNNs on high-resolution, where most of the information
is contained. Ideally we want to combine the high-resolution information
with more global context, as pathologists can do during daily practive.
However, computer memory becomes a limiting factor. The memory
requirements of CNNs increase proportionally to the input size of the
network, quickly filling up memory with multi-megapixel images. As a
result, only small CNNs can be trained with such images, rendering
state-of-the-art architectures unattainable even on large computing
clusters.</p>
<h1 id="introduce-computation-pathology">Introduce Computation
Pathology</h1>
<ul>
<li>examples of pathology research (Wouter’s gleason papers etc)</li>
</ul>
<p>Some computational pathology tasks can be achieved with patch-based
methods.</p>
<h1 id="weakly-supervised-methods">Weakly supervised methods</h1>
<p>For others, several authors have suggested approaches to train
convolutional neural networks (CNNs) with large input images while
preventing memory bottlenecks. Their methods can be roughly grouped into
three categories: (A) altering the dataset, (B) altering usage of the
dataset, and (C) altering the network or underlying implementations.</p>
<p>Chapter 2 will go more into depth on this.</p>
</body>
</html>
