@misc{190501270DRIT,
  title = {[1905.01270] {{DRIT}}++: {{Diverse Image-to-Image Translation}} via {{Disentangled Representations}}},
  urldate = {2022-11-29},
  howpublished = {https://arxiv.org/abs/1905.01270}
}

@misc{210712357StructurePreserving,
  title = {[2107.12357] {{Structure-Preserving Multi-Domain Stain Color Augmentation}} Using {{Style-Transfer}} with {{Disentangled Representations}}},
  urldate = {2022-11-29},
  howpublished = {https://arxiv.org/abs/2107.12357}
}

@inproceedings{abbetSelfRuleAdaptLearning2021,
  title = {Self-{{Rule}} to {{Adapt}}: {{Learning Generalized Features}} from {{Sparsely-Labeled Data Using Unsupervised Domain Adaptation}} for {{Colorectal Cancer Tissue Phenotyping}}},
  shorttitle = {Self-{{Rule}} to {{Adapt}}},
  booktitle = {Medical {{Imaging}} with {{Deep Learning}}},
  author = {Abbet, Christian and Studer, Linda and Fischer, Andreas and Dawson, Heather and Zlobec, Inti and Bozorgtabar, Behzad and Thiran, Jean-Philippe},
  year = {2021},
  month = jan,
  urldate = {2022-02-09},
  abstract = {Self-supervised learning and domain adaptation with few-labels applied to colorectal cancer tissue types classification.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/Z79HTCM3/Self-Rule to Adapt - 2021 - Abbet et al..pdf}
}

@article{amorimEvaluatingFaithfulnessSaliency2023,
  title = {Evaluating the Faithfulness of Saliency Maps in Explaining Deep Learning Models Using Realistic Perturbations},
  author = {Amorim, Jos{\'e} P. and Abreu, Pedro H. and Santos, Jo{\~a}o and Cortes, Marc and Vila, Victor},
  year = {2023},
  month = mar,
  journal = {Information Processing \& Management},
  volume = {60},
  number = {2},
  pages = {103225},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2022.103225},
  urldate = {2022-12-22},
  abstract = {Deep Learning has reached human-level performance in several medical tasks including classification of histopathological images. Continuous effort has been made at finding effective strategies to interpret these types of models, among them saliency maps, which depict the weights of the pixels on the classification as an heatmap of intensity values, have been by far the most used for image classification. However, there is a lack of tools for the systematic evaluation of saliency maps, and existing works introduce non-natural noise such as random or uniform values. To address this issue, we propose an approach to evaluate the faithfulness of the saliency maps by introducing natural perturbations in the image, based on oppose-class substitution, and studying their impact on evaluation metrics adapted from saliency models. We validate the proposed approach on a breast cancer metastases detection dataset PatchCamelyon with 327,680 patches of histopathological images of sentinel lymph node sections. Results show that GradCAM, Guided-GradCAM and gradient-based saliency map methods are sensitive to natural perturbations and correlate to the presence of tumor evidence in the image. Overall, this approach proves to be a solution for the validation of saliency map methods without introducing confounding variables and shows potential for application on other medical imaging tasks.},
  langid = {english},
  keywords = {Convolutional neural network,Deep learning,Interpretability,Saliency map}
}

@article{arikTabNetAttentiveInterpretable2020,
  title = {{{TabNet}}: {{Attentive Interpretable Tabular Learning}}},
  shorttitle = {{{TabNet}}},
  author = {Arik, Sercan O. and Pfister, Tomas},
  year = {2020},
  month = dec,
  journal = {arXiv:1908.07442 [cs, stat]},
  eprint = {1908.07442},
  primaryclass = {cs, stat},
  urldate = {2022-01-12},
  abstract = {We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into its global behavior. Finally, we demonstrate self-supervised learning for tabular data, significantly improving performance when unlabeled data is abundant.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/Hans/Zotero/storage/86ADY996/Arik and Pfister - 2020 - TabNet Attentive Interpretable Tabular Learning.pdf}
}

@article{atallahPotentialQualityPitfalls2021,
  title = {Potential Quality Pitfalls of Digitalized Whole Slide Image of Breast Pathology in Routine Practice},
  author = {Atallah, Nehal M. and Toss, Michael S. and Verrill, Clare and {Salto-Tellez}, Manuel and Snead, David and Rakha, Emad A.},
  year = {2021},
  month = dec,
  journal = {Modern Pathology},
  issn = {0893-3952, 1530-0285},
  doi = {10.1038/s41379-021-01000-8},
  urldate = {2022-03-29},
  abstract = {Abstract                            Using digitalized whole slide images (WSI) in routine histopathology practice is a revolutionary technology. This study aims to assess the clinical impacts of WSI quality and representation of the corresponding glass slides. 40,160 breast WSIs were examined and compared with their corresponding glass slides. The presence, frequency, location, tissue type, and the clinical impacts of missing tissue were assessed. Scanning time, type of the specimens, time to WSIs implementation, and quality control (QC) measures were also considered. The frequency of missing tissue ranged from 2\% to 19\%. The area size of the missed tissue ranged from 1\textendash 70\%. In most cases ({$>$}75\%), the missing tissue area size was {$<$}10\% and peripherally located. In all cases the missed tissue was fat with or without small entrapped normal breast parenchyma. No missing tissue was identified in WSIs of the core biopsy specimens. QC measures improved images quality and reduced WSI failure rates by seven-fold. A negative linear correlation between the frequency of missing tissue and both the scanning time and the image file size was observed (               p               \,{$<$}\,0.05). None of the WSI with missing tissues resulted in a change in the final diagnosis. Missing tissue on breast WSI is observed but with variable frequency and little diagnostic consequence. Balancing between WSI quality and scanning time/image file size should be considered and pathology laboratories should undertake their own assessments of risk and provide the relevant mitigations with the appropriate level of caution.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/GPDCIIPY/Atallah et al. - 2021 - Potential quality pitfalls of digitalized whole sl.pdf}
}

@techreport{azherAssessmentEmergingPretraining2022,
  type = {Preprint},
  title = {Assessment of {{Emerging Pretraining Strategies}} in {{Interpretable Multimodal Deep Learning}} for {{Cancer Prognostication}}},
  author = {Azher, Zarif L. and Suvarna, Anish and Chen, Ji-Qing and Zhang, Ze and Christensen, Brock C. and Salas, Lucas A. and Vaickus, Louis J. and Levy, Joshua J.},
  year = {2022},
  month = nov,
  institution = {{Pathology}},
  doi = {10.1101/2022.11.21.517440},
  urldate = {2023-05-16},
  abstract = {Abstract           Deep learning models have demonstrated the remarkable ability to infer cancer patient prognosis from molecular and anatomic pathology information. Studies in recent years have demonstrated that leveraging information from complementary multimodal data can improve prognostication, further illustrating the potential utility of such methods. Model interpretation is crucial for facilitating the clinical adoption of deep learning methods by fostering practitioner understanding and trust in the technology. However, while prior works have presented novel multimodal neural network architectures as means to improve prognostication performance, these approaches: 1) do not comprehensively leverage biological and histomorphological relationships and 2) make use of emerging strategies to ``pretrain'' models (i.e., train models on a slightly orthogonal dataset/modeling objective) which may aid prognostication by reducing the amount of information required for achieving optimal performance. Here, we develop an interpretable multimodal modeling framework that combines DNA methylation, gene expression, and histopathology (i.e., tissue slides) data, and we compare the performances of crossmodal pretraining, contrastive learning, and transfer learning versus the standard procedure in this context. Our models outperform the existing state-of-the-art method (average 11.54\% C-index increase), and baseline clinically driven models. Our results demonstrate that the selection of pretraining strategies is crucial for obtaining highly accurate prognostication models, even more so than devising an innovative model architecture, and further emphasize the all-important role of the tumor microenvironment on disease progression.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/TKTABIU9/Azher et al. - 2022 - Assessment of Emerging Pretraining Strategies in I.pdf}
}

@misc{azherDevelopmentBiologicallyInterpretable2021,
  title = {Development of {{Biologically Interpretable Multimodal Deep Learning Model}} for {{Cancer Prognosis Prediction}}},
  author = {Azher, Zarif L. and Vaickus, Louis J. and Salas, Lucas A. and Christensen, Brock C. and Levy, Joshua J.},
  year = {2021},
  month = nov,
  primaryclass = {New Results},
  pages = {2021.10.30.466610},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.10.30.466610},
  urldate = {2023-05-16},
  abstract = {Robust cancer prognostication can enable more effective patient care and management, which may potentially improve health outcomes. Deep learning has proven to be a powerful tool to extract meaningful information from cancer patient data. In recent years it has displayed promise in quantifying prognostication by predicting patient risk. However, most current deep learning-based cancer prognosis prediction methods use only a single data source and miss out on learning from potentially rich relationships across modalities. Existing multimodal approaches are challenging to interpret in a biological or medical context, limiting real-world clinical integration as a trustworthy prognostic decision aid. Here, we developed a multimodal modeling approach that can integrate information from the central modalities of gene expression, DNA methylation, and histopathological imaging with clinical information for cancer prognosis prediction. Our multimodal modeling approach combines pathway and gene-based sparsely coded layers with patch-based graph convolutional networks to facilitate biological interpretation of the model results. We present a preliminary analysis that compares the potential applicability of combining all modalities to uni- or bi-modal approaches. Leveraging data from four cancer subtypes from the Cancer Genome Atlas, results demonstrate the encouraging performance of our multimodal approach (C-index=0.660 without clinical features; C-index=0.665 with clinical features) across four cancer subtypes versus unimodal approaches and existing state-of-the-art approaches. This work brings insight to the development of interpretable multimodal methods of applying AI to biomedical data and can potentially serve as a foundation for clinical implementations of such software. We plan to follow up this preliminary analysis with an in-depth exploration of factors to improve multimodal modeling approaches on an in-house dataset.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/6BCYVRUM/Azher et al. - 2021 - Development of Biologically Interpretable Multimod.pdf}
}

@article{beckPragmaticProgrammer,
  title = {The {{Pragmatic Programmer}}},
  author = {Beck, Kent and Fowler, Martin and Ruland, Kevin and Lakos, John},
  pages = {352},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/TBNE2I7S/The Pragmatic Programmer - - Becket al..pdf}
}

@article{bond-taylorDeepGenerativeModelling2022,
  title = {Deep {{Generative Modelling}}: {{A Comparative Review}} of {{VAEs}}, {{GANs}}, {{Normalizing Flows}}, {{Energy-Based}} and {{Autoregressive Models}}},
  shorttitle = {Deep {{Generative Modelling}}},
  author = {{Bond-Taylor}, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
  year = {2022},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {11},
  pages = {7327--7347},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3116668},
  abstract = {Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations.},
  keywords = {Analytical models,autoregressive models,Computational modeling,Data models,Deep learning,energy-based models,generative adversarial networks,Generative adversarial networks,generative models,Neurons,normalizing flows,Predictive models,Training,variational autoencoders},
  file = {/Users/Hans/Zotero/storage/JE3WU5XC/Bond-Taylor et al. - 2022 - Deep Generative Modelling A Comparative Review of.pdf}
}

@article{bouteldjaImprovingUnsupervisedStaintostain2022,
  title = {Improving Unsupervised Stain-to-Stain Translation Using Self-Supervision and Meta-Learning},
  author = {Bouteldja, Nassim and Klinkhammer, Barbara M. and Schlaich, Tarek and Boor, Peter and Merhof, Dorit},
  year = {2022},
  month = jan,
  journal = {Journal of Pathology Informatics},
  volume = {13},
  pages = {100107},
  issn = {2153-3539},
  doi = {10.1016/j.jpi.2022.100107},
  urldate = {2022-12-01},
  abstract = {Background In digital pathology, many image analysis tasks are challenged by the need for large and time-consuming manual data annotations to cope with various sources of variability in the image domain. Unsupervised domain adaptation based on image-to-image translation is gaining importance in this field by addressing variabilities without the manual overhead. Here, we tackle the variation of different histological stains by unsupervised stain-to-stain translation to enable a stain-independent applicability of a deep learning segmentation model. Methods We use CycleGANs for stain-to-stain translation in kidney histopathology, and propose two novel approaches to improve translational effectivity. First, we integrate a prior segmentation network into the CycleGAN for a self-supervised, application-oriented optimization of translation through semantic guidance, and second, we incorporate extra channels to the translation output to implicitly separate artificial meta-information otherwise encoded for tackling underdetermined reconstructions. Results The latter showed partially superior performances to the unmodified CycleGAN, but the former performed best in all stains providing instance-level Dice scores ranging between 78\% and 92\% for most kidney structures, such as glomeruli, tubules, and veins. However, CycleGANs showed only limited performance in the translation of other structures, e.g. arteries. Our study also found somewhat lower performance for all structures in all stains when compared to segmentation in the original stain. Conclusions Our study suggests that with current unsupervised technologies, it seems unlikely to produce ``generally'' applicable simulated stains.},
  langid = {english},
  keywords = {Deep learning,Digital pathology,Domain translation,Kidney,Segmentation,Stain-to-stain translation},
  file = {/Users/Hans/Zotero/storage/26MR66G3/Bouteldja et al. - 2022 - Improving unsupervised stain-to-stain translation .pdf}
}

@article{bouteldjaTacklingStainVariability2022,
  title = {Tackling Stain Variability Using {{CycleGAN-based}} Stain Augmentation},
  author = {Bouteldja, Nassim and H{\"o}lscher, David L. and B{\"u}low, Roman D. and Roberts, Ian S. D. and Coppo, Rosanna and Boor, Peter},
  year = {2022},
  month = jan,
  journal = {Journal of Pathology Informatics},
  volume = {13},
  pages = {100140},
  issn = {2153-3539},
  doi = {10.1016/j.jpi.2022.100140},
  urldate = {2022-12-01},
  abstract = {Background Considerable inter- and intra-laboratory stain variability exists in pathology, representing a challenge in development and application of deep learning (DL) approaches. Since tackling all sources of stain variability with manual annotation is not feasible, we here investigated and compared unsupervised DL approaches to reduce the consequences of stain variability in kidney pathology. Methods We aimed to improve the applicability of a pretrained DL segmentation model to 3 external multi-centric cohorts with large stain variability. In contrast to the traditional approach of training generative adversarial networks (GAN) for stain normalization, we here propose to tackle stain variability by data augmentation. We augment the training data of the pretrained model by the stain variability using CycleGANs and then retrain the model on the stain-augmented dataset. We compared the performance of i/ the unmodified pretrained segmentation model with ii/ CycleGAN-based stain normalization, iii/ a feature-preserving modification to ii/ for improved normalization, and iv/ the proposed stain-augmented model. Results The proposed stain-augmented model showed highest mean segmentation accuracy in all external cohorts and maintained comparable performance on the training cohort. However, the increase in performance was only marginal compared to the pretrained model. CycleGAN-based stain normalization suffered from encoded imperceptible information into the normalizations that confused the pretrained model and thus resulted in slightly worse performance. Conclusions Our findings suggest that stain variability can be tackled more effectively by augmenting data by it than by following the commonly used approach of normalizing the stain. However, the applicability of this approach providing only a rather slight performance increase has to be weighted against an additional carbon footprint.},
  langid = {english},
  keywords = {Deep learning,Digital pathology,Kidney,Segmentation,Stain augmentation,Stain normalization},
  file = {/Users/Hans/Zotero/storage/LF67AY25/Bouteldja et al. - 2022 - Tackling stain variability using CycleGAN-based st.pdf}
}

@article{campanellaEbasedComputationalBiomarker,
  title = {H\&{{E-based Computational Biomarker Enables Universal EGFR Screening}} for {{Lung Adenocarcinoma}}},
  author = {Campanella, Gabriele and Ho, David and H{\"a}ggstr{\"o}m, Ida and Becker, Anton S and Chang, Jason and Vanderbilt, Chad and Fuchs, Thomas J},
  pages = {36},
  abstract = {Lung cancer is the leading cause of cancer death worldwide, with lung adenocarcinoma being the most prevalent form of lung cancer. EGFR positive lung adenocarcinomas have been shown to have high response rates to TKI therapy, underlying the essential nature of molecular testing for lung cancers. Despite current guidelines consider testing necessary, a large portion of patients are not routinely profiled, resulting in millions of people not receiving the optimal treatment for their lung cancer. Sequencing is the gold standard for molecular testing of EGFR mutations, but it can take several weeks for results to come back, which is not ideal in a time constrained scenario. The development of alternative screening tools capable of detecting EGFR mutations quickly and cheaply while preserving tissue for sequencing could help reduce the amount of sub-optimally treated patients. We propose a multi-modal approach which integrates pathology images and clinical variables to predict EGFR mutational status achieving an AUC of 84\% on the largest clinical cohort to date. Such a computational model could be deployed at large at little additional cost. Its clinical application could reduce the number of patients who receive sub-optimal treatments by 53.1\% in China, and up to 96.6\% in the US.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/9U3237S8/Campanella et al. - H&E-based Computational Biomarker Enables Universa.pdf}
}

@misc{chambonRoentGenVisionLanguageFoundation2022,
  title = {{{RoentGen}}: {{Vision-Language Foundation Model}} for {{Chest X-ray Generation}}},
  shorttitle = {{{RoentGen}}},
  author = {Chambon, Pierre and Bluethgen, Christian and Delbrouck, Jean-Benoit and {Van der Sluijs}, Rogier and Po{\l}acin, Ma{\l}gorzata and Chaves, Juan Manuel Zambrano and Abraham, Tanishq Mathew and Purohit, Shivanshu and Langlotz, Curtis P. and Chaudhari, Akshay},
  year = {2022},
  month = nov,
  number = {arXiv:2211.12737},
  eprint = {2211.12737},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.12737},
  urldate = {2023-05-11},
  abstract = {Multimodal models trained on large natural image-text pair datasets have exhibited astounding abilities in generating high-quality images. Medical imaging data is fundamentally different to natural images, and the language used to succinctly capture relevant details in medical data uses a different, narrow but semantically rich, domain-specific vocabulary. Not surprisingly, multi-modal models trained on natural image-text pairs do not tend to generalize well to the medical domain. Developing generative imaging models faithfully representing medical concepts while providing compositional diversity could mitigate the existing paucity of high-quality, annotated medical imaging datasets. In this work, we develop a strategy to overcome the large natural-medical distributional shift by adapting a pre-trained latent diffusion model on a corpus of publicly available chest x-rays (CXR) and their corresponding radiology (text) reports. We investigate the model's ability to generate high-fidelity, diverse synthetic CXR conditioned on text prompts. We assess the model outputs quantitatively using image quality metrics, and evaluate image quality and text-image alignment by human domain experts. We present evidence that the resulting model (RoentGen) is able to create visually convincing, diverse synthetic CXR images, and that the output can be controlled to a new extent by using free-form text prompts including radiology-specific language. Fine-tuning this model on a fixed training set and using it as a data augmentation method, we measure a 5\% improvement of a classifier trained jointly on synthetic and real images, and a 3\% improvement when trained on a larger but purely synthetic training set. Finally, we observe that this fine-tuning distills in-domain knowledge in the text-encoder and can improve its representation capabilities of certain diseases like pneumothorax by 25\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/CDAHAZ7B/Chambon et al. - RoentGen - Vision-Language Foundation Model for Chest X-ray Generation.pdf}
}

@inproceedings{chandraGradientDescentUltimate2022,
  title = {Gradient {{Descent}}: {{The Ultimate Optimizer}}},
  shorttitle = {Gradient {{Descent}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chandra, Kartik and Xie, Audrey and {Ragan-Kelley}, Jonathan and Meijer, Erik},
  year = {2022},
  month = oct,
  urldate = {2022-11-30},
  abstract = {Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer's hyperparameters, such as its step size. Recent work has shown how the step size can itself be optimized alongside the model parameters by manually deriving expressions for "hypergradients" ahead of time. We show how to *automatically* compute hypergradients with a simple and elegant modification to backpropagation. This allows us to easily apply the method to other optimizers and hyperparameters (e.g. momentum coefficients). We can even recursively apply the method to its own *hyper*-hyperparameters, and so on ad infinitum. As these towers of optimizers grow taller, they become less sensitive to the initial choice of hyperparameters. We present experiments validating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch implementation of this algorithm (see http://people.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer).},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/F6DSJ9A6/Chandra et al. - 2022 - Gradient Descent The Ultimate Optimizer.pdf}
}

@incollection{changStainMixUpUnsupervised2021,
  title = {Stain {{Mix-Up}}: {{Unsupervised Domain Generalization}} for {{Histopathology Images}}},
  shorttitle = {Stain {{Mix-Up}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} \textendash{} {{MICCAI}} 2021},
  author = {Chang, Jia-Ren and Wu, Min-Sheng and Yu, Wei-Hsiang and Chen, Chi-Chung and Yang, Cheng-Kung and Lin, Yen-Yu and Yeh, Chao-Yuan},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  volume = {12903},
  pages = {117--126},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-87199-4\_11},
  urldate = {2022-12-12},
  abstract = {Computational histopathology studies have shown that stain color variations considerably hamper the performance. Stain color variations indicate the slides exhibit greatly different color appearance due to the diversity of chemical stains, staining procedures, and slide scanners. Previous approaches tend to improve model robustness via data augmentation or stain color normalization. However, they still suffer from generalization to new domains with unseen stain colors. In this study, we address the issue of unseen color domain generalization in histopathology images by encouraging the model to adapt varied stain colors. To this end, we propose a novel data augmentation method, stain mixup, which incorporates the stain colors of unseen domains into training data. Unlike previous mix-up methods employed in computer vision, the proposed method constructs the combination of stain colors without using any label information, hence enabling unsupervised domain generalization. Extensive experiments are conducted and demonstrate that our method is general enough to different tasks and stain methods, including H\&E stains for tumor classification and hematological stains for bone marrow cell instance segmentation. The results validate that the proposed stain mix-up can significantly improves the performance on the unseen domains.},
  isbn = {978-3-030-87198-7 978-3-030-87199-4},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/YM6P8X3Q/Chang et al. - 2021 - Stain Mix-Up Unsupervised Domain Generalization f.pdf}
}

@article{chenBiomarkersClinicalMedicine,
  title = {Biomarkers in Clinical Medicine},
  author = {Chen, Xiao-He and Huang, Shuwen and Kerr, David},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/M483V7P2/Chen et al. - Biomarkers in clinical medicine.pdf}
}

@article{chenFastScalableSearch2022,
  title = {Fast and Scalable Search of Whole-Slide Images via Self-Supervised Deep Learning},
  author = {Chen, Chengkuan and Lu, Ming Y. and Williamson, Drew F. K. and Chen, Tiffany Y. and Schaumberg, Andrew J. and Mahmood, Faisal},
  year = {2022},
  month = oct,
  journal = {Nature Biomedical Engineering},
  issn = {2157-846X},
  doi = {10.1038/s41551-022-00929-8},
  urldate = {2022-10-14},
  abstract = {Abstract             The adoption of digital pathology has enabled the curation of large repositories of gigapixel whole-slide images (WSIs). Computationally identifying WSIs with similar morphologic features within large repositories without requiring supervised training can have significant applications. However, the retrieval speeds of algorithms for searching similar WSIs often scale with the repository size, which limits their clinical and research potential. Here we show that self-supervised deep learning can be leveraged to search for and retrieve WSIs at speeds that are independent of repository size. The algorithm, which we named SISH (for self-supervised image search for histology) and provide as an open-source package, requires only slide-level annotations for training, encodes WSIs into meaningful discrete latent representations and leverages a tree data structure for fast searching followed by an uncertainty-based ranking algorithm for WSI retrieval. We evaluated SISH on multiple tasks (including retrieval tasks based on tissue-patch queries) and on datasets spanning over 22,000 patient cases and 56 disease subtypes. SISH can also be used to aid the diagnosis of rare cancer types for which the number of available WSIs is often insufficient to train supervised deep-learning models.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/IAPQ7869/Chen et al. - 2022 - Fast and scalable search of whole-slide images via.pdf}
}

@misc{chenKnowledgeDistillationReused2022,
  title = {Knowledge {{Distillation}} with the {{Reused Teacher Classifier}}},
  author = {Chen, Defang and Mei, Jian-Ping and Zhang, Hailin and Wang, Can and Feng, Yan and Chen, Chun},
  year = {2022},
  month = mar,
  number = {arXiv:2203.14001},
  eprint = {2203.14001},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-07},
  abstract = {Knowledge distillation aims to compress a powerful yet cumbersome teacher model into a lightweight student model without much sacrifice of performance. For this purpose, various approaches have been proposed over the past few years, generally with elaborately designed knowledge representations, which in turn increase the difficulty of model development and interpretation. In contrast, we empirically show that a simple knowledge distillation technique is enough to significantly narrow down the teacher-student performance gap. We directly reuse the discriminative classifier from the pre-trained teacher model for student inference and train a student encoder through feature alignment with a single \$\textbackslash ell\_2\$ loss. In this way, the student model is able to achieve exactly the same performance as the teacher model provided that their extracted features are perfectly aligned. An additional projector is developed to help the student encoder match with the teacher classifier, which renders our technique applicable to various teacher and student architectures. Extensive experiments demonstrate that our technique achieves state-of-the-art results at the modest cost of compression ratio due to the added projector.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/NVFP7WXY/Chen et al. - Knowledge Distillation with the Reused Teacher Classifier.pdf}
}

@article{chenScalingVisionTransformers,
  title = {Scaling {{Vision Transformers}} to {{Gigapixel Images}} via {{Hierarchical Self-Supervised Learning}}},
  author = {Chen, Richard J and Chen, Chengkuan and Li, Yicong and Chen, Tiffany Y and Trister, Andrew D and Krishnan, Rahul G and Mahmood, Faisal},
  pages = {19},
  abstract = {Vision Transformers (ViTs) and their multi-scale and hierarchical variations have been successful at capturing image representations but their use has been generally studied for low-resolution images (e.g. 256 \texttimes{} 256, 384 \texttimes{} 384). For gigapixel whole-slide imaging (WSI) in computational pathology, WSIs can be as large as 150000 \texttimes{} 150000 pixels at 20\texttimes{} magnification and exhibit a hierarchical structure of visual tokens across varying resolutions: from 16 \texttimes{} 16 images capturing individual cells, to 4096\texttimes 4096 images characterizing interactions within the tissue microenvironment. We introduce a new ViT architecture called the Hierarchical Image Pyramid Transformer (HIPT), which leverages the natural hierarchical structure inherent in WSIs using two levels of self-supervised learning to learn high-resolution image representations. HIPT is pretrained across 33 cancer types using 10,678 gigapixel WSIs, 408,218 4096\texttimes 4096 images, and 104M 256 \texttimes{} 256 images. We benchmark HIPT representations on 9 slide-level tasks, and demonstrate that: 1) HIPT with hierarchical pretraining outperforms current state-of-the-art methods for cancer subtyping and survival prediction, 2) self-supervised ViTs are able to model important inductive biases about the hierarchical structure of phenotypes in the tumor microenvironment.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/FZHWWLZR/Chen et al. - Scaling Vision Transformers to Gigapixel Images vi.pdf}
}

@misc{ConnectomeInsectBrain,
  title = {The Connectome of an Insect Brain | {{bioRxiv}}},
  urldate = {2022-11-29},
  howpublished = {https://www.biorxiv.org/content/10.1101/2022.11.28.516756v1}
}

@article{damatoComparisonSingleMultiScale2022,
  title = {A {{Comparison Between Single-}} and {{Multi-Scale Approaches}} for {{Classification}} of {{Histopathology Images}}},
  author = {D'Amato, Marina and Szostak, Przemys{\l}aw and {Torben-Nielsen}, Benjamin},
  year = {2022},
  journal = {Frontiers in Public Health},
  volume = {10},
  issn = {2296-2565},
  urldate = {2023-01-30},
  abstract = {Whole slide images (WSIs) are digitized histopathology images. WSIs are stored in a pyramidal data structure that contains the same images at multiple magnification levels. In digital pathology, most algorithmic approaches to analyze WSIs use a single magnification level. However, images at different magnification levels may reveal relevant and distinct properties in the image, such as global context or detailed spatial arrangement. Given their high resolution, WSIs cannot be processed as a whole and are broken down into smaller pieces called tiles. Then, a prediction at the tile-level is made for each tile in the larger image. As many classification problems require a prediction at a slide-level, there exist common strategies to integrate the tile-level insights into a slide-level prediction. We explore two approaches to tackle this problem, namely a multiple instance learning framework and a representation learning algorithm (the so-called ``barcode approach'') based on clustering. In this work, we apply both approaches in a single- and multi-scale setting and compare the results in a multi-label histopathology classification task to show the promises and pitfalls of multi-scale analysis. Our work shows a consistent improvement in performance of the multi-scale models over single-scale ones. Using multiple instance learning and the barcode approach we achieved a 0.06 and 0.06 improvement in F1 score, respectively, highlighting the importance of combining multiple scales to integrate contextual and detailed information.},
  file = {/Users/Hans/Zotero/storage/MXIUEZ3Y/D'Amato et al. - 2022 - A Comparison Between Single- and Multi-Scale Appro.pdf}
}

@misc{defazioLearningRateFreeLearningDAdaptation2023,
  title = {Learning-{{Rate-Free Learning}} by {{D-Adaptation}}},
  author = {Defazio, Aaron and Mishchenko, Konstantin},
  year = {2023},
  month = jan,
  number = {arXiv:2301.07733},
  eprint = {2301.07733},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.07733},
  urldate = {2023-01-20},
  abstract = {The speed of gradient descent for convex Lipschitz functions is highly dependent on the choice of learning rate. Setting the learning rate to achieve the optimal convergence rate requires knowing the distance D from the initial point to the solution set. In this work, we describe a single-loop method, with no back-tracking or line searches, which does not require knowledge of \$D\$ yet asymptotically achieves the optimal rate of convergence for the complexity class of convex Lipschitz functions. Our approach is the first parameter-free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems. Our method is practical, efficient and requires no additional function value or gradient evaluations each step. An open-source implementation is available (https://github.com/facebookresearch/dadaptation).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/Hans/Zotero/storage/9FXDVBHV/Defazio Mishchenko - Learning-Rate-Free Learning by D-Adaptation.pdf}
}

@article{diaoHumaninterpretableImageFeatures2021,
  title = {Human-Interpretable Image Features Derived from Densely Mapped Cancer Pathology Slides Predict Diverse Molecular Phenotypes},
  author = {Diao, James A. and Wang, Jason K. and Chui, Wan Fung and Mountain, Victoria and Gullapally, Sai Chowdary and Srinivasan, Ramprakash and Mitchell, Richard N. and Glass, Benjamin and Hoffman, Sara and Rao, Sudha K. and Maheshwari, Chirag and Lahiri, Abhik and Prakash, Aaditya and McLoughlin, Ryan and Kerner, Jennifer K. and Resnick, Murray B. and Montalto, Michael C. and Khosla, Aditya and Wapinski, Ilan N. and Beck, Andrew H. and Elliott, Hunter L. and {Taylor-Weiner}, Amaro},
  year = {2021},
  month = mar,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {1613},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-21896-9},
  urldate = {2023-05-23},
  abstract = {Computational methods have made substantial progress in improving the accuracy and throughput of pathology workflows for diagnostic, prognostic, and genomic prediction. Still, lack of interpretability remains a significant barrier to clinical integration. We present an approach for predicting clinically-relevant molecular phenotypes from whole-slide histopathology images using human-interpretable image features (HIFs). Our method leverages {$>$}1.6 million annotations from board-certified pathologists across {$>$}5700 samples to train deep learning models for cell and tissue classification that can exhaustively map whole-slide images at two and four micron-resolution. Cell- and tissue-type model outputs are combined into 607 HIFs that quantify specific and biologically-relevant characteristics across five cancer types. We demonstrate that these HIFs correlate with well-known markers of the tumor microenvironment and can predict diverse molecular signatures (AUROC 0.601\textendash 0.864), including expression of four immune checkpoint proteins and homologous recombination deficiency, with performance comparable to `black-box' methods. Our HIF-based approach provides a comprehensive, quantitative, and interpretable window into the composition and spatial architecture of the tumor microenvironment.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Cancer imaging,Machine learning,Predictive markers}
}

@article{dietrichExplainableEndtoEndProstate,
  title = {Towards {{Explainable End-to-End Prostate Cancer Relapse Prediction}} from {{H}}\&{{E Images Combining Self-Attention Multiple Instance Learning}} with a {{Recurrent Neural Network}}},
  author = {Dietrich, Esther and Fuhlert, Patrick and Ernst, Anne and Sauter, Guido and Lennartz, Maximilian and Stiehl, H Siegfried and Zimmermann, Marina and Bonn, Stefan},
  pages = {16},
  abstract = {Clinical decision support for histopathology image data mainly focuses on strongly supervised annotations, which offers intuitive interpretability, but is bound by expert performance. Here, we propose an explainable cancer relapse prediction network (eCaReNet) and show that end-to-end learning without strong annotations offers state-of-the-art performance while interpretability can be included through an attention mechanism. On the use case of prostate cancer survival prediction, using 14,479 images and only relapse times as annotations, we reach a cumulative dynamic AUC of 0.78 on a validation set, being on par with an expert pathologist (and an AUC of 0.77 on a separate test set). Our model is well-calibrated and outputs survival curves as well as a risk score and group per patient. Making use of the attention weights of a multiple instance learning layer, we show that malignant patches have a higher influence on the prediction than benign patches, thus offering an intuitive interpretation of the prediction. Our code is available at www.github.com/imsb-uke/ecarenet.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/WKQ2TZBV/Dietrich et al. - Towards Explainable End-to-End Prostate Cancer Rel.pdf}
}

@article{dooperGigapixelEndtoendTraining2022,
  title = {Gigapixel End-to-End Training Using Streaming and Attention},
  author = {Dooper, Stephan and Pinckaers, Hans and Litjens, Geert},
  year = {2022},
  journal = {Medical Image Analysis},
  pages = {10},
  abstract = {Current hardware limitations make it impossible to train convolutional neural networks on large image inputs directly. Recent developments in weakly supervised learning have shown strong results, but often use decoupled or incomplete training strategies when used on huge image sizes, which ultimately negatively impacts performance. In this paper, we propose to train a ResNet-34 encoder with an attention head in an endto-end fashion, which we call StreamingCLAM using a streaming implementation of convolutional layers. This allows us to train on gigapixel images using only slide-level labels without resorting to patches or decoupled network training strategies. We show that our model offers a degree of interpretability through the attention mechanism. We achieve a mean area under the receiver operating characteristic curve of 0.9757 for metastatic breast cancer detection (CAMELYON16), which is close to fully supervised approaches that use pixel-level annotations. Our model is also able to perform competitively in detecting MYC-translocation in slides of diffuse large B-cell lymphoma, achieving a mean area of 0.8259.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/973GA3WD/Dooper et al. - 2022 - Gigapixel end-to-end training using streaming and .pdf}
}

@misc{el-noubyXCiTCrossCovarianceImage2021,
  title = {{{XCiT}}: {{Cross-Covariance Image Transformers}}},
  shorttitle = {{{XCiT}}},
  author = {{El-Nouby}, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and Jegou, Herv{\'e}},
  year = {2021},
  month = jun,
  number = {arXiv:2106.09681},
  eprint = {2106.09681},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-08-22},
  abstract = {Following tremendous success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens, i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a ``transposed'' version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) \textendash{} built upon XCA \textendash{} combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including (self-supervised) image classification on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/XKPD9Y7A/El-Nouby et al. - 2021 - XCiT Cross-Covariance Image Transformers.pdf}
}

@article{elGILTGeneratingImages2019,
  title = {{{GILT}}: {{Generating Images}} from {{Long Text}}},
  shorttitle = {{{GILT}}},
  author = {El, Ori Bar and Licht, Ori and Yosephian, Netanel},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.02404 [cs]},
  eprint = {1901.02404},
  primaryclass = {cs},
  urldate = {2021-12-14},
  abstract = {Creating an image reflecting the content of a long text is a complex process that requires a sense of creativity. For example, creating a book cover or a movie poster based on their summary or a food image based on its recipe. In this paper we present the new task of generating images from long text that does not describe the visual content of the image directly. For this, we build a system for generating high-resolution 256 \texttimes{} 256 images of food conditioned on their recipes. The relation between the recipe text (without its title) to the visual content of the image is vague, and the textual structure of recipes is complex, consisting of two sections (ingredients and instructions) both containing multiple sentences. We used the recipe1M [11] dataset to train and evaluate our model that is based on a the StackGAN-v2 architecture [15].},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/J5YE32WK/GILT - 2019 - Elet al..pdf}
}

@article{ericksonAutoGluonTabularRobustAccurate2020,
  title = {{{AutoGluon-Tabular}}: {{Robust}} and {{Accurate AutoML}} for {{Structured Data}}},
  shorttitle = {{{AutoGluon-Tabular}}},
  author = {Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
  year = {2020},
  month = mar,
  urldate = {2022-02-03},
  abstract = {We introduce AutoGluon-Tabular, an open-source AutoML framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best. A second contribution is an extensive evaluation of public and commercial AutoML platforms including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and Google AutoML Tables. Tests on a suite of 50 classification and regression tasks from Kaggle and the OpenML AutoML Benchmark reveal that AutoGluon is faster, more robust, and much more accurate. We find that AutoGluon often even outperforms the best-in-hindsight combination of all of its competitors. In two popular Kaggle competitions, AutoGluon beat 99\% of the participating data scientists after merely 4h of training on the raw data.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/UAXMQ5TR/AutoGluon-Tabular - 2020 - Erickson et al..pdf}
}

@article{estevaProstateCancerTherapy,
  title = {Prostate Cancer Therapy Personalization via Multi-Modal Deep Learning},
  author = {Esteva, Andre and Feng, Jean and Huang, Shih-Cheng and DeVries, Sandy and Chen, Emmalyn and Schaeffer, Edward M and Morgan, Todd M and Sun, Yilun and Ghorbani, Amirata and Naik, Nikhil and Nathawani, Dhruv and Socher, Richard and Naz, Farah and Wallace, James and Ferguson, Michelle J and Bahary, Jean-Paul and Zou, James and Lungren, Matthew and Yeung, Serena and Sandler, Howard M and Tran, Phuoc T and Spratt, Daniel E and Pugh, Stephanie and Feng, Felix Y and Mohamad, Osama},
  pages = {21},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/XNG63BC9/Esteva et al. - Prostate cancer therapy personalization via multi-.pdf}
}

@article{fanCancerSurvivalPrediction2022,
  title = {Cancer {{Survival Prediction}} from {{Whole Slide Images}} with {{Self-Supervised Learning}} and {{Slide Consistency}}},
  author = {Fan, Lei and Sowmya, Arcot and Meijering, Erik and Song, Yang},
  year = {2022},
  journal = {IEEE Transactions on Medical Imaging},
  pages = {1--1},
  issn = {1558-254X},
  doi = {10.1109/TMI.2022.3228275},
  abstract = {Histopathological Whole Slide Images (WSIs) at giga-pixel resolution are the gold standard for cancer analysis and prognosis. Due to the scarcity of pixel- or patch-level annotations of WSIs, many existing methods attempt to predict survival outcomes based on a three-stage strategy that includes patch selection, patch-level feature extraction and aggregation. However, the patch features are usually extracted by using truncated models (e.g. ResNet) pretrained on ImageNet without fine-tuning on WSI tasks, and the aggregation stage does not consider the many-to-one relationship between multiple WSIs and the patient. In this paper, we propose a novel survival prediction framework that consists of patch sampling, feature extraction and patient-level survival prediction. Specifically, we employ two kinds of self-supervised learning methods, i.e. colorization and cross-channel, as pretext tasks to train convnet-based models that are tailored for extracting features from WSIs. Then, at the patient-level survival prediction we explicitly aggregate features from multiple WSIs, using consistency and contrastive losses to normalize slide-level features at the patient level. We conduct extensive experiments on three large-scale datasets: TCGA-GBM, TCGA-LUSC and NLST. Experimental results demonstrate the effectiveness of our proposed framework, as it achieves state-of-the-art performance in comparison with previous studies, with concordance index of 0.670, 0.679 and 0.711 on TCGA-GBM, TCGA-LUSC and NLST, respectively.},
  keywords = {Annotations,Cancer,Computational modeling,Deep Learning,Feature extraction,Self-supervised learning,Self-supervised Learning,Survival prediction,Task analysis,Training,Whole Slide Images},
  file = {/Users/Hans/Zotero/storage/D5SP8ZBJ/Fan et al. - Cancer Survival Prediction from Whole Slide Images with Self-Supervised Learning and Slide Consistency.pdf}
}

@article{fiedlerSimpleModificationsImprove2021,
  title = {Simple {{Modifications}} to {{Improve Tabular Neural Networks}}},
  author = {Fiedler, James},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.03214 [cs]},
  eprint = {2108.03214},
  primaryclass = {cs},
  urldate = {2022-01-27},
  abstract = {There is growing interest in neural network architectures for tabular data. Many general-purpose tabular deep learning models have been introduced recently, with performance sometimes rivaling gradient boosted decision trees (GBDTs). These recent models draw inspiration from various sources, including GBDTs, factorization machines, and neural networks from other application domains. Previous tabular neural networks are also drawn upon, but are possibly underconsidered, especially models associated with specific tabular problems. This paper focuses on several such models, and proposes modifications for improving their performance. When modified, these models are shown to be competitive with leading general-purpose tabular models, including GBDTs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/UPI7YB53/Fiedler - 2021 - Simple Modifications to Improve Tabular Neural Net.pdf}
}

@article{footeREETRobustnessEvaluation2022,
  title = {{{REET}}: {{Robustness Evaluation}} and {{Enhancement Toolbox}} for {{Computational Pathology}}},
  shorttitle = {{{REET}}},
  author = {Foote, Alex and Asif, Amina and Rajpoot, Nasir and Minhas, Fayyaz},
  editor = {Peng, Hanchuan},
  year = {2022},
  month = may,
  journal = {Bioinformatics},
  pages = {btac315},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btac315},
  urldate = {2022-05-17},
  abstract = {Motivation: Digitization of pathology laboratories through digital slide scanners and advances in deep learning approaches for objective histological assessment have resulted in rapid progress in the field of computational pathology (CPath) with wide-ranging applications in medical and pharmaceutical research as well as clinical workflows. However, the estimation of robustness of CPath models to variations in input images is an open problem with a significant impact on the downstream practical applicability, deployment and acceptability of these approaches. Furthermore, development of domain-specific strategies for enhancement of robustness of such models is of prime importance as well.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/355SMX8A/Foote et al. - 2022 - REET Robustness Evaluation and Enhancement Toolbo.pdf}
}

@article{fortDrawingMultipleAugmentation2021,
  title = {Drawing {{Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error}}},
  author = {Fort, Stanislav and Brock, Andrew and Pascanu, Razvan and De, Soham and Smith, Samuel L.},
  year = {2021},
  month = may,
  journal = {arXiv:2105.13343 [cs]},
  eprint = {2105.13343},
  primaryclass = {cs},
  urldate = {2021-12-14},
  abstract = {In computer vision, it is standard practice to draw a single sample from the data augmentation procedure for each unique image in the mini-batch, however it is not clear whether this choice is optimal for generalization. In this work, we provide a detailed empirical evaluation of how the number of augmentation samples per unique image influences performance on held out data. Remarkably, we find that drawing multiple samples per image consistently enhances the test accuracy achieved for both small and large batch training, despite reducing the number of unique training examples in each mini-batch. This benefit arises even when different augmentation multiplicities perform the same number of parameter updates and gradient evaluations. Our results suggest that, although the variance in the gradient estimate arising from subsampling the dataset has an implicit regularization benefit, the variance which arises from the data augmentation process harms test accuracy. By applying augmentation multiplicity to the recently proposed NFNet model family, we achieve a new ImageNet state of the art of 86.8\% top-1 w/o extra data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/HPIR34MT/Drawing Multiple Augmentation Samples Per Image During Training Efficiently - 2021 - Fortet al..pdf}
}

@article{fortExploringLimitsOutofDistribution2021,
  title = {Exploring the {{Limits}} of {{Out-of-Distribution Detection}}},
  author = {Fort, Stanislav and Ren, Jie and Lakshminarayanan, Balaji},
  year = {2021},
  month = jul,
  journal = {arXiv:2106.03004 [cs]},
  eprint = {2106.03004},
  primaryclass = {cs},
  urldate = {2021-12-14},
  abstract = {Near out-of-distribution detection (OOD) is a major challenge for deep neural networks. We demonstrate that large-scale pre-trained transformers can significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks across different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD detection, we improve the AUROC from 85\% (current SOTA) to 96\% using Vision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD detection benchmark, we improve the AUROC from 66\% to 77\% using transformers and unsupervised pre-training. To further improve performance, we explore the few-shot outlier exposure setting where a few examples from outlier classes may be available; we show that pre-trained transformers are particularly well-suited for outlier exposure, and that the AUROC of OOD detection on CIFAR-100 vs CIFAR10 can be improved to 98.7\% with just 1 image per OOD class, and 99.46\% with 10 images per OOD class. For multi-modal image-text pre-trained transformers such as CLIP, we explore a new way of using just the names of outlier classes as a sole source of information without any accompanying images, and show that this outperforms previous SOTA on standard vision OOD benchmark tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/8MJLXQLH/Exploring the Limits of Out-of-Distribution Detection - 2021 - Fortet al..pdf}
}

@misc{ganinUnsupervisedDomainAdaptation2015,
  title = {Unsupervised {{Domain Adaptation}} by {{Backpropagation}}},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  year = {2015},
  month = feb,
  number = {arXiv:1409.7495},
  eprint = {1409.7495},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.7495},
  urldate = {2022-12-08},
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/Hans/Zotero/storage/KKGWPMWJ/Ganin Lempitsky - Unsupervised Domain Adaptation by Backpropagation.pdf}
}

@misc{grahamOneModelAll2022,
  title = {One {{Model}} Is {{All You Need}}: {{Multi-Task Learning Enables Simultaneous Histology Image Segmentation}} and {{Classification}}},
  shorttitle = {One {{Model}} Is {{All You Need}}},
  author = {Graham, Simon and Vu, Quoc Dang and Jahanifar, Mostafa and Raza, Shan E. Ahmed and Minhas, Fayyaz and Snead, David and Rajpoot, Nasir},
  year = {2022},
  month = nov,
  number = {arXiv:2203.00077},
  eprint = {2203.00077},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.00077},
  urldate = {2023-02-28},
  abstract = {The recent surge in performance for image analysis of digitised pathology slides can largely be attributed to the advances in deep learning. Deep models can be used to initially localise various structures in the tissue and hence facilitate the extraction of interpretable features for biomarker discovery. However, these models are typically trained for a single task and therefore scale poorly as we wish to adapt the model for an increasing number of different tasks. Also, supervised deep learning models are very data hungry and therefore rely on large amounts of training data to perform well. In this paper, we present a multi-task learning approach for segmentation and classification of nuclei, glands, lumina and different tissue regions that leverages data from multiple independent data sources. While ensuring that our tasks are aligned by the same tissue type and resolution, we enable meaningful simultaneous prediction with a single network. As a result of feature sharing, we also show that the learned representation can be used to improve the performance of additional tasks via transfer learning, including nuclear classification and signet ring cell detection. As part of this work, we train our developed Cerberus model on a huge amount of data, consisting of over 600K objects for segmentation and 440K patches for classification. We use our approach to process 599 colorectal whole-slide images from TCGA, where we localise 377 million, 900K and 2.1 million nuclei, glands and lumina, respectively and make the results available to the community for downstream analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/VY96MZQA/Graham et al. - One Model is All You Need - Multi-Task Learning Enables Simultaneous Histology Image Segmentation and Classification.pdf}
}

@article{guoVisualAttentionNetwork2022,
  title = {Visual {{Attention Network}}},
  author = {Guo, Meng-Hao and Lu, Cheng-Ze and Liu, Zheng-Ning and Cheng, Ming-Ming and Hu, Shi-Min},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.09741 [cs]},
  eprint = {2202.09741},
  primaryclass = {cs},
  urldate = {2022-02-24},
  abstract = {While originally designed for natural language processing (NLP) tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple and efficient, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at https://github.com/Visual-Attention-Network.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/FYHJ4ATA/Visual Attention Network - 2022 - Guo et al..pdf}
}

@misc{Hansprostatefusion,
  title = {Hans-Prostate-Fusion},
  urldate = {2023-02-28},
  howpublished = {https://new-ui.neptune.ai/o/artera/org/hans-prostate-fusion/runs/compare?viewId=98857baa-7e32-472a-bf06-4bc301d25c03\&detailsTab=charts\&shortId=HAN1-247\&dash=charts\&type=run\&compare=EwFg7ANGCcQ}
}

@article{heckenbachNuclearMorphologyDeep2022,
  title = {Nuclear Morphology Is a Deep Learning Biomarker of Cellular Senescence},
  author = {Heckenbach, Indra and Mkrtchyan, Garik V. and Ezra, Michael Ben and Bakula, Daniela and Madsen, Jakob Sture and Nielsen, Malte Hasle and Or{\'o}, Denise and Osborne, Brenna and Covarrubias, Anthony J. and Idda, M. Laura and Gorospe, Myriam and Mortensen, Laust and Verdin, Eric and Westendorp, Rudi and {Scheibye-Knudsen}, Morten},
  year = {2022},
  month = aug,
  journal = {Nature Aging},
  volume = {2},
  number = {8},
  pages = {742--755},
  publisher = {{Nature Publishing Group}},
  issn = {2662-8465},
  doi = {10.1038/s43587-022-00263-3},
  urldate = {2022-11-21},
  abstract = {Cellular senescence is an important factor in aging and many age-related diseases, but understanding its role in health is challenging due to the lack of exclusive or universal markers. Using neural networks, we predict senescence from the nuclear morphology of human fibroblasts with up to 95\% accuracy, and investigate murine astrocytes, murine neurons, and fibroblasts with premature aging in culture. After generalizing our approach, the predictor recognizes higher rates of senescence in p21-positive and ethynyl-2'-deoxyuridine (EdU)-negative nuclei in tissues and shows an increasing rate of senescent cells with age in H\&E-stained murine liver tissue and human dermal biopsies. Evaluating medical records reveals that higher rates of senescent cells correspond to decreased rates of malignant neoplasms and increased rates of osteoporosis, osteoarthritis, hypertension and cerebral infarction. In sum, we show that morphological alterations of the nucleus can serve as a deep learning predictor of senescence that is applicable across tissues and species and is associated with health outcomes in humans.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Ageing,Machine learning,Senescence},
  file = {/Users/Hans/Zotero/storage/2GW2FLFD/Heckenbach et al. - Nuclear morphology is a deep learning biomarker of cellular senescence.pdf}
}

@misc{heMaskedAutoencodersAre2021,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2021},
  month = dec,
  number = {arXiv:2111.06377},
  eprint = {2111.06377},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.06377},
  urldate = {2023-04-25},
  abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/ZGC7C4GJ/He et al. - Masked Autoencoders Are Scalable Vision Learners.pdf}
}

@article{heMomentumContrastUnsupervised2019,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year = {2019},
  month = nov,
  urldate = {2022-01-10},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/46WV75TZ/Momentum Contrast for Unsupervised Visual Representation Learning - 2019 - He et al..pdf}
}

@article{hintonForwardForwardAlgorithmPreliminary,
  title = {The {{Forward-Forward Algorithm}}: {{Some Preliminary Investigations}}},
  author = {Hinton, Geoffrey},
  abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth serious investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes can be separated in time, the negative passes can be done offline, which makes the learning much simpler in the positive pass and allows video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/A2PVKT89/Hinton - The Forward-Forward Algorithm Some Preliminary In.pdf}
}

@article{hofmannWSGADAPTAdjuvant2013,
  title = {{{WSG ADAPT}} \textendash{} Adjuvant Dynamic Marker-Adjusted Personalized Therapy Trial Optimizing Risk Assessment and Therapy Response Prediction in Early Breast Cancer: Study Protocol for a Prospective, Multi-Center, Controlled, Non-Blinded, Randomized, Investigator Initiated Phase {{II}}/{{III}} Trial},
  shorttitle = {{{WSG ADAPT}} \textendash{} Adjuvant Dynamic Marker-Adjusted Personalized Therapy Trial Optimizing Risk Assessment and Therapy Response Prediction in Early Breast Cancer},
  author = {Hofmann, Daniel and Nitz, Ulrike and Gluz, Oleg and Kates, Ronald E and Schinkoethe, Timo and Staib, Peter and Harbeck, Nadia},
  year = {2013},
  journal = {Trials},
  volume = {14},
  number = {1},
  pages = {261},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-14-261},
  urldate = {2022-08-18},
  abstract = {Background: Adjuvant treatment decision-making based on conventional clinical/pathological and prognostic single molecular markers or genomic signatures is a therapeutic area in which over-/under-treatment are still key clinical problems even though substantial and continuous improvement of outcome has been achieved over the past decades. Response to therapy is currently not considered in the decision-making procedure.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/VUUYEWGZ/Hofmann et al. - 2013 - WSG ADAPT  adjuvant dynamic marker-adjusted perso.pdf}
}

@article{holsteIMPROVEDMULTIMODALFUSION,
  title = {{{IMPROVED MULTIMODAL FUSION FOR SMALL DATASETS WITH AUXILIARY SUPERVISION}}},
  author = {Holste, Gregory and Mitani, Akinori and Esteva, Andre},
  abstract = {Prostate cancer is one of the leading causes of cancerrelated death in men worldwide. Like many cancers, diagnosis involves expert integration of heterogeneous patient information such as imaging, clinical risk factors, and more. For this reason, there have been many recent efforts toward deep multimodal fusion of image and non-image data for clinical decision tasks. Many of these studies propose methods to fuse learned features from each patient modality, providing significant downstream improvements with techniques like cross-modal attention gating, Kronecker product fusion, orthogonality regularization, and more. While these enhanced fusion operations can improve upon feature concatenation, they often come with an extremely high learning capacity, meaning they are likely to overfit when applied even to small or low-dimensional datasets. Rather than designing a highly expressive fusion operation, we propose three simple methods for improved multimodal fusion with small datasets that aid optimization by generating auxiliary sources of supervision during training: extra supervision, clinical prediction, and dense fusion. We validate the proposed approaches on prostate cancer diagnosis from paired histopathology imaging and tabular clinical features. The proposed methods are straightforward to implement and can be applied to any classification task with paired image and non-image data.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/L85T5VDN/Holste et al. - IMPROVED MULTIMODAL FUSION FOR SMALL DATASETS WITH.pdf}
}

@misc{hoyerMICMaskedImage2022,
  title = {{{MIC}}: {{Masked Image Consistency}} for {{Context-Enhanced Domain Adaptation}}},
  shorttitle = {{{MIC}}},
  author = {Hoyer, Lukas and Dai, Dengxin and Wang, Haoran and Van Gool, Luc},
  year = {2022},
  month = dec,
  number = {arXiv:2212.01322},
  eprint = {2212.01322},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.01322},
  urldate = {2022-12-06},
  abstract = {In unsupervised domain adaptation (UDA), a model trained on source data (e.g. synthetic) is adapted to target data (e.g. real-world) without access to target annotation. Most previous UDA methods struggle with classes that have a similar visual appearance on the target domain as no ground truth is available to learn the slight appearance differences. To address this problem, we propose a Masked Image Consistency (MIC) module to enhance UDA by learning spatial context relations of the target domain as additional clues for robust visual recognition. MIC enforces the consistency between predictions of masked target images, where random patches are withheld, and pseudo-labels that are generated based on the complete image by an exponential moving average teacher. To minimize the consistency loss, the network has to learn to infer the predictions of the masked regions from their context. Due to its simple and universal concept, MIC can be integrated into various UDA methods across different visual recognition tasks such as image classification, semantic segmentation, and object detection. MIC significantly improves the state-of-the-art performance across the different recognition tasks for synthetic-to-real, day-to-nighttime, and clear-to-adverse-weather UDA. For instance, MIC achieves an unprecedented UDA performance of 75.9 mIoU and 92.8\% on GTA-to-Cityscapes and VisDA-2017, respectively, which corresponds to an improvement of +2.1 and +3.0 percent points over the previous state of the art. The implementation is available at https://github.com/lhoyer/MIC.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/QHD2LNF5/Hoyer et al. - 2022 - MIC Masked Image Consistency for Context-Enhanced.pdf}
}

@misc{hoyerMICMaskedImage2022a,
  title = {{{MIC}}: {{Masked Image Consistency}} for {{Context-Enhanced Domain Adaptation}}},
  shorttitle = {{{MIC}}},
  author = {Hoyer, Lukas and Dai, Dengxin and Wang, Haoran and Van Gool, Luc},
  year = {2022},
  month = dec,
  number = {arXiv:2212.01322},
  eprint = {2212.01322},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-06},
  abstract = {In unsupervised domain adaptation (UDA), a model trained on source data (e.g. synthetic) is adapted to target data (e.g. real-world) without access to target annotation. Most previous UDA methods struggle with classes that have a similar visual appearance on the target domain as no ground truth is available to learn the slight appearance differences. To address this problem, we propose a Masked Image Consistency (MIC) module to enhance UDA by learning spatial context relations of the target domain as additional clues for robust visual recognition. MIC enforces the consistency between predictions of masked target images, where random patches are withheld, and pseudo-labels that are generated based on the complete image by an exponential moving average teacher. To minimize the consistency loss, the network has to learn to infer the predictions of the masked regions from their context. Due to its simple and universal concept, MIC can be integrated into various UDA methods across different visual recognition tasks such as image classification, semantic segmentation, and object detection. MIC significantly improves the state-of-the-art performance across the different recognition tasks for synthetic-to-real, day-to-nighttime, and clear-to-adverse-weather UDA. For instance, MIC achieves an unprecedented UDA performance of 75.9 mIoU and 92.8\% on GTA-to-Cityscapes and VisDA-2017, respectively, which corresponds to an improvement of +2.1 and +3.0 percent points over the previous state of the art. The implementation is available at https://github.com/lhoyer/MIC.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/22GU3HRB/Hoyer et al. - 2022 - MIC Masked Image Consistency for Context-Enhanced.pdf}
}

@article{hoyezUnsupervisedImagetoImageTranslation2022,
  title = {Unsupervised {{Image-to-Image Translation}}: {{A Review}}},
  shorttitle = {Unsupervised {{Image-to-Image Translation}}},
  author = {Hoyez, Henri and Schockaert, C{\'e}dric and Rambach, Jason and Mirbach, Bruno and Stricker, Didier},
  year = {2022},
  month = nov,
  journal = {Sensors},
  volume = {22},
  number = {21},
  pages = {8540},
  issn = {1424-8220},
  doi = {10.3390/s22218540},
  urldate = {2022-12-06},
  abstract = {Supervised image-to-image translation has been proven to generate realistic images with sharp details and to have good quantitative performance. Such methods are trained on a paired dataset, where an image from the source domain already has a corresponding translated image in the target domain. However, this paired dataset requirement imposes a huge practical constraint, requires domain knowledge or is even impossible to obtain in certain cases. Due to these problems, unsupervised image-to-image translation has been proposed, which does not require domain expertise and can take advantage of a large unlabeled dataset. Although such models perform well, they are hard to train due to the major constraints induced in their loss functions, which make training unstable. Since CycleGAN has been released, numerous methods have been proposed which try to address various problems from different perspectives. In this review, we firstly describe the general image-to-image translation framework and discuss the datasets and metrics involved in the topic. Furthermore, we revise the current state-of-the-art with a classification of existing works. This part is followed by a small quantitative evaluation, for which results were taken from papers.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/2RFE8AX6/Hoyez et al. - 2022 - Unsupervised Image-to-Image Translation A Review.pdf}
}

@misc{InterpretabilityExplainabilityMachine,
  title = {Interpretability and {{Explainability}}: {{A Machine Learning Zoo Mini-tour}}},
  shorttitle = {Interpretability and {{Explainability}}},
  journal = {OpenReview},
  urldate = {2022-11-21},
  abstract = {In this review, we examine the problem of designing interpretable and explainable machine learning models. Interpretability and explainability lie at the core of many machine learning and...},
  howpublished = {https://openreview.net/forum?id=ek8Zr3HKiSK},
  langid = {english}
}

@misc{javedRethinkingMachineLearning2022,
  title = {Rethinking {{Machine Learning Model Evaluation}} in {{Pathology}}},
  author = {Javed, Syed Ashar and Juyal, Dinkar and Shanis, Zahil and Chakraborty, Shreya and Pokkalla, Harsha and Prakash, Aaditya},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05205},
  eprint = {2204.05205},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2022-05-17},
  abstract = {Machine Learning has been applied to pathology images in research and clinical practice with promising outcomes. However, standard ML models often lack the rigorous evaluation required for clinical decisions. Machine learning techniques for natural images are ill-equipped to deal with pathology images that are significantly large and noisy, require expensive labeling, are hard to interpret, and are susceptible to spurious correlations. We propose a set of practical guidelines for ML evaluation in pathology that address the above concerns. The paper includes measures for setting up the evaluation framework, effectively dealing with variability in labels, and a recommended suite of tests to address issues related to domain shift, robustness, and confounding variables. We hope that the proposed framework will bridge the gap between ML researchers and domain experts, leading to wider adoption of ML techniques in pathology and improving patient outcomes.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/WI3LCP7A/Javed et al. - 2022 - Rethinking Machine Learning Model Evaluation in Pa.pdf}
}

@article{kadraWelltunedSimpleNets2021,
  title = {Well-Tuned {{Simple Nets Excel}} on {{Tabular Datasets}}},
  author = {Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
  year = {2021},
  month = jun,
  urldate = {2022-02-03},
  abstract = {Tabular datasets are the last "unconquered castle" for deep learning, with traditional ML methods like Gradient-Boosted Decision Trees still performing strongly even against recent specialized neural architectures. In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters. We empirically assess the impact of these regularization cocktails for MLPs in a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/THTYTGKN/Well-tuned Simple Nets Excel on Tabular Datasets - 2021 - Kadra et al..pdf}
}

@misc{kangParamNetParametervariableNetwork2023,
  title = {{{ParamNet}}: {{A Parameter-variable Network}} for {{Fast Stain Normalization}}},
  shorttitle = {{{ParamNet}}},
  author = {Kang, Hongtao and Luo, Die and Chen, Li and Hu, Junbo and Cheng, Shenghua and Quan, Tingwei and Zeng, Shaoqun and Liu, Xiuli},
  year = {2023},
  month = may,
  number = {arXiv:2305.06511},
  eprint = {2305.06511},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-05-17},
  abstract = {In practice, digital pathology images are often affected by various factors, resulting in very large differences in color and brightness. Stain normalization can effectively reduce the differences in color and brightness of digital pathology images, thus improving the performance of computer-aided diagnostic systems. Conventional stain normalization methods rely on one or several reference images, but one or several images are difficult to represent the entire dataset. Although learning-based stain normalization methods are a general approach, they use complex deep networks, which not only greatly reduce computational efficiency, but also risk introducing artifacts. StainNet is a fast and robust stain normalization network, but it has not a sufficient capability for complex stain normalization due to its too simple network structure. In this study, we proposed a parameter-variable stain normalization network, ParamNet. ParamNet contains a parameter prediction sub-network and a color mapping sub-network, where the parameter prediction sub-network can automatically determine the appropriate parameters for the color mapping sub-network according to each input image. The feature of parameter variable ensures that our network has a sufficient capability for various stain normalization tasks. The color mapping sub-network is a fully 1x1 convolutional network with a total of 59 variable parameters, which allows our network to be extremely computationally efficient and does not introduce artifacts. The results on cytopathology and histopathology datasets show that our ParamNet outperforms state-of-the-art methods and can effectively improve the generalization of classifiers on pathology diagnosis tasks. The code has been available at https://github.com/khtao/ParamNet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/Q6Q8YH58/Kang et al. - ParamNet - A Parameter-variable Network for Fast Stain Normalization.pdf}
}

@article{konerOODformerOutOfDistributionDetection2021,
  title = {{{OODformer}}: {{Out-Of-Distribution Detection Transformer}}},
  shorttitle = {{{OODformer}}},
  author = {Koner, Rajat and Sinhamahapatra, Poulami and Roscher, Karsten and G{\"u}nnemann, Stephan and Tresp, Volker},
  year = {2021},
  month = dec,
  journal = {arXiv:2107.08976 [cs]},
  eprint = {2107.08976},
  primaryclass = {cs},
  urldate = {2021-12-14},
  abstract = {A serious problem in image classification is that a trained model might perform well for input data that originates from the same distribution as the data available for model training, but performs much worse for out-of-distribution (OOD) samples. In real-world safety-critical applications, in particular, it is important to be aware if a new data point is OOD. To date, OOD detection is typically addressed using either confidence scores, autoencoder based reconstruction, or contrastive learning. However, the global image context has not yet been explored to discriminate the non-local objectness between in-distribution and OOD samples. This paper proposes a first-of-its-kind OOD detection architecture named OODformer that leverages the contextualization capabilities of the transformer. Incorporating the transformer as the principal feature extractor allows us to exploit the object concepts and their discriminatory attributes along with their co-occurrence via visual attention. Based on contextualised embedding, we demonstrate OOD detection using both class-conditioned latent space similarity and a network confidence score. Our approach shows improved generalizability across various datasets. We have achieved a new state-of-the-art result on CIFAR-10/-100 and ImageNet30. Code is available at : https://github.com/rajatkoner08/oodformer.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/WBFZPDCC/OODformer - 2021 - Koneret al..pdf}
}

@misc{kwonDiffusionbasedImageTranslation2022,
  title = {Diffusion-Based {{Image Translation}} Using {{Disentangled Style}} and {{Content Representation}}},
  author = {Kwon, Gihyun and Ye, Jong Chul},
  year = {2022},
  month = sep,
  number = {arXiv:2209.15264},
  eprint = {2209.15264},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.15264},
  urldate = {2022-12-06},
  abstract = {Diffusion-based image translation guided by semantic texts or a single target image has enabled flexible style transfer which is not limited to the specific domains. Unfortunately, due to the stochastic nature of diffusion models, it is often difficult to maintain the original content of the image during the reverse diffusion. To address this, here we present a novel diffusion-based unsupervised image translation method using disentangled style and content representation. Specifically, inspired by the splicing Vision Transformer, we extract intermediate keys of multihead self attention layer from ViT model and used them as the content preservation loss. Then, an image guided style transfer is performed by matching the [CLS] classification token from the denoised samples and target image, whereas additional CLIP loss is used for the text-driven style transfer. To further accelerate the semantic change during the reverse diffusion, we also propose a novel semantic divergence loss and resampling strategy. Our experimental results show that the proposed method outperforms state-of-the-art baseline models in both text-guided and image-guided translation tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/Hans/Zotero/storage/KYCJYY8J/Kwon and Ye - 2022 - Diffusion-based Image Translation using Disentangl.pdf}
}

@article{lafargeLearningDomainInvariantRepresentations2019,
  title = {Learning {{Domain-Invariant Representations}} of {{Histological Images}}},
  author = {Lafarge, Maxime W. and Pluim, Josien P. W. and Eppenhof, Koen A. J. and Veta, Mitko},
  year = {2019},
  journal = {Frontiers in Medicine},
  volume = {6},
  pages = {162},
  issn = {2296-858X},
  doi = {10.3389/fmed.2019.00162},
  abstract = {Histological images present high appearance variability due to inconsistent latent parameters related to the preparation and scanning procedure of histological slides, as well as the inherent biological variability of tissues. Machine-learning models are trained with images from a limited set of domains, and are expected to generalize to images from unseen domains. Methodological design choices have to be made in order to yield domain invariance and proper generalization. In digital pathology, standard approaches focus either on ad-hoc normalization of the latent parameters based on prior knowledge, such as staining normalization, or aim at anticipating new variations of these parameters via data augmentation. Since every histological image originates from a unique data distribution, we propose to consider every histological slide of the training data as a domain and investigated the alternative approach of domain-adversarial training to learn features that are invariant to this available domain information. We carried out a comparative analysis with staining normalization and data augmentation on two different tasks: generalization to images acquired in unseen pathology labs for mitosis detection and generalization to unseen organs for nuclei segmentation. We report that the utility of each method depends on the type of task and type of data variability present at training and test time. The proposed framework for domain-adversarial training is able to improve generalization performances on top of conventional methods.},
  langid = {english},
  pmcid = {PMC6646468},
  pmid = {31380377},
  keywords = {domain-adversarial convolutional network,domain-invariant representation,histopathology image analysis,mitosis detection,nuclei segmentation},
  file = {/Users/Hans/Zotero/storage/97PT2ZYK/Lafarge et al. - 2019 - Learning Domain-Invariant Representations of Histo.pdf}
}

@article{larsenHowManyDegrees2021,
  title = {How Many Degrees of Freedom Do We Need to Train Deep Networks: A Loss Landscape Perspective},
  shorttitle = {How Many Degrees of Freedom Do We Need to Train Deep Networks},
  author = {Larsen, Brett W. and Fort, Stanislav and Becker, Nic and Ganguli, Surya},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.05802 [cs, stat]},
  eprint = {2107.05802},
  primaryclass = {cs, stat},
  urldate = {2021-12-14},
  abstract = {A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We explain this phenomenon by first examining the success probability of hitting a training loss sublevel set when training within a random subspace of a given training dimensionality. We find a sharp phase transition in the success probability from 0 to 1 as the training dimension surpasses a threshold. This threshold training dimension increases as the desired final loss decreases, but decreases as the initial loss decreases. We then theoretically explain the origin of this phase transition, and its dependence on initialization and final desired loss, in terms of precise properties of the high dimensional geometry of the loss landscape. In particular, we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired loss sub-level set, projected onto a unit sphere surrounding the initialization, must exceed the total number of parameters for the success probability to be large. In several architectures and datasets, we measure the threshold training dimension as a function of initialization and demonstrate that it is a small fraction of the total number of parameters, thereby implying, by our theory, that successful training with so few dimensions is possible precisely because the Gaussian width of low loss sub-level sets is very large. Moreover, this threshold training dimension provides a strong null model for assessing the efficacy of more sophisticated ways to reduce training degrees of freedom, including lottery tickets as well a more optimal method we introduce: lottery subspaces.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/Hans/Zotero/storage/2PNBMREW/s41467-021-24025-8.pdf;/Users/Hans/Zotero/storage/ZUKLTCAE/How many degrees of freedom do we need to train deep networks - 2021 - Larsenet al..pdf}
}

@misc{leeDRITDiverseImagetoImage2019,
  title = {{{DRIT}}++: {{Diverse Image-to-Image Translation}} via {{Disentangled Representations}}},
  shorttitle = {{{DRIT}}++},
  author = {Lee, Hsin-Ying and Tseng, Hung-Yu and Mao, Qi and Huang, Jia-Bin and Lu, Yu-Ding and Singh, Maneesh and Yang, Ming-Hsuan},
  year = {2019},
  month = dec,
  number = {arXiv:1905.01270},
  eprint = {1905.01270},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.01270},
  urldate = {2022-11-29},
  abstract = {Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for this task: 1) lack of aligned training pairs and 2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for generating diverse outputs without paired training images. To synthesize diverse outputs, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Our model takes the encoded content features extracted from a given input and attribute vectors sampled from the attribute space to synthesize diverse outputs at test time. To handle unpaired training data, we introduce a cross-cycle consistency loss based on disentangled representations. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks without paired training data. For quantitative evaluations, we measure realism with user study and Fr\textbackslash '\{e\}chet inception distance, and measure diversity with the perceptual distance metric, Jensen-Shannon divergence, and number of statistically-different bins.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/GDRWZFXF/Lee et al. - 2019 - DRIT++ Diverse Image-to-Image Translation via Dis.pdf}
}

@article{linMultiplexed3DAtlas2023,
  title = {Multiplexed {{3D}} Atlas of State Transitions and Immune Interaction in Colorectal Cancer},
  author = {Lin, Jia-Ren and Wang, Shu and Coy, Shannon and Chen, Yu-An and Yapp, Clarence and Tyler, Madison and Nariya, Maulik K. and Heiser, Cody N. and Lau, Ken S. and Santagata, Sandro and Sorger, Peter K.},
  year = {2023},
  month = jan,
  journal = {Cell},
  volume = {186},
  number = {2},
  pages = {363-381.e19},
  issn = {00928674},
  doi = {10.1016/j.cell.2022.12.028},
  urldate = {2023-01-26},
  abstract = {Advanced solid cancers are complex assemblies of tumor, immune, and stromal cells characterized by high intratumoral variation. We use highly multiplexed tissue imaging, 3D reconstruction, spatial statistics, and machine learning to identify cell types and states underlying morphological features of known diagnostic and prognostic significance in colorectal cancer. Quantitation of these features in high-plex marker space reveals recurrent transitions from one tumor morphology to the next, some of which are coincident with longrange gradients in the expression of oncogenes and epigenetic regulators. At the tumor invasive margin, where tumor, normal, and immune cells compete, T cell suppression involves multiple cell types and 3D imaging shows that seemingly localized 2D features such as tertiary lymphoid structures are commonly interconnected and have graded molecular properties. Thus, while cancer genetics emphasizes the importance of discrete changes in tumor state, whole-specimen imaging reveals large-scale morphological and molecular gradients analogous to those in developing tissues.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/TYUAKVXW/Lin et al. - 2023 - Multiplexed 3D atlas of state transitions and immu.pdf}
}

@article{liSingleImageSuperresolution2021,
  title = {Single Image Super-Resolution for Whole Slide Image Using Convolutional Neural Networks and Self-Supervised Color Normalization},
  author = {Li, Bin and Keikhosravi, Adib and Loeffler, Agnes G. and Eliceiri, Kevin W.},
  year = {2021},
  month = feb,
  journal = {Medical Image Analysis},
  volume = {68},
  pages = {101938},
  issn = {1361-8423},
  doi = {10.1016/j.media.2020.101938},
  abstract = {High-quality whole slide scanners used for animal and human pathology scanning are expensive and can produce massive datasets, which limits the access to and adoption of this technique. As a potential solution to these challenges, we present a deep learning-based approach making use of single image super-resolution (SISR) to reconstruct high-resolution histology images from low-resolution inputs. Such low-resolution images can easily be shared, require less storage, and can be acquired quickly using widely available low-cost slide scanners. The network consists of multi-scale fully convolutional networks capable of capturing hierarchical features. Conditional generative adversarial loss is incorporated to penalize blurriness in the output images. The network is trained using a progressive strategy where the scaling factor is sampled from a normal distribution with an increasing mean. The results are evaluated with quantitative metrics and are used in a clinical histopathology diagnosis procedure which shows that the SISR framework can be used to reconstruct high-resolution images with clinical level quality. We further propose a self-supervised color normalization method that can remove staining variation artifacts. Quantitative evaluations show that the SISR framework can generalize well on unseen data collected from other patient tissue cohorts by incorporating the color normalization method.},
  langid = {english},
  pmid = {33359932},
  keywords = {Convolutional neural network,Digital pathology,Generative adversarial networks,Histological Techniques,Humans,Magnetic Resonance Imaging,{Neural Networks, Computer},Super-resolution}
}

@misc{liuAdaptiveEarlyLearningCorrection2022,
  title = {Adaptive {{Early-Learning Correction}} for {{Segmentation}} from {{Noisy Annotations}}},
  author = {Liu, Sheng and Liu, Kangning and Zhu, Weicheng and Shen, Yiqiu and {Fernandez-Granda}, Carlos},
  year = {2022},
  month = mar,
  number = {arXiv:2110.03740},
  eprint = {2110.03740},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.03740},
  urldate = {2022-11-21},
  abstract = {Deep learning in the presence of noisy annotations has been studied extensively in classification, but much less in segmentation tasks. In this work, we study the learning dynamics of deep segmentation networks trained on inaccurately-annotated data. We discover a phenomenon that has been previously reported in the context of classification: the networks tend to first fit the clean pixel-level labels during an "early-learning" phase, before eventually memorizing the false annotations. However, in contrast to classification, memorization in segmentation does not arise simultaneously for all semantic categories. Inspired by these findings, we propose a new method for segmentation from noisy annotations with two key elements. First, we detect the beginning of the memorization phase separately for each category during training. This allows us to adaptively correct the noisy annotations in order to exploit early learning. Second, we incorporate a regularization term that enforces consistency across scales to boost robustness against annotation noise. Our method outperforms standard approaches on a medical-imaging segmentation task where noises are synthesized to mimic human annotation errors. It also provides robustness to realistic noisy annotations present in weakly-supervised semantic segmentation, achieving state-of-the-art results on PASCAL VOC 2012. Code is available at https://github.com/Kangningthu/ADELE},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/6N8NX8RI/Liu et al. - Adaptive Early-Learning Correction for Segmentation from Noisy Annotations.pdf}
}

@misc{liuConvolutionalNormalizationImproving2022,
  title = {Convolutional {{Normalization}}: {{Improving Deep Convolutional Network Robustness}} and {{Training}}},
  shorttitle = {Convolutional {{Normalization}}},
  author = {Liu, Sheng and Li, Xiao and Zhai, Yuexiang and You, Chong and Zhu, Zhihui and {Fernandez-Granda}, Carlos and Qu, Qing},
  year = {2022},
  month = jan,
  number = {arXiv:2103.00673},
  eprint = {2103.00673},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.00673},
  urldate = {2022-11-21},
  abstract = {Normalization techniques have become a basic component in modern convolutional neural networks (ConvNets). In particular, many recent works demonstrate that promoting the orthogonality of the weights helps train deep models and improve robustness. For ConvNets, most existing methods are based on penalizing or normalizing weight matrices derived from concatenating or flattening the convolutional kernels. These methods often destroy or ignore the benign convolutional structure of the kernels; therefore, they are often expensive or impractical for deep ConvNets. In contrast, we introduce a simple and efficient "Convolutional Normalization" (ConvNorm) method that can fully exploit the convolutional structure in the Fourier domain and serve as a simple plug-and-play module to be conveniently incorporated into any ConvNets. Our method is inspired by recent work on preconditioning methods for convolutional sparse coding and can effectively promote each layer's channel-wise isometry. Furthermore, we show that our ConvNorm can reduce the layerwise spectral norm of the weight matrices and hence improve the Lipschitzness of the network, leading to easier training and improved robustness for deep ConvNets. Applied to classification under noise corruptions and generative adversarial network (GAN), we show that the ConvNorm improves the robustness of common ConvNets such as ResNet and the performance of GAN. We verify our findings via numerical experiments on CIFAR and ImageNet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/YVELR6IH/Liu et al. - Convolutional Normalization.pdf}
}

@article{liuEarlyLearningRegularizationPrevents,
  title = {Early-{{Learning Regularization Prevents Memorization}} of {{Noisy Labels}}},
  author = {Liu, Sheng and {Niles-Weed}, Jonathan and Razavian, Narges and {Fernandez-Granda}, Carlos},
  pages = {12},
  abstract = {We propose a novel framework to perform classification via deep learning in the presence of noisy annotations. When trained on noisy labels, deep neural networks have been observed to first fit the training data with clean labels during an ``early learning'' phase, before eventually memorizing the examples with false labels. We prove that early learning and memorization are fundamental phenomena in high-dimensional classification tasks, even in simple linear models, and give a theoretical explanation in this setting. Motivated by these findings, we develop a new technique for noisy classification tasks, which exploits the progress of the early learning phase. In contrast with existing approaches, which use the model output during early learning to detect the examples with clean labels, and either ignore or attempt to correct the false labels, we take a different route and instead capitalize on early learning via regularization. There are two key elements to our approach. First, we leverage semi-supervised learning techniques to produce target probabilities based on the model outputs. Second, we design a regularization term that steers the model towards these targets, implicitly preventing memorization of the false labels. The resulting framework is shown to provide robustness to noisy annotations on several standard benchmarks and real-world datasets, where it achieves results comparable to the state of the art.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/VDQ5JD6H/Liu et al. - Early-Learning Regularization Prevents Memorizatio.pdf}
}

@article{liuEarlyLearningRegularizationPrevents2020,
  title = {Early-{{Learning Regularization Prevents Memorization}} of {{Noisy Labels}}},
  author = {Liu, Sheng and {Niles-Weed}, Jonathan and Razavian, Narges and {Fernandez-Granda}, Carlos},
  year = {2020},
  month = jun,
  urldate = {2022-02-22},
  abstract = {We propose a novel framework to perform classification via deep learning in the presence of noisy annotations. When trained on noisy labels, deep neural networks have been observed to first fit the training data with clean labels during an "early learning" phase, before eventually memorizing the examples with false labels. We prove that early learning and memorization are fundamental phenomena in high-dimensional classification tasks, even in simple linear models, and give a theoretical explanation in this setting. Motivated by these findings, we develop a new technique for noisy classification tasks, which exploits the progress of the early learning phase. In contrast with existing approaches, which use the model output during early learning to detect the examples with clean labels, and either ignore or attempt to correct the false labels, we take a different route and instead capitalize on early learning via regularization. There are two key elements to our approach. First, we leverage semi-supervised learning techniques to produce target probabilities based on the model outputs. Second, we design a regularization term that steers the model towards these targets, implicitly preventing memorization of the false labels. The resulting framework is shown to provide robustness to noisy annotations on several standard benchmarks and real-world datasets, where it achieves results comparable to the state of the art.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/UQPIA5HP/Liu et al. - 2020 - Early-Learning Regularization Prevents Memorizatio.pdf}
}

@misc{liuEvolvingNormalizationActivationLayers2020,
  title = {Evolving {{Normalization-Activation Layers}}},
  author = {Liu, Hanxiao and Brock, Andrew and Simonyan, Karen and Le, Quoc V.},
  year = {2020},
  month = jul,
  number = {arXiv:2004.02967},
  eprint = {2004.02967},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-11-01},
  abstract = {Normalization layers and activation functions are fundamental components in deep networks and typically co-locate with each other. Here we propose to design them using an automated approach. Instead of designing them separately, we unify them into a single tensor-to-tensor computation graph, and evolve its structure starting from basic mathematical functions. Examples of such mathematical functions are addition, multiplication and statistical moments. The use of low-level mathematical functions, in contrast to the use of high-level modules in mainstream NAS, leads to a highly sparse and large search space which can be challenging for search methods. To address the challenge, we develop efficient rejection protocols to quickly filter out candidate layers that do not work well. We also use multi-objective evolution to optimize each layer's performance across many architectures to prevent overfitting. Our method leads to the discovery of EvoNorms, a set of new normalization-activation layers with novel, and sometimes surprising structures that go beyond existing design patterns. For example, some EvoNorms do not assume that normalization and activation functions must be applied sequentially, nor need to center the feature maps, nor require explicit activation functions. Our experiments show that EvoNorms work well on image classification models including ResNets, MobileNets and EfficientNets but also transfer well to Mask R-CNN with FPN/SpineNet for instance segmentation and to BigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers in many cases.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/Hans/Zotero/storage/UIWUD2ZY/Liu et al. - 2020 - Evolving Normalization-Activation Layers.pdf}
}

@article{liuLearningDisentangledRepresentations2022,
  title = {Learning {{Disentangled Representations}} in the {{Imaging Domain}}},
  author = {Liu, Xiao and Sanchez, Pedro and Thermos, Spyridon and O'Neil, Alison Q. and Tsaftaris, Sotirios A.},
  year = {2022},
  month = aug,
  journal = {Medical Image Analysis},
  volume = {80},
  eprint = {2108.12043},
  primaryclass = {cs},
  pages = {102516},
  issn = {13618415},
  doi = {10.1016/j.media.2022.102516},
  urldate = {2022-12-06},
  abstract = {Disentangled representation learning has been proposed as an approach to learning general representations even in the absence of, or with limited, supervision. A good general representation can be fine-tuned for new target tasks using modest amounts of data, or used directly in unseen domains achieving remarkable performance in the corresponding task. This alleviation of the data and annotation requirements offers tantalising prospects for applications in computer vision and healthcare. In this tutorial paper, we motivate the need for disentangled representations, revisit key concepts, and describe practical building blocks and criteria for learning such representations. We survey applications in medical imaging emphasising choices made in exemplar key works, and then discuss links to computer vision applications. We conclude by presenting limitations, challenges, and opportunities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/6F85TUPQ/Liu et al. - 2022 - Learning Disentangled Representations in the Imagi.pdf}
}

@misc{liuPixMIMRethinkingPixel2023,
  title = {{{PixMIM}}: {{Rethinking Pixel Reconstruction}} in {{Masked Image Modeling}}},
  shorttitle = {{{PixMIM}}},
  author = {Liu, Yuan and Zhang, Songyang and Chen, Jiacheng and Chen, Kai and Lin, Dahua},
  year = {2023},
  month = mar,
  number = {arXiv:2303.02416},
  eprint = {2303.02416},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.02416},
  urldate = {2023-04-25},
  abstract = {Masked Image Modeling (MIM) has achieved promising progress with the advent of Masked Autoencoders (MAE) and BEiT. However, subsequent works have complicated the framework with new auxiliary tasks or extra pre-trained models, inevitably increasing computational overhead. This paper undertakes a fundamental analysis of MIM from the perspective of pixel reconstruction, which examines the input image patches and reconstruction target, and highlights two critical but previously overlooked bottlenecks. Based on this analysis, we propose a remarkably simple and effective method, \{\textbackslash ourmethod\}, that entails two strategies: 1) filtering the high-frequency components from the reconstruction target to de-emphasize the network's focus on texture-rich details and 2) adopting a conservative data transform strategy to alleviate the problem of missing foreground in MIM training. \{\textbackslash ourmethod\} can be easily integrated into most existing pixel-based MIM approaches (\textbackslash ie, using raw images as reconstruction target) with negligible additional computation. Without bells and whistles, our method consistently improves three MIM approaches, MAE, ConvMAE, and LSMAE, across various downstream tasks. We believe this effective plug-and-play method will serve as a strong baseline for self-supervised learning and provide insights for future improvements of the MIM framework. Code and models are available at \textbackslash url\{https://github.com/open-mmlab/mmselfsup/tree/dev-1.x/configs/selfsup/pixmim\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/L69L3XEQ/Liu et al. - PixMIM - Rethinking Pixel Reconstruction in Masked Image Modeling.pdf}
}

@misc{liuSeparatingContentStyle2021,
  title = {Separating {{Content}} and {{Style}} for {{Unsupervised Image-to-Image Translation}}},
  author = {Liu, Yunfei and Wang, Haofei and Yue, Yang and Lu, Feng},
  year = {2021},
  month = oct,
  number = {arXiv:2110.14404},
  eprint = {2110.14404},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.14404},
  urldate = {2022-12-06},
  abstract = {Unsupervised image-to-image translation aims to learn the mapping between two visual domains with unpaired samples. Existing works focus on disentangling domain-invariant content code and domain-specific style code individually for multimodal purposes. However, less attention has been paid to interpreting and manipulating the translated image. In this paper, we propose to separate the content code and style code simultaneously in a unified framework. Based on the correlation between the latent features and the high-level domain-invariant tasks, the proposed framework demonstrates superior performance in multimodal translation, interpretability and manipulation of the translated image. Experimental results show that the proposed approach outperforms the existing unsupervised image translation methods in terms of visual quality and diversity.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/2F5ZWLZR/Liu et al. - 2021 - Separating Content and Style for Unsupervised Imag.pdf}
}

@misc{liuSmoothingDisentangledLatent2021,
  title = {Smoothing the {{Disentangled Latent Style Space}} for {{Unsupervised Image-to-Image Translation}}},
  author = {Liu, Yahui and Sangineto, Enver and Chen, Yajing and Bao, Linchao and Zhang, Haoxian and Sebe, Nicu and Lepri, Bruno and Wang, Wei and De Nadai, Marco},
  year = {2021},
  month = jun,
  number = {arXiv:2106.09016},
  eprint = {2106.09016},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.09016},
  urldate = {2022-12-06},
  abstract = {Image-to-Image (I2I) multi-domain translation models are usually evaluated also using the quality of their semantic interpolation results. However, state-of-the-art models frequently show abrupt changes in the image appearance during interpolation, and usually perform poorly in interpolations across domains. In this paper, we propose a new training protocol based on three specific losses which help a translation network to learn a smooth and disentangled latent style space in which: 1) Both intra- and inter-domain interpolations correspond to gradual changes in the generated images and 2) The content of the source image is better preserved during the translation. Moreover, we propose a novel evaluation metric to properly measure the smoothness of latent style space of I2I translation models. The proposed method can be plugged into existing translation approaches, and our extensive experiments on different datasets show that it can significantly boost the quality of the generated images and the graduality of the interpolations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/Z8KL2ZRX/Liu et al. - 2021 - Smoothing the Disentangled Latent Style Space for .pdf}
}

@misc{liWhacAMoleDilemmaShortcuts2022,
  title = {A {{Whac-A-Mole Dilemma}}: {{Shortcuts Come}} in {{Multiples Where Mitigating One Amplifies Others}}},
  shorttitle = {A {{Whac-A-Mole Dilemma}}},
  author = {Li, Zhiheng and Evtimov, Ivan and Gordo, Albert and Hazirbas, Caner and Hassner, Tal and Ferrer, Cristian Canton and Xu, Chenliang and Ibrahim, Mark},
  year = {2022},
  month = dec,
  number = {arXiv:2212.04825},
  eprint = {2212.04825},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.04825},
  urldate = {2022-12-12},
  abstract = {Machine learning models have been found to learn shortcuts -- unintended decision rules that are unable to generalize -- undermining models' reliability. Previous works address this problem under the tenuous assumption that only a single shortcut exists in the training data. Real-world images are rife with multiple visual cues from background to texture. Key to advancing the reliability of vision systems is understanding whether existing methods can overcome multiple shortcuts or struggle in a Whac-A-Mole game, i.e., where mitigating one shortcut amplifies reliance on others. To address this shortcoming, we propose two benchmarks: 1) UrbanCars, a dataset with precisely controlled spurious cues, and 2) ImageNet-W, an evaluation set based on ImageNet for watermark, a shortcut we discovered affects nearly every modern vision model. Along with texture and background, ImageNet-W allows us to study multiple shortcuts emerging from training on natural images. We find computer vision models, including large foundation models -- regardless of training set, architecture, and supervision -- struggle when multiple shortcuts are present. Even methods explicitly designed to combat shortcuts struggle in a Whac-A-Mole dilemma. To tackle this challenge, we propose Last Layer Ensemble, a simple-yet-effective method to mitigate multiple shortcuts without Whac-A-Mole behavior. Our results surface multi-shortcut mitigation as an overlooked challenge critical to advancing the reliability of vision systems. The datasets and code are released: https://github.com/facebookresearch/Whac-A-Mole.git.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/V7QXD62Z/Li et al. - A Whac-A-Mole Dilemma - Shortcuts Come in Multiples Where Mitigating One Amplifies Others.pdf}
}

@misc{loweComplexValuedAutoencodersObject2022,
  title = {Complex-{{Valued Autoencoders}} for {{Object Discovery}}},
  author = {L{\"o}we, Sindy and Lippe, Phillip and Rudolph, Maja and Welling, Max},
  year = {2022},
  month = nov,
  number = {arXiv:2204.02075},
  eprint = {2204.02075},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.02075},
  urldate = {2022-11-23},
  abstract = {Object-centric representations form the basis of human perception, and enable us to reason about the world and to systematically generalize to new settings. Currently, most works on unsupervised object discovery focus on slot-based approaches, which explicitly separate the latent representations of individual objects. While the result is easily interpretable, it usually requires the design of involved architectures. In contrast to this, we propose a comparatively simple approach - the Complex AutoEncoder (CAE) - that creates distributed object-centric representations. Following a coding scheme theorized to underlie object representations in biological neurons, its complex-valued activations represent two messages: their magnitudes express the presence of a feature, while the relative phase differences between neurons express which features should be bound together to create joint object representations. In contrast to previous approaches using complex-valued activations for object discovery, we present a fully unsupervised approach that is trained end-to-end - resulting in significant improvements in performance and efficiency. Further, we show that the CAE achieves competitive or better unsupervised object discovery performance on simple multi-object datasets compared to a state-of-the-art slot-based approach while being up to 100 times faster to train.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/F5NIVLNI/Lwe et al. - Complex-Valued Autoencoders for Object Discovery.pdf}
}

@inproceedings{mahapatraStructurePreservingStain2020,
  title = {Structure {{Preserving Stain Normalization}} of {{Histopathology Images Using Self Supervised Semantic Guidance}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} \textendash{} {{MICCAI}} 2020},
  author = {Mahapatra, Dwarikanath and Bozorgtabar, Behzad and Thiran, Jean-Philippe and Shao, Ling},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {309--319},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-59722-1\_30},
  abstract = {Although generative adversarial network (GAN) based style transfer is state of the art in histopathology color-stain normalization, they do not explicitly integrate structural information of tissues. We propose a self-supervised approach to incorporate semantic guidance into a GAN based stain normalization framework and preserve detailed structural information. Our method does not require manual segmentation maps which is a significant advantage over existing methods. We integrate semantic information at different layers between a pre-trained semantic network and the stain color normalization network. The proposed scheme outperforms other color normalization methods leading to better classification and segmentation performance.},
  isbn = {978-3-030-59722-1},
  langid = {english},
  keywords = {Color normalization,Digital pathology,GANs,Semantic guidance},
  file = {/Users/Hans/Zotero/storage/ZD9CF9JQ/Mahapatra et al. - 2020 - Structure Preserving Stain Normalization of Histop.pdf}
}

@article{mariniEadversarialNetworkConvolutional2021,
  title = {H\&{{E-adversarial}} Network: A Convolutional Neural Network to Learn Stain-Invariant Features through {{Hematoxylin}} \& {{Eosin}} Regression},
  shorttitle = {H\&{{E-adversarial}} Network},
  author = {Marini, Niccol{\`o} and Atzori, Manfredo and Ot{\'a}lora Montenegro, Juan and {Marchand-Maillet}, Stephane and M{\"u}ller, Henning},
  year = {2021},
  month = sep,
  abstract = {Computational pathology is a domain that aims to develop algorithms to automatically analyze large digitized histopathology images, called whole slide images (WSI). WSIs are produced scanning thin tissue samples that are stained to make specific structures visible. They show stain colour heterogeneity due to different preparation and scanning settings applied across medical centers. Stain colour heterogeneity is a problem to train convolutional neural networks (CNN), the state-of-the-art algorithms for most computational pathology tasks, since CNNs usually underper-form when tested on images including different stain variations than those within data used to train the CNN. Despite several methods that were developed, stain colour hetero-geneity is still an unsolved challenge that limits the development of CNNs that can generalize on data from several medical centers. This paper aims to present a novel method to train CNNs that better generalize on data including several colour variations. The method, called H\&E-adversarial CNN, exploits H\&E matrix information to learn stain-invariant features during the training. The method is evaluated on the classification of colon and prostate histopathol-ogy images, involving eleven heterogeneous datasets, and compared with five other techniques used to handle stain colour heterogeneity. H\&E-adversarial CNNs show an improvement in performance compared to the other algorithms , demonstrating that it can help to better deal with stain colour heterogeneous images. * niccolo.marini@hevs.ch},
  file = {/Users/Hans/Zotero/storage/JJ3CLJPJ/H&E-adversarial network - 2021 - Marini et al..pdf}
}

@book{martinCleanCodeHandbook2009,
  title = {Clean Code: A Handbook of Agile Software Craftsmanship},
  shorttitle = {Clean Code},
  editor = {Martin, Robert C.},
  year = {2009},
  publisher = {{Prentice Hall}},
  address = {{Upper Saddle River, NJ}},
  isbn = {978-0-13-235088-4},
  langid = {english},
  lccn = {QA76.76.D47 C583 2009},
  keywords = {Agile software development,Computer software,Reliability},
  file = {/Users/Hans/Zotero/storage/6U63SATG/Clean code - 2009 - Martin.pdf}
}

@article{martinPredictingTrendsQuality2021,
  title = {Predicting Trends in the Quality of State-of-the-Art Neural Networks without Access to Training or Testing Data},
  author = {Martin, Charles H. and Peng, Tongsu and Mahoney, Michael W.},
  year = {2021},
  month = dec,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {4122},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-24025-8},
  urldate = {2022-08-10},
  abstract = {Abstract             In many applications, one works with neural network models trained by someone else. For such pretrained models, one may not have access to training data or test data. Moreover, one may not know details about the model, e.g., the specifics of the training data, the loss function, the hyperparameter values, etc. Given one or many pretrained models, it is a challenge to say anything about the expected performance or quality of the models. Here, we address this challenge by providing a detailed meta-analysis of hundreds of publicly available pretrained models. We examine norm-based capacity control metrics as well as power law based metrics from the recently-developed Theory of Heavy-Tailed Self Regularization. We find that norm based metrics correlate well with reported test accuracies for well-trained models, but that they often cannot distinguish well-trained versus poorly trained models. We also find that power law based metrics can do much better\textemdash quantitatively better at discriminating among series of well-trained models with a given architecture; and qualitatively better at discriminating well-trained versus poorly trained models. These methods can be used to identify when a pretrained neural network has problems that cannot be detected simply by examining training/test accuracies.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/C3679EBP/s41467-021-24025-8-2.pdf}
}

@book{mcconnellCodeComplete2004,
  title = {Code Complete},
  author = {McConnell, Steve},
  year = {2004},
  edition = {2nd ed},
  publisher = {{Microsoft Press}},
  address = {{Redmond, Wash}},
  isbn = {978-0-7356-1967-8},
  langid = {english},
  lccn = {QA76.76.D47 M39 2004},
  keywords = {Computer software,Development,{Handbooks, manuals, etc}},
  file = {/Users/Hans/Zotero/storage/4ZRMEIJ2/Code complete - 2004 - McConnell.pdf}
}

@misc{meinkeProvablyRobustDetection2022,
  title = {Provably {{Robust Detection}} of {{Out-of-distribution Data}} (Almost) for Free},
  author = {Meinke, Alexander and Bitterwolf, Julian and Hein, Matthias},
  year = {2022},
  month = oct,
  number = {arXiv:2106.04260},
  eprint = {2106.04260},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.04260},
  urldate = {2022-12-08},
  abstract = {The application of machine learning in safety-critical systems requires a reliable assessment of uncertainty. However, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data. Even if trained to be non-confident on OOD data, one can still adversarially manipulate OOD data so that the classifier again assigns high confidence to the manipulated samples. We show that two previously published defenses can be broken by better adapted attacks, highlighting the importance of robustness guarantees around OOD data. Since the existing method for this task is hard to train and significantly limits accuracy, we construct a classifier that can simultaneously achieve provably adversarially robust OOD detection and high clean accuracy. Moreover, by slightly modifying the classifier's architecture our method provably avoids the asymptotic overconfidence problem of standard neural networks. We provide code for all our experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/TXLC4IM6/Meinke et al. - Provably Robust Detection of Out-of-distribution Data (almost) for free.pdf}
}

@article{michielliStainNormalizationDigital2022,
  title = {Stain Normalization in Digital Pathology: {{Clinical}} Multi-Center Evaluation of Image Quality},
  shorttitle = {Stain Normalization in Digital Pathology},
  author = {Michielli, Nicola and Caputo, Alessandro and Scotto, Manuela and Mogetta, Alessandro and Pennisi, Orazio Antonino Maria and Molinari, Filippo and Balmativola, Davide and Bosco, Martino and Gambella, Alessandro and Metovic, Jasna and Tota, Daniele and Carpenito, Laura and Gasparri, Paolo and Salvi, Massimo},
  year = {2022},
  month = jan,
  journal = {Journal of Pathology Informatics},
  volume = {13},
  pages = {100145},
  issn = {2153-3539},
  doi = {10.1016/j.jpi.2022.100145},
  urldate = {2022-12-01},
  abstract = {In digital pathology, the final appearance of digitized images is affected by several factors, resulting in stain color and intensity variation. Stain normalization is an innovative solution to overcome stain variability. However, the validation of color normalization tools has been assessed only from a quantitative perspective, through the computation of similarity metrics between the original and normalized images. To the best of our knowledge, no works investigate the impact of normalization on the pathologist's evaluation. The objective of this paper is to propose a multi-tissue (i.e., breast, colon, liver, lung, and prostate) and multi-center qualitative analysis of a stain normalization tool with the involvement of pathologists with different years of experience. Two qualitative studies were carried out for this purpose: (i) a first study focused on the analysis of the perceived image quality and absence of significant image artifacts after the normalization process; (ii) a second study focused on the clinical score of the normalized image with respect to the original one. The results of the first study prove the high quality of the normalized image with a low impact artifact generation, while the second study demonstrates the superiority of the normalized image with respect to the original one in clinical practice. The normalization process can help both to reduce variability due to tissue staining procedures and facilitate the pathologist in the histological examination. The experimental results obtained in this work are encouraging and can justify the use of a stain normalization tool in clinical routine.},
  langid = {english},
  keywords = {Digital pathology,H\&E staining,Image quality,Qualitative score,Stain normalization},
  file = {/Users/Hans/Zotero/storage/G5PTD9ZY/Michielli et al. - 2022 - Stain normalization in digital pathology Clinical.pdf}
}

@article{nitzWestGermanStudy2019,
  title = {West {{German Study PlanB Trial}}: {{Adjuvant Four Cycles}} of {{Epirubicin}} and {{Cyclophosphamide Plus Docetaxel Versus Six Cycles}} of {{Docetaxel}} and {{Cyclophosphamide}} in {{HER2-Negative Early Breast Cancer}}},
  shorttitle = {West {{German Study PlanB Trial}}},
  author = {Nitz, Ulrike and Gluz, Oleg and Clemens, Michael and Malter, Wolfram and Reimer, Toralf and Nuding, Benno and Aktas, Bahriye and Stefek, Andrea and Pollmanns, Anke and {Lorenz-Salehi}, Fatemeh and Uleer, Christoph and Krabisch, Petra and Kuemmel, Sherko and Liedtke, Cornelia and Shak, Steven and Wuerstlein, Rachel and Christgen, Matthias and Kates, Ronald E. and Kreipe, Hans H. and Harbeck, Nadia and {on behalf of the West German Study Group PlanB Investigators}},
  year = {2019},
  month = apr,
  journal = {Journal of Clinical Oncology},
  volume = {37},
  number = {10},
  pages = {799--808},
  issn = {0732-183X, 1527-7755},
  doi = {10.1200/JCO.18.00028},
  urldate = {2022-08-18},
  abstract = {PURPOSE The West German Study Group PlanB trial evaluated an anthracycline-free chemotherapy standard (six cycles of docetaxel and cyclophosphamide [TC]) in the routine treatment of human epidermal growth factor receptor 2\textendash negative early breast cancer (EBC). PATIENTS AND METHODS Patients with pT1 to pT4c, all pN+, and pN0/high-risk EBC were eligible. High-risk pN0 was defined by one or more of the following: pT greater than 2, grade 2 to 3, high urokinase-type plasminogen activator/plasminogen activator inhibitor-1, hormone receptor (HR) negativity, and less than 35 years of age. After an early amendment, all HR-positive tumors underwent recurrence score (RS) testing, with chemotherapy omission recommended in RS less than or equal to 11 pN0 to pN1 disease. Patients were randomly assigned to four cycles of epirubicin (E)90/cyclophoshamide (C)600 followed by four cycles of docetaxel (T)100 or six cycles of T75C600 (administered once every 3 weeks). The primary end point was disease-free survival (DFS); secondary end points were overall survival (OS) and safety. The protocol specified P = .05 for a noninferiority margin of 4.4\% for all patients combined. ASSOCIATED CONTENT Appendix Data Supplements Author affiliations and support information (if applicable) appear at the end of this article. Accepted on January 2, 2019 and published at jco.org on February 20, 2019: DOI https://doi.org/10. 1200/JCO.18.00028 Clinical trials information: NCT01049425. RESULTS Of the 3,198 registered patients, 348 (RS \# 11) omitted chemotherapy, and 401 were not randomly assigned. The intention-to-treat population included 2,449 patients (1,227 EC-T v 1,222 TC: postmenopausal, 62.2\% v 60.8\%; pN0, 58.2\% v 59.5\%; pT1, 57.6\% v 52.3\%; HR positive, 81.4\% v 82.2\%; RS greater than 25 [in HR-positive patients], 26.2\% v 27.5\%). Within the safety population (1,167 v 1,178 patients), 87.5\% v 93.0\% completed therapy. After a 60-month median follow-up, 5-year outcomes were similar in the EC-T and TC arms (DFS, 89.6\% [95\% CI, 87.9\% to 91.5\%] v 89.9\% [95\% CI, 88.1\% to 91.8\%]; OS, 94.5\% [95\% CI, 93.1\% to 95.9\%] v 94.7\% [95\% CI, 93.3\% to 96.1\%]). The DFS difference was within the noninferiority margin of the original trial design. Five treatment-related deaths were reported for TC (one for EC-T), despite a trend toward more-severe adverse events in the latter. Interaction analysis revealed no predictive trends with respect to key factors, including triple-negative, luminal A/B-like, pN, age, and RS status. CONCLUSION In the West German Study Group PlanB trial, 5-year outcomes for TC and EC-T were equally excellent. Six cycles of TC is an effective/safe option in human epidermal growth factor receptor 2\textendash negative EBC with pN0 high genomic risk or pN1 EBC with genomically intermediate- to high-risk disease.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/3VSM373H/Nitz et al. - 2019 - West German Study PlanB Trial Adjuvant Four Cycle.pdf}
}

@article{olssonEstimatingDiagnosticUncertainty2022,
  title = {Estimating Diagnostic Uncertainty in Artificial Intelligence Assisted Pathology Using Conformal Prediction},
  author = {Olsson, Henrik and Kartasalo, Kimmo and Mulliqi, Nita and Capuccini, Marco and Ruusuvuori, Pekka and Samaratunga, Hemamali and Delahunt, Brett and Lindskog, Cecilia and Janssen, Emiel A. M. and Blilie, Anders and Egevad, Lars and Spjuth, Ola and Eklund, Martin},
  year = {2022},
  month = dec,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {7761},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-34945-8},
  urldate = {2022-12-17},
  abstract = {Unreliable predictions can occur when an artificial intelligence (AI) system is presented with data it has not been exposed to during training. We demonstrate the use of conformal prediction to detect unreliable predictions, using histopathological diagnosis and grading of prostate biopsies as example. We digitized 7788 prostate biopsies from 1192 men in the STHLM3 diagnostic study, used for training, and 3059 biopsies from 676 men used for testing. With conformal prediction, 1 in 794 (0.1\%) predictions is incorrect for cancer diagnosis (compared to 14 errors [2\%] without conformal prediction) while 175 (22\%) of the predictions are flagged as unreliable when the AI-system is presented with new data from the same lab and scanner that it was trained on. Conformal prediction could with small samples (N\,=\,49 for external scanner, N\,=\,10 for external lab and scanner, and N\,=\,12 for external lab, scanner and pathology assessment) detect systematic differences in external data leading to worse predictive performance. The AI-system with conformal prediction commits 3 (2\%) errors for cancer detection in cases of atypical prostate tissue compared to 44 (25\%) without conformal prediction, while the system flags 143 (80\%) unreliable predictions. We conclude that conformal prediction can increase patient safety of AI-systems.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Cancer screening,Machine learning,Prostate cancer,Statistical methods},
  file = {/Users/Hans/Zotero/storage/PY4L9WZQ/Olsson et al. - 2022 - Estimating diagnostic uncertainty in artificial in.pdf}
}

@misc{ozbeyUnsupervisedMedicalImage2022,
  title = {Unsupervised {{Medical Image Translation}} with {{Adversarial Diffusion Models}}},
  author = {{\"O}zbey, Muzaffer and Dalmaz, Onat and Dar, Salman UH and Bedel, Hasan A. and {\"O}zturk, {\c S}aban and G{\"u}ng{\"o}r, Alper and {\c C}ukur, Tolga},
  year = {2022},
  month = oct,
  number = {arXiv:2207.08208},
  eprint = {2207.08208},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2022-12-06},
  abstract = {Imputation of missing images via source-totarget modality translation can improve diversity in medical imaging protocols. A pervasive approach for synthesizing target images involves one-shot mapping through generative adversarial networks (GAN). Yet, GAN models that implicitly characterize the image distribution can suffer from limited sample fidelity. Here, we propose a novel method based on adversarial diffusion modeling, SynDiff, for improved performance in medical image translation. To capture a direct correlate of the image distribution, SynDiff leverages a conditional diffusion process that progressively maps noise and source images onto the target image. For fast and accurate image sampling during inference, large diffusion steps are taken with adversarial projections in the reverse diffusion direction. To enable training on unpaired datasets, a cycle-consistent architecture is devised with coupled diffusive and non-diffusive modules that bilaterally translate between two modalities. Extensive assessments are reported on the utility of SynDiff against competing GAN and diffusion models in multicontrast MRI and MRI-CT translation. Our demonstrations indicate that SynDiff offers quantitatively and qualitatively superior performance against competing baselines.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/VLAQAET2/zbey et al. - 2022 - Unsupervised Medical Image Translation with Advers.pdf}
}

@misc{PapersCodeProMix,
  title = {Papers with {{Code}} - {{ProMix}}: {{Combating Label Noise}} via {{Maximizing Clean Sample Utility}}},
  shorttitle = {Papers with {{Code}} - {{ProMix}}},
  urldate = {2023-02-20},
  abstract = { SOTA for Learning with noisy labels on CIFAR-10N-Worst (Accuracy (mean) metric)},
  howpublished = {https://paperswithcode.com/paper/promix-combating-label-noise-via-maximizing},
  langid = {english}
}

@article{parkHowVisionTransformers2022,
  title = {How {{Do Vision Transformers Work}}?},
  author = {Park, Namuk and Kim, Songkuk},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.06709 [cs]},
  eprint = {2202.06709},
  primaryclass = {cs},
  urldate = {2022-02-22},
  abstract = {The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at https://github.com/xxxnell/how-do-vits-work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/6Z9TPV3J/How Do Vision Transformers Work - 2022 - Park_Kim.pdf}
}

@article{petersenLearningAlgorithmicSupervision2021,
  title = {Learning with {{Algorithmic Supervision}} via {{Continuous Relaxations}}},
  author = {Petersen, Felix and Borgelt, Christian and Kuehne, Hilde and Deussen, Oliver},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.05651 [cs, stat]},
  eprint = {2110.05651},
  primaryclass = {cs, stat},
  urldate = {2021-12-14},
  abstract = {The integration of algorithmic components into neural architectures has gained increased attention recently, as it allows training neural networks with new forms of supervision such as ordering constraints or silhouettes instead of using ground truth labels. Many approaches in the field focus on the continuous relaxation of a specific task and show promising results in this context. But the focus on single tasks also limits the applicability of the proposed concepts to a narrow range of applications. In this work, we build on those ideas to propose an approach that allows to integrate algorithms into end-to-end trainable neural network architectures based on a general approximation of discrete conditions. To this end, we relax these conditions in control structures such as conditional statements, loops, and indexing, so that resulting algorithms are smoothly differentiable. To obtain meaningful gradients, each relevant variable is perturbed via logistic distributions and the expectation value under this perturbation is approximated. We evaluate the proposed continuous relaxation model on four challenging tasks and show that it can keep up with relaxations specifically designed for each individual task.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/Hans/Zotero/storage/GFDF9JVF/Learning with Algorithmic Supervision via Continuous Relaxations - 2021 - Petersenet al..pdf}
}

@article{pohjonenSpectralDecouplingAllows2022,
  title = {Spectral Decoupling Allows Training Transferable Neural Networks in Medical Imaging},
  author = {Pohjonen, Joona and St{\"u}renberg, Carolin and Rannikko, Antti and Mirtti, Tuomas and Pitk{\"a}nen, Esa},
  year = {2022},
  month = feb,
  journal = {iScience},
  volume = {25},
  number = {2},
  eprint = {2103.17171},
  primaryclass = {cs, eess, stat},
  pages = {103767},
  issn = {25890042},
  doi = {10.1016/j.isci.2022.103767},
  urldate = {2022-12-19},
  abstract = {Many current neural networks for medical imaging generalise poorly to data unseen during training. Such behaviour can be caused by networks overfitting easy-to-learn, or statistically dominant, features while disregarding other potentially informative features. For example, indistinguishable differences in the sharpness of the images from two different scanners can degrade the performance of the network significantly. All neural networks intended for clinical practice need to be robust to variation in data caused by differences in imaging equipment, sample preparation and patient populations. To address these challenges, we evaluate the utility of spectral decoupling as an implicit bias mitigation method. Spectral decoupling encourages the neural network to learn more features by simply regularising the networks' unnormalised prediction scores with an L2 penalty, thus having no added computational costs. We show that spectral decoupling allows training neural networks on datasets with strong spurious correlations and increases networks' robustness for data distribution shifts. To validate our findings, we train networks with and without spectral decoupling to detect prostate cancer tissue slides and COVID-19 in chest radiographs. Networks trained with spectral decoupling achieve up to 9.5 percent point higher performance on external datasets. Our results show that spectral decoupling helps with generalisation issues associated with neural networks, and can be used to complement or replace computationally expensive explicit bias mitigation methods, such as stain normalization in histological images. We recommend using spectral decoupling as an implicit bias mitigation method in any neural network intended for clinical use.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {/Users/Hans/Zotero/storage/AP9KGZRC/Pohjonen et al. - Spectral decoupling allows training transferable neural networks in medical imaging.pdf}
}

@article{renSimpleFixMahalanobis2021,
  title = {A {{Simple Fix}} to {{Mahalanobis Distance}} for {{Improving Near-OOD Detection}}},
  author = {Ren, Jie and Fort, Stanislav and Liu, Jeremiah and Roy, Abhijit Guha and Padhy, Shreyas and Lakshminarayanan, Balaji},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.09022 [cs]},
  eprint = {2106.09022},
  primaryclass = {cs},
  urldate = {2021-12-14},
  abstract = {Mahalanobis distance (MD) is a simple and popular post-processing method for detecting outof-distribution (OOD) inputs in neural networks. We analyze its failure modes for near-OOD detection and propose a simple fix called relative Mahalanobis distance (RMD) which improves performance and is more robust to hyperparameter choice. On a wide selection of challenging vision, language, and biology OOD benchmarks (CIFAR-100 vs CIFAR-10, CLINC OOD intent detection, Genomics OOD), we show that RMD meaningfully improves upon MD performance (by up to 15\% AUROC on genomics OOD).},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/YY5D65U2/A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection - 2021 - Renet al..pdf}
}

@article{rusakAdaptingImageNetscaleModels2021,
  title = {Adapting {{ImageNet-scale}} Models to Complex Distribution Shifts with Self-Learning},
  author = {Rusak, Evgenia and Schneider, Steffen and Gehler, Peter and Bringmann, Oliver and Brendel, Wieland and Bethge, Matthias},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.12928 [cs]},
  eprint = {2104.12928},
  primaryclass = {cs},
  urldate = {2022-03-03},
  abstract = {While self-learning methods are an important component in many recent domain adaptation techniques, they are not yet comprehensively evaluated on ImageNet-scale datasets common in robustness research. In extensive experiments on ResNet and EfficientNet models, we find that three components are crucial for increasing performance with self-learning: (i) using short update times between the teacher and the student network, (ii) fine-tuning only few affine parameters distributed across the network, and (iii) leveraging methods from robust classification to counteract the effect of label noise. We use these insights to obtain drastically improved state-of-the-art results on ImageNet-C (22.0\% mCE), ImageNet-R (17.4\% error) and ImageNet-A (14.8\% error). Our techniques yield further improvements in combination with previously proposed robustification methods. Self-learning is able to reduce the top-1 error to a point where no substantial further progress can be expected. We therefore re-purpose the dataset from the Visual Domain Adaptation Challenge 2019 and use a subset of it as a new robustness benchmark (ImageNet-D) which proves to be a more challenging dataset for all current state-of-the-art models (58.2\% error) to guide future research efforts at the intersection of robustness and domain adaptation on ImageNet scale.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/5EBG9J4V/Rusak et al. - 2021 - Adapting ImageNet-scale models to complex distribu.pdf}
}

@misc{sahariaPaletteImagetoImageDiffusion2022,
  title = {Palette: {{Image-to-Image Diffusion Models}}},
  shorttitle = {Palette},
  author = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris A. and Ho, Jonathan and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = may,
  number = {arXiv:2111.05826},
  eprint = {2111.05826},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-06},
  abstract = {This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/78J74ZVN/Saharia et al. - 2022 - Palette Image-to-Image Diffusion Models.pdf}
}

@misc{sasakiUNITDDPMUNpairedImage2021,
  title = {{{UNIT-DDPM}}: {{UNpaired Image Translation}} with {{Denoising Diffusion Probabilistic Models}}},
  shorttitle = {{{UNIT-DDPM}}},
  author = {Sasaki, Hiroshi and Willcocks, Chris G. and Breckon, Toby P.},
  year = {2021},
  month = apr,
  number = {arXiv:2104.05358},
  eprint = {2104.05358},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.05358},
  urldate = {2022-11-30},
  abstract = {We propose a novel unpaired image-to-image translation method that uses denoising diffusion probabilistic models without requiring adversarial training. Our method, UNpaired Image Translation with Denoising Diffusion Probabilistic Models (UNIT-DDPM), trains a generative model to infer the joint distribution of images over both domains as a Markov chain by minimising a denoising score matching objective conditioned on the other domain. In particular, we update both domain translation models simultaneously, and we generate target domain images by a denoising Markov Chain Monte Carlo approach that is conditioned on the input source domain images, based on Langevin dynamics. Our approach provides stable model training for image-to-image translation and generates high-quality image outputs. This enables state-of-the-art Fr\textbackslash 'echet Inception Distance (FID) performance on several public datasets, including both colour and multispectral imagery, significantly outperforming the contemporary adversarial image-to-image translation methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/MKCSY29R/Sasaki et al. - 2021 - UNIT-DDPM UNpaired Image Translation with Denoisi.pdf}
}

@article{schomig-markiefkaQualityControlStress2021,
  title = {Quality Control Stress Test for Deep Learning-Based Diagnostic Model in Digital Pathology},
  author = {{Sch{\"o}mig-Markiefka}, Birgid and Pryalukhin, Alexey and Hulla, Wolfgang and Bychkov, Andrey and Fukuoka, Junya and Madabhushi, Anant and Achter, Viktor and Nieroda, Lech and B{\"u}ttner, Reinhard and Quaas, Alexander and Tolkach, Yuri},
  year = {2021},
  month = dec,
  journal = {Modern Pathology},
  volume = {34},
  number = {12},
  pages = {2098--2108},
  publisher = {{Nature Publishing Group}},
  issn = {1530-0285},
  doi = {10.1038/s41379-021-00859-x},
  urldate = {2022-01-25},
  abstract = {Digital pathology provides a possibility for computational analysis of histological slides and automatization of routine pathological tasks. Histological slides are very heterogeneous concerning staining, sections' thickness, and artifacts arising during tissue processing, cutting, staining, and digitization. In this study, we digitally reproduce major types of artifacts. Using six datasets from four different institutions digitized by different scanner systems, we systematically explore artifacts' influence on the accuracy of the pre-trained, validated, deep learning-based model for prostate cancer detection in histological slides. We provide evidence that any histological artifact dependent on severity can lead to a substantial loss in model performance. Strategies for the prevention of diagnostic model accuracy losses in the context of artifacts are warranted. Stress-testing of diagnostic models using synthetically generated artifacts might be an essential step during clinical validation of deep learning-based algorithms.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Pathology,Prostate cancer},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Pathology;Prostate cancer Subject\_term\_id: pathology;prostate-cancer},
  file = {/Users/Hans/Zotero/storage/5W2EK896/Schmig-Markiefka et al. - 2021 - Quality control stress test for deep learning-base.pdf}
}

@article{shaoTransMILTransformerBased2021,
  title = {{{TransMIL}}: {{Transformer}} Based {{Correlated Multiple Instance Learning}} for {{Whole Slide Image Classification}}},
  shorttitle = {{{TransMIL}}},
  author = {Shao, Zhuchen and Bian, Hao and Chen, Yang and Wang, Yifeng and Zhang, Jian and Ji, Xiangyang and Zhang, Yongbing},
  year = {2021},
  month = oct,
  journal = {arXiv:2106.00908 [cs]},
  eprint = {2106.00908},
  primaryclass = {cs},
  urldate = {2022-03-02},
  abstract = {Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, the current MIL methods are usually based on independent and identical distribution hypothesis, thus neglect the correlation among different instances. To address this problem, we proposed a new framework, called correlated MIL, and provided a proof for convergence. Based on this framework, we devised a Transformer based MIL (TransMIL), which explored both morphological and spatial information. The proposed TransMIL can effectively deal with unbalanced/balanced and binary/multiple classification with great visualization and interpretability. We conducted various experiments for three different computational pathology problems and achieved better performance and faster convergence compared with state-of-the-art methods. The test AUC for the binary tumor classification can be up to 93.09\% over CAMELYON16 dataset. And the AUC over the cancer subtypes classification can be up to 96.03\% and 98.82\% over TCGA-NSCLC dataset and TCGA-RCC dataset, respectively. Implementation is available at: https://github.com/szc19990412/TransMIL.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/CVWHSEZJ/Shao et al. - 2021 - TransMIL Transformer based Correlated Multiple In.pdf}
}

@article{shaoWeaklySupervisedDeep2021,
  title = {Weakly {{Supervised Deep Ordinal Cox Model}} for {{Survival Prediction From Whole-Slide Pathological Images}}},
  author = {Shao, Wei and Wang, Tongxin and Huang, Zhi and Han, Zhi and Zhang, Jie and Huang, Kun},
  year = {2021},
  month = dec,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {40},
  number = {12},
  pages = {3739--3747},
  issn = {1558-254X},
  doi = {10.1109/TMI.2021.3097319},
  abstract = {Whole-Slide Histopathology Image (WSI) is generally considered the gold standard for cancer diagnosis and prognosis. Given the large inter-operator variation among pathologists, there is an imperative need to develop machine learning models based on WSIs for consistently predicting patient prognosis. The existing WSI-based prediction methods do not utilize the ordinal ranking loss to train the prognosis model, and thus cannot model the strong ordinal information among different patients in an efficient way. Another challenge is that a WSI is of large size (e.g., 100,000-by-100,000 pixels) with heterogeneous patterns but often only annotated with a single WSI-level label, which further complicates the training process. To address these challenges, we consider the ordinal characteristic of the survival process by adding a ranking-based regularization term on the Cox model and propose a weakly supervised deep ordinal Cox model (BDOCOX) for survival prediction from WSIs. Here, we generate amounts of bags from WSIs, and each bag is comprised of the image patches representing the heterogeneous patterns of WSIs, which is assumed to match the WSI-level labels for training the proposed model. The effectiveness of the proposed method is well validated by theoretical analysis as well as the prognosis and patient stratification results on three cancer datasets from The Cancer Genome Atlas (TCGA).},
  keywords = {Analytical models,Cancer,Computational modeling,Hazards,Histopathological images,ordinal cox model,Predictive models,Prognostics and health management,survival analysis,Tumors,weakly supervised learning}
}

@article{smithGeneralCyclicalTraining2022,
  title = {General {{Cyclical Training}} of {{Neural Networks}}},
  author = {Smith, Leslie N.},
  year = {2022},
  month = feb,
  urldate = {2022-02-22},
  abstract = {This paper describes the principle of "General Cyclical Training" in machine learning, where training starts and ends with "easy training" and the "hard training" happens during the middle epochs. We propose several manifestations for training neural networks, including algorithmic examples (via hyper-parameters and loss functions), data-based examples, and model-based examples. Specifically, we introduce several novel techniques: cyclical weight decay, cyclical batch size, cyclical focal loss, cyclical softmax temperature, cyclical data augmentation, cyclical gradient clipping, and cyclical semi-supervised learning. In addition, we demonstrate that cyclical weight decay, cyclical softmax temperature, and cyclical gradient clipping (as three examples of this principle) are beneficial in the test accuracy performance of a trained model. Furthermore, we discuss model-based examples (such as pretraining and knowledge distillation) from the perspective of general cyclical training and recommend some changes to the typical training methodology. In summary, this paper defines the general cyclical training concept and discusses several specific ways in which this concept can be applied to training neural networks. In the spirit of reproducibility, the code used in our experiments is available at \textbackslash url\{https://github.com/lnsmith54/CFL\}.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/362LVCS3/General Cyclical Training of Neural Networks - 2022 - Smith.pdf}
}

@misc{taghanakiMaskTuneMitigatingSpurious2022,
  title = {{{MaskTune}}: {{Mitigating Spurious Correlations}} by {{Forcing}} to {{Explore}}},
  shorttitle = {{{MaskTune}}},
  author = {Taghanaki, Saeid Asgari and Khani, Aliasghar and Khani, Fereshte and Gholami, Ali and Tran, Linh and {Mahdavi-Amiri}, Ali and Hamarneh, Ghassan},
  year = {2022},
  month = oct,
  number = {arXiv:2210.00055},
  eprint = {2210.00055},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.00055},
  urldate = {2022-11-25},
  abstract = {A fundamental challenge of over-parameterized deep learning models is learning meaningful data representations that yield good performance on a downstream task without over-fitting spurious input features. This work proposes MaskTune, a masking strategy that prevents over-reliance on spurious (or a limited number of) features. MaskTune forces the trained model to explore new features during a single epoch finetuning by masking previously discovered features. MaskTune, unlike earlier approaches for mitigating shortcut learning, does not require any supervision, such as annotating spurious features or labels for subgroup samples in a dataset. Our empirical results on biased MNIST, CelebA, Waterbirds, and ImagenNet-9L datasets show that MaskTune is effective on tasks that often suffer from the existence of spurious correlations. Finally, we show that MaskTune outperforms or achieves similar performance to the competing methods when applied to the selective classification (classification with rejection option) task. Code for MaskTune is available at https://github.com/aliasgharkhani/Masktune.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/K75VTCEL/Taghanaki et al. - MaskTune.pdf}
}

@misc{tianDesigningBERTConvolutional2023,
  title = {Designing {{BERT}} for {{Convolutional Networks}}: {{Sparse}} and {{Hierarchical Masked Modeling}}},
  shorttitle = {Designing {{BERT}} for {{Convolutional Networks}}},
  author = {Tian, Keyu and Jiang, Yi and Diao, Qishuai and Lin, Chen and Wang, Liwei and Yuan, Zehuan},
  year = {2023},
  month = jan,
  number = {arXiv:2301.03580},
  eprint = {2301.03580},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.03580},
  urldate = {2023-01-23},
  abstract = {We identify and overcome two key obstacles in extending the success of BERT-style pre-training, or the masked image modeling, to convolutional networks (convnets): (i) convolution operation cannot handle irregular, random-masked input images; (ii) the single-scale nature of BERT pre-training is inconsistent with convnet's hierarchical structure. For (i), we treat unmasked pixels as sparse voxels of 3D point clouds and use sparse convolution to encode. This is the first use of sparse convolution for 2D masked modeling. For (ii), we develop a hierarchical decoder to reconstruct images from multi-scale encoded features. Our method called Sparse masKed modeling (SparK) is general: it can be used directly on any convolutional model without backbone modifications. We validate it on both classical (ResNet) and modern (ConvNeXt) models: on three downstream tasks, it surpasses both state-of-the-art contrastive learning and transformer-based masked modeling by similarly large margins (around +1.0\%). Improvements on object detection and instance segmentation are more substantial (up to +3.5\%), verifying the strong transferability of features learned. We also find its favorable scaling behavior by observing more gains on larger models. All this evidence reveals a promising future of generative pre-training on convnets. Codes and models are released at https://github.com/keyu-tian/SparK.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/KXCJTLEH/Tian et al. - Designing BERT for Convolutional Networks - Sparse and Hierarchical Masked Modeling.pdf}
}

@article{trahearnMulticlassStainSeparation2015,
  title = {Multi-Class Stain Separation Using Independent Component Analysis},
  author = {Trahearn, Nicholas and Snead, David and Cree, Ian and Rajpoot, Nasir},
  year = {2015},
  month = mar,
  journal = {Progress in Biomedical Optics and Imaging - Proceedings of SPIE},
  volume = {9420},
  doi = {10.1117/12.2081933},
  abstract = {Stain separation is the process whereby a full colour histology section image is transformed into a series of single channel images, each corresponding to a given stain's expression. Many algorithms in the field of digital pathology are concerned with the expression of a single stain, thus stain separation is a key preprocessing step in these situations. We present a new versatile method of stain separation. The method uses Independent Component Analysis (ICA) to determine a set of statistically independent vectors, corresponding to the individual stain expressions. In comparison to other popular approaches, such as PCA and NNMF, we found that ICA gives a superior projection of the data with respect to each stain. In addition, we introduce a correction step to improve the initial results provided by the ICA coefficients. Many existing approaches only consider separation of two stains, with primary emphasis on Haematoxylin and Eosin. We show that our method is capable of making a good separation when there are more than two stains present. We also demonstrate our method's ability to achieve good separation on a variety of different stain types.},
  file = {/Users/Hans/Zotero/storage/4IM8V5BJ/Trahearn et al. - Multi-class stain separation using independent component analysis.pdf}
}

@misc{tschannenImageandLanguageUnderstandingPixels2022,
  title = {Image-and-{{Language Understanding}} from {{Pixels Only}}},
  author = {Tschannen, Michael and Mustafa, Basil and Houlsby, Neil},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08045},
  eprint = {2212.08045},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.08045},
  urldate = {2022-12-17},
  abstract = {Multimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a pure pixel-based model to perform image, text, and multimodal tasks. Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP, with half the number of parameters and no text-specific tower or embedding. When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Finally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/I437ARLH/Tschannen et al. - Image-and-Language Understanding from Pixels Only.pdf}
}

@techreport{vale-silvaMultiSurvLongtermCancer2020,
  type = {Preprint},
  title = {{{MultiSurv}}: {{Long-term}} Cancer Survival Prediction Using Multimodal Deep Learning},
  shorttitle = {{{MultiSurv}}},
  author = {{Vale-Silva}, Lu{\'i}s A. and Rohr, Karl},
  year = {2020},
  month = aug,
  institution = {{Health Informatics}},
  doi = {10.1101/2020.08.06.20169698},
  urldate = {2023-05-16},
  abstract = {The age of precision medicine demands powerful computational techniques to handle high-dimensional patient data. We present MultiSurv, a multimodal deep learning method for long-term pan-cancer survival prediction. MultiSurv is composed of three main modules. A feature representation module includes a dedicated submodel for each input data modality. A data fusion layer aggregates the multimodal representations. Finally, a prediction submodel yields conditional survival probabilities for a predefined set of follow-up time intervals. We trained MultiSurv on clinical, imaging, and four different high-dimensional omics data modalities from patients diagnosed with one of 33 different cancer types. We evaluated unimodal input configurations against several previous methods and different multimodal data combinations. MultiSurv achieved the best results according to different time-dependent metrics and delivered highly accurate long-term patient survival curves. The best performance was obtained when combining clinical information with either gene expression or DNA methylation data, depending on the evaluation metric. Additionally, MultiSurv can handle missing data, including missing values and complete data modalitites. Interestingly, for unimodal data we found that simpler modeling approaches, including the classical Cox proportional hazards method, can achieve results rivaling those of more complex methods for certain data modalities. We also show how the learned feature representations of MultiSurv can be used to visualize relationships between cancer types and individual patients, after embedding into a low-dimensional space.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/X8HMHDZU/Vale-Silva and Rohr - 2020 - MultiSurv Long-term cancer survival prediction us.pdf}
}

@misc{vasiljevicHistopathologicalStainInvariance2020,
  title = {Towards {{Histopathological Stain Invariance}} by {{Unsupervised Domain Augmentation}} Using {{Generative Adversarial Networks}}},
  author = {Vasiljevi{\'c}, Jelica and Feuerhake, Friedrich and Wemmert, C{\'e}dric and Lampert, Thomas},
  year = {2020},
  month = dec,
  number = {arXiv:2012.12413},
  eprint = {2012.12413},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.12413},
  urldate = {2022-12-01},
  abstract = {The application of supervised deep learning methods in digital pathology is limited due to their sensitivity to domain shift. Digital Pathology is an area prone to high variability due to many sources, including the common practice of evaluating several consecutive tissue sections stained with different staining protocols. Obtaining labels for each stain is very expensive and time consuming as it requires a high level of domain knowledge. In this article, we propose an unsupervised augmentation approach based on adversarial image-to-image translation, which facilitates the training of stain invariant supervised convolutional neural networks. By training the network on one commonly used staining modality and applying it to images that include corresponding, but differently stained, tissue structures, the presented method demonstrates significant improvements over other approaches. These benefits are illustrated in the problem of glomeruli segmentation in seven different staining modalities (PAS, Jones H\&E, CD68, Sirius Red, CD34, H\&E and CD3) and analysis of the learned representations demonstrate their stain invariance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/RMDU42XU/Vasiljevi et al. - 2020 - Towards Histopathological Stain Invariance by Unsu.pdf}
}

@misc{vonkugelgenSelfSupervisedLearningData2022,
  title = {Self-{{Supervised Learning}} with {{Data Augmentations Provably Isolates Content}} from {{Style}}},
  author = {{von K{\"u}gelgen}, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Sch{\"o}lkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
  year = {2022},
  month = jan,
  number = {arXiv:2106.04619},
  eprint = {2106.04619},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.04619},
  urldate = {2022-11-29},
  abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/Hans/Zotero/storage/7LZGPS5G/von Kgelgen et al. - 2022 - Self-Supervised Learning with Data Augmentations P.pdf}
}

@misc{wagnerBuiltLastReproducibility2022,
  title = {Built to Last? {{Reproducibility}} and {{Reusability}} of {{Deep Learning Algorithms}} in {{Computational Pathology}}},
  shorttitle = {Built to Last?},
  author = {Wagner, Sophia J. and Matek, Christian and Boushehri, Sayedali Shetab and Boxberg, Melanie and Lamm, Lorenz and Sadafi, Ario and Waibel, Dominik J. E. and Marr, Carsten and Peng, Tingying},
  year = {2022},
  month = may,
  publisher = {{Pathology}},
  doi = {10.1101/2022.05.15.22275108},
  urldate = {2022-12-19},
  abstract = {Abstract           Recent progress in computational pathology has been driven by deep learning. While code and data availability are essential to reproduce findings from preceding publications, ensuring a deep learning model's reusability is more challenging. For that, the codebase should be well-documented and easy to integrate in existing workflows, and models should be robust towards noise and generalizable towards data from different sources. Strikingly, only a few computational pathology algorithms have been reused by other researchers so far, let alone employed in a clinical setting.           To assess the current state of reproducibility and reusability of computational pathology algorithms, we evaluated peer-reviewed articles available in Pubmed, published between January 2019 and March 2021, in five use cases: stain normalization, tissue type segmentation, evaluation of cell-level features, genetic alteration prediction, and direct extraction of grading, staging, and prognostic information. We compiled criteria for data and code availability, and for statistical result analysis and assessed them in 161 publications. We found that only one quarter (42 out of 161 publications) made code publicly available and thus fulfilled our minimum requirement for reproducibility and reusability. Among these 42 papers, three quarters (30 out of 42) analyzed their results statistically, less than half (20 out of 42) have released their trained model weights, and only about a third (16 out of 42) used an independent cohort for evaluation.           This review highlights candidates for reproducible and reusable algorithms in computational pathology. It is intended for both pathologists interested in deep learning, and researchers applying deep learning algorithms to computational pathology challenges. We provide a list of reusable data handling tools and a detailed overview of the publications together with our criteria for reproducibility and reusability.},
  archiveprefix = {Pathology},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/R3I6332B/Wagner et al. - Built to last - Reproducibility and Reusability of Deep Learning Algorithms in Computational Pathology.pdf}
}

@misc{wagnerStructurePreservingMultiDomainStain2021,
  title = {Structure-{{Preserving Multi-Domain Stain Color Augmentation}} Using {{Style-Transfer}} with {{Disentangled Representations}}},
  author = {Wagner, Sophia J. and Khalili, Nadieh and Sharma, Raghav and Boxberg, Melanie and Marr, Carsten and {de Back}, Walter and Peng, Tingying},
  year = {2021},
  month = jul,
  number = {arXiv:2107.12357},
  eprint = {2107.12357},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.12357},
  urldate = {2022-11-29},
  abstract = {In digital pathology, different staining procedures and scanners cause substantial color variations in whole-slide images (WSIs), especially across different laboratories. These color shifts result in a poor generalization of deep learning-based methods from the training domain to external pathology data. To increase test performance, stain normalization techniques are used to reduce the variance between training and test domain. Alternatively, color augmentation can be applied during training leading to a more robust model without the extra step of color normalization at test time. We propose a novel color augmentation technique, HistAuGAN, that can simulate a wide variety of realistic histology stain colors, thus making neural networks stain-invariant when applied during training. Based on a generative adversarial network (GAN) for image-to-image translation, our model disentangles the content of the image, i.e., the morphological tissue structure, from the stain color attributes. It can be trained on multiple domains and, therefore, learns to cover different stain colors as well as other domain-specific variations introduced in the slide preparation and imaging process. We demonstrate that HistAuGAN outperforms conventional color augmentation techniques on a classification task on the publicly available dataset Camelyon17 and show that it is able to mitigate present batch effects.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/DKADI9TC/Wagner et al. - 2021 - Structure-Preserving Multi-Domain Stain Color Augm.pdf}
}

@misc{wangProMixCombatingLabel2022,
  title = {{{ProMix}}: {{Combating Label Noise}} via {{Maximizing Clean Sample Utility}}},
  shorttitle = {{{ProMix}}},
  author = {Wang, Haobo and Xiao, Ruixuan and Dong, Yiwen and Feng, Lei and Zhao, Junbo},
  year = {2022},
  month = jul,
  number = {arXiv:2207.10276},
  eprint = {2207.10276},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-20},
  abstract = {The ability to train deep neural networks under label noise is appealing, as imperfectly annotated data are relatively cheaper to obtain. State-of-the-art approaches are based on semi-supervised learning(SSL), which selects small loss examples as clean and then applies SSL techniques for boosted performance. However, the selection step mostly provides a medium-sized and decent-enough clean subset, which overlooks a rich set of clean samples. In this work, we propose a novel noisy label learning framework ProMix that attempts to maximize the utility of clean samples for boosted performance. Key to our method, we propose a matched high-confidence selection technique that selects those examples having high confidence and matched prediction with its given labels. Combining with the small-loss selection, our method is able to achieve a precision of 99.27 and a recall of 98.22 in detecting clean samples on the CIFAR-10N dataset. Based on such a large set of clean data, ProMix improves the best baseline method by +2.67\% on CIFAR-10N and +1.61\% on CIFAR-100N datasets. The code and data are available at https://github.com/Justherozen/ProMix},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/BWY3MYW4/Wang et al. - ProMix - Combating Label Noise via Maximizing Clean Sample Utility.pdf}
}

@misc{wangRealESRGANTrainingRealWorld2021,
  title = {Real-{{ESRGAN}}: {{Training Real-World Blind Super-Resolution}} with {{Pure Synthetic Data}}},
  shorttitle = {Real-{{ESRGAN}}},
  author = {Wang, Xintao and Xie, Liangbin and {Chao Dong} and Shan, Ying},
  year = {2021},
  month = aug,
  number = {arXiv:2107.10833},
  eprint = {2107.10833},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.10833},
  urldate = {2022-12-06},
  abstract = {Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/64ZAZ7N2/Wang et al. - 2021 - Real-ESRGAN Training Real-World Blind Super-Resol.pdf}
}

@misc{wilmMindGapScannerinduced2022,
  title = {Mind the {{Gap}}: {{Scanner-induced}} Domain Shifts Pose Challenges for Representation Learning in Histopathology},
  shorttitle = {Mind the {{Gap}}},
  author = {Wilm, Frauke and Fragoso, Marco and Bertram, Christof A. and Stathonikos, Nikolas and {\"O}ttl, Mathias and Qiu, Jingna and Klopfleisch, Robert and Maier, Andreas and Aubreville, Marc and Breininger, Katharina},
  year = {2022},
  month = nov,
  number = {arXiv:2211.16141},
  eprint = {2211.16141},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2022-12-06},
  abstract = {Computer-aided systems in histopathology are often challenged by various sources of domain shift that impact the performance of these algorithms considerably. We investigated the potential of using self-supervised pre-training to overcome scanner-induced domain shifts for the downstream task of tumor segmentation. For this, we present the Barlow Triplets to learn scanner-invariant representations from a multi-scanner dataset with local image correspondences. We show that self-supervised pre-training successfully aligned different scanner representations, which, interestingly only results in a limited benefit for our downstream task. We thereby provide insights into the influence of scanner characteristics for downstream applications and contribute to a better understanding of why established self-supervised methods have not yet shown the same success on histopathology data as they have for natural images.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/XMTG724S/Wilm et al. - 2022 - Mind the Gap Scanner-induced domain shifts pose c.pdf}
}

@misc{windingConnectomeInsectBrain2022,
  title = {The Connectome of an Insect Brain},
  author = {Winding, Michael and Pedigo, Benjamin D. and Barnes, Christopher L. and Patsolic, Heather G. and Park, Youngser and Kazimiers, Tom and Fushiki, Akira and Andrade, Ingrid V. and Li, Feng and {Valdes-Aleman}, Javier and Khandelwal, Avinash and Randel, Nadine and Barsotti, Elizabeth and Correia, Ana and Fetter, Richard D. and Hartenstein, Volker and Priebe, Carey E. and Vogelstein, Joshua T. and Cardona, Albert and Zlatic, Marta},
  year = {2022},
  month = nov,
  primaryclass = {New Results},
  pages = {2022.11.28.516756},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.11.28.516756},
  urldate = {2022-11-29},
  abstract = {Brains contain networks of interconnected neurons, so knowing the network architecture is essential for understanding brain function. We therefore mapped the synaptic-resolution connectome of an insect brain (Drosophila larva) with rich behavior, including learning, value-computation, and action-selection, comprising 3,013 neurons and 544,000 synapses. We characterized neuron-types, hubs, feedforward and feedback pathways, and cross-hemisphere and brain-nerve cord interactions. We found pervasive multisensory and interhemispheric integration, highly recurrent architecture, abundant feedback from descending neurons, and multiple novel circuit motifs. The brain's most recurrent circuits comprised the input and output neurons of the learning center. Some structural features, including multilayer shortcuts and nested recurrent loops, resembled powerful machine learning architectures. The identified brain architecture provides a basis for future experimental and theoretical studies of neural circuits.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/AC5MEURN/Winding et al. - 2022 - The connectome of an insect brain.pdf}
}

@techreport{wisselSurvBoardStandardisedBenchmarking2022,
  type = {Preprint},
  title = {{{SurvBoard}}: {{Standardised Benchmarking}} for {{Multi-omics Cancer Survival Models}}},
  shorttitle = {{{SurvBoard}}},
  author = {Wissel, David and Janakarajan, Nikita and Grover, Aayush and Toniato, Enrico and Mart{\'i}nez, Mar{\'i}a Rodr{\'i}guez and Boeva, Valentina},
  year = {2022},
  month = nov,
  institution = {{Bioinformatics}},
  doi = {10.1101/2022.11.18.517043},
  urldate = {2023-05-16},
  abstract = {High-throughput "omics" data, including genomic, transcriptomic, and epigenetic data, have become increasingly produced and have contributed in recent years to the advances in cancer research. In particular, multi-modal omics data get now employed in addition to clinical data to stratify patients according to their clinical outcomes. Despite some recent work on benchmarking multi-modal integration strategies for cancer survival prediction, there is still a need for the standardization of the results of model performances and for the consecutive exploration of the relative performance of statistical and deep learning models. Here, we propose a unique benchmark, SurvBoard, which standardizes several important experimental design choices to enable comparability between cancer survival models that incorporate multi-omics data. By designing several benchmarking scenarios, SurvBoard allows for the comparison of single-cancer models and models trained on pan-cancer data; SurvBoard also makes it possible to investigate the added value of using patient data with missing modalities. Additionally, in this work, we point out several potential pitfalls that might arise during the preprocessing and validation of multi-omics cancer survival models and address them in our benchmark. We compare statistical and deep learning models revealing that statistical models often outperform deep learning models, particularly in terms of model calibration. Finally, we offer a web service that enables quick model evaluation against our benchmark (https://www.survboard.science/). All code and other resources are available on GitHub: https://github.com/BoevaLab/survboard/.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/A7QP8AKM/Wissel et al. - 2022 - SurvBoard Standardised Benchmarking for Multi-omi.pdf}
}

@misc{wollebSwissArmyKnife2022,
  title = {The {{Swiss Army Knife}} for {{Image-to-Image Translation}}: {{Multi-Task Diffusion Models}}},
  shorttitle = {The {{Swiss Army Knife}} for {{Image-to-Image Translation}}},
  author = {Wolleb, Julia and Sandk{\"u}hler, Robin and Bieder, Florentin and Cattin, Philippe C.},
  year = {2022},
  month = apr,
  number = {arXiv:2204.02641},
  eprint = {2204.02641},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-06},
  abstract = {Recently, diffusion models were applied to a wide range of image analysis tasks. We build on a method for image-to-image translation using denoising diffusion implicit models and include a regression problem and a segmentation problem for guiding the image generation to the desired output. The main advantage of our approach is that the guidance during the denoising process is done by an external gradient. Consequently, the diffusion model does not need to be retrained for the different tasks on the same dataset. We apply our method to simulate the aging process on facial photos using a regression task, as well as on a brain magnetic resonance (MR) imaging dataset for the simulation of brain tumor growth. Furthermore, we use a segmentation model to inpaint tumors at the desired location in healthy slices of brain MR images. We achieve convincing results for all problems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/IK2DTUJ7/Wolleb et al. - 2022 - The Swiss Army Knife for Image-to-Image Translatio.pdf}
}

@misc{wooConvNeXtV2Codesigning2023,
  title = {{{ConvNeXt V2}}: {{Co-designing}} and {{Scaling ConvNets}} with {{Masked Autoencoders}}},
  shorttitle = {{{ConvNeXt V2}}},
  author = {Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
  year = {2023},
  month = jan,
  number = {arXiv:2301.00808},
  eprint = {2301.00808},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.00808},
  urldate = {2023-01-23},
  abstract = {Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7\% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9\% accuracy using only public training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/VLKALSBT/Woo et al. - ConvNeXt V2 - Co-designing and Scaling ConvNets with Masked Autoencoders.pdf}
}

@misc{wuUnifyingDiffusionModels2022,
  title = {Unifying {{Diffusion Models}}' {{Latent Space}}, with {{Applications}} to {{CycleDiffusion}} and {{Guidance}}},
  author = {Wu, Chen Henry and {De la Torre}, Fernando},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05559},
  eprint = {2210.05559},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.05559},
  urldate = {2022-12-01},
  abstract = {Diffusion models have achieved unprecedented performance in generative modeling. The commonly-adopted formulation of the latent code of diffusion models is a sequence of gradually denoised samples, as opposed to the simpler (e.g., Gaussian) latent space of GANs, VAEs, and normalizing flows. This paper provides an alternative, Gaussian formulation of the latent space of various diffusion models, as well as an invertible DPM-Encoder that maps images into the latent space. While our formulation is purely based on the definition of diffusion models, we demonstrate several intriguing consequences. (1) Empirically, we observe that a common latent space emerges from two diffusion models trained independently on related domains. In light of this finding, we propose CycleDiffusion, which uses DPM-Encoder for unpaired image-to-image translation. Furthermore, applying CycleDiffusion to text-to-image diffusion models, we show that large-scale text-to-image diffusion models can be used as zero-shot image-to-image editors. (2) One can guide pre-trained diffusion models and GANs by controlling the latent codes in a unified, plug-and-play formulation based on energy-based models. Using the CLIP model and a face recognition model as guidance, we demonstrate that diffusion models have better coverage of low-density sub-populations and individuals than GANs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/BJM5GUUE/Wu and De la Torre - 2022 - Unifying Diffusion Models' Latent Space, with Appl.pdf}
}

@misc{xiaoUnifiedPerceptualParsing2018,
  title = {Unified {{Perceptual Parsing}} for {{Scene Understanding}}},
  author = {Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian},
  year = {2018},
  month = jul,
  number = {arXiv:1807.10221},
  eprint = {1807.10221},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-11-04},
  abstract = {Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/TWKLC8J7/Xiao et al. - 2018 - Unified Perceptual Parsing for Scene Understanding.pdf}
}

@misc{yangDiffusionModelsComprehensive2022,
  title = {Diffusion {{Models}}: {{A Comprehensive Survey}} of {{Methods}} and {{Applications}}},
  shorttitle = {Diffusion {{Models}}},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  year = {2022},
  month = oct,
  number = {arXiv:2209.00796},
  eprint = {2209.00796},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.00796},
  urldate = {2022-12-01},
  abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/XZZGNUS6/Yang et al. - 2022 - Diffusion Models A Comprehensive Survey of Method.pdf}
}

@misc{yangFocalModulationNetworks2022,
  title = {Focal {{Modulation Networks}}},
  author = {Yang, Jianwei and Li, Chunyuan and Dai, Xiyang and Yuan, Lu and Gao, Jianfeng},
  year = {2022},
  month = nov,
  number = {arXiv:2203.11926},
  eprint = {2203.11926},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-11-04},
  abstract = {We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-theart SA counterparts (e.g., Swin and Focal Transformers) with similar computational cost on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3\% and 83.9\% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 2242 resolution, it attains 86.5\% and 87.3\% top-1 accuracy when finetuned with resolution 2242 and 3842, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN [29], FocalNet base trained with 1\texttimes{} outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\texttimes{} schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet [90], FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multiscale (50.5 v.s. 49.7). Using large FocalNet and Mask2former [13], we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO [106], we achieved 64.2 and 64.3 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G [53] and BEIT-3 [84]. These encouraging results render focal modulation is probably what we need for vision1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/QTGLLCG4/Yang et al. - 2022 - Focal Modulation Networks.pdf}
}

@article{yuanLargescaleRobustDeep2021,
  title = {Large-Scale {{Robust Deep AUC Maximization}}: {{A New Surrogate Loss}} and {{Empirical Studies}} on {{Medical Image Classification}}},
  shorttitle = {Large-Scale {{Robust Deep AUC Maximization}}},
  author = {Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao},
  year = {2021},
  month = sep,
  journal = {arXiv:2012.03173 [cs, math, stat]},
  eprint = {2012.03173},
  primaryclass = {cs, math, stat},
  urldate = {2022-02-23},
  abstract = {Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural network by maximizing the AUC score of the model on a dataset. Most previous works of AUC maximization focus on the perspective of optimization by designing efficient stochastic algorithms, and studies on generalization performance of large-scale DAM on difficult tasks are missing. In this work, we aim to make DAM more practical for interesting real-world applications (e.g., medical image classification). First, we propose a new margin-based min-max surrogate loss function for the AUC score (named as AUC min-max-margin loss or simply AUC margin loss for short). It is more robust than the commonly used AUC square loss, while enjoying the same advantage in terms of large-scale stochastic optimization. Second, we conduct extensive empirical studies of our DAM method on four difficult medical image classification tasks, namely (i) classification of chest x-ray images for identifying many threatening diseases, (ii) classification of images of skin lesions for identifying melanoma, (iii) classification of mammogram for breast cancer screening, and (iv) classification of microscopic images for identifying tumor tissue. Our studies demonstrate that the proposed DAM method improves the performance of optimizing cross-entropy loss by a large margin, and also achieves better performance than optimizing the existing AUC square loss on these medical image classification tasks. Specifically, our DAM method has achieved the 1st place on Stanford CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is the first work that makes DAM succeed on large-scale medical image datasets. We also conduct extensive ablation studies to demonstrate the advantages of the new AUC margin loss over the AUC square loss on benchmark datasets. The proposed method is implemented in our open-sourced library LibAUC (www.libauc.org).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/Hans/Zotero/storage/TRDVPBCH/Large-scale Robust Deep AUC Maximization - 2021 - Yuan et al..pdf}
}

@misc{yuMEGABYTEPredictingMillionbyte2023,
  title = {{{MEGABYTE}}: {{Predicting Million-byte Sequences}} with {{Multiscale Transformers}}},
  shorttitle = {{{MEGABYTE}}},
  author = {Yu, Lili and Simig, D{\'a}niel and Flaherty, Colin and Aghajanyan, Armen and Zettlemoyer, Luke and Lewis, Mike},
  year = {2023},
  month = may,
  number = {arXiv:2305.07185},
  eprint = {2305.07185},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.07185},
  urldate = {2023-05-26},
  abstract = {Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/Hans/Zotero/storage/B4XLKAKT/Yu et al. - MEGABYTE - Predicting Million-byte Sequences with Multiscale Transformers.pdf}
}

@misc{zhangAdamCanConverge2022,
  title = {Adam {{Can Converge Without Any Modification On Update Rules}}},
  author = {Zhang, Yushun and Chen, Congliang and Shi, Naichen and Sun, Ruoyu and Luo, Zhi-Quan},
  year = {2022},
  month = oct,
  number = {arXiv:2208.09632},
  eprint = {2208.09632},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  urldate = {2022-12-05},
  abstract = {Ever since Reddi et al. (2018) pointed out the divergence issue of Adam, many new variants have been designed to obtain convergence. However, vanilla Adam remains exceptionally popular and it works well in practice. Why is there a gap between theory and practice? We point out there is a mismatch between the settings of theory and practice: Reddi et al. (2018) pick the problem after picking the hyperparameters of Adam, i.e., ({$\beta$}1, {$\beta$}2); while practical applications often fix the problem first and then tune ({$\beta$}1, {$\beta$}2). Due to this observation, we conjecture that the empirical convergence can be theoretically justified, only if we change the order of picking the problem and hyperparameter. In this work, we confirm this conjecture. We prove that, wh{$\surd$}en the 2nd-order momentum parameter {$\beta$}2 is large and 1st-order momentum parameter {$\beta$}1 {$<$} {$\beta$}2 {$<$} 1, Adam converges to the neighborhood of critical points. The size of the neighborhood is propositional to the variance of stochastic gradients. Under an extra condition (strong growth condition), Adam converges to critical points. It is worth mentioning that our results cover a wide range of hyperparameters: as {$\beta$}2 increases, our convergence result can cover any {$\beta$}1 {$\in$} [0, 1) including {$\beta$}1 = 0.9, which is the default setting in deep learning libraries. To our knowledge, this is the first result showing that Adam can converge without any modification on its update rules. Further, our analysis does not require assumptions of bounded gradients or bounded 2nd-order momentum. When {$\beta$}2 is small, we further point out a large region of ({$\beta$}1, {$\beta$}2) combinations where Adam can diverge to infinity. Our divergence result considers the same setting (fixing the optimization problem ahead) as our convergence result, indicating that there is a phase transition from divergence to convergence when increasing {$\beta$}2. These positive and negative results provide suggestions on how to tune Adam hyperparame{$\surd$}ters: for instance, when Adam does not work well, we suggest tuning up {$\beta$}2 and trying {$\beta$}1 {$<$} {$\beta$}2.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/Hans/Zotero/storage/CBXB85TL/Zhang et al. - 2022 - Adam Can Converge Without Any Modification On Upda.pdf}
}

@misc{zhaoLossFunctionsNeural2018,
  title = {Loss {{Functions}} for {{Neural Networks}} for {{Image Processing}}},
  author = {Zhao, Hang and Gallo, Orazio and Frosio, Iuri and Kautz, Jan},
  year = {2018},
  month = apr,
  number = {arXiv:1511.08861},
  eprint = {1511.08861},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-11-03},
  abstract = {Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is 2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/S663CM9G/Zhao et al. - 2018 - Loss Functions for Neural Networks for Image Proce.pdf}
}

@article{zhaoRestainNetSelfsupervisedDigital2022,
  title = {{{RestainNet}}: A Self-Supervised Digital Re-Stainer for Stain Normalization},
  shorttitle = {{{RestainNet}}},
  author = {Zhao, Bingchao and Lin, Jiatai and Liang, Changhong and Yi, Zongjian and Chen, Xin and Li, Bingbing and Qiu, Weihao and Li, Danyi and Liang, Li and Han, Chu and Liu, Zaiyi},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.13804 [cs, eess]},
  eprint = {2202.13804},
  primaryclass = {cs, eess},
  urldate = {2022-03-06},
  abstract = {Color inconsistency is an inevitable challenge in computational pathology, which generally happens because of stain intensity variations or sections scanned by different scanners. It harms the pathological image analysis methods, especially the learning-based models. A series of approaches have been proposed for stain normalization. However, most of them are lack of flexibility in practice. In this paper, we formulated stain normalization as a digital re-staining process and proposed a self-supervised learning model, which is called RestainNet. Our network is regarded as a digital re-stainer which learns how to re-stain an unstained (grayscale) image. Two digital stains, Hematoxylin (H) and Eosin (E), were extracted from the original image by Beer-Lambert's Law. We proposed a staining loss to maintain the correctness of stain intensity during the restaining process. Thanks to the self-supervised nature, paired training samples are no longer necessary, which demonstrates great flexibility in practical usage. Our RestainNet outperforms existing approaches and achieves state-of-the-art performance with regard to color correctness and structure preservation. We further conducted experiments on the segmentation and classification tasks and the proposed RestainNet achieved outstanding performance compared with SOTA methods. The self-supervised design allows the network learn any staining style with no extra effort.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/Hans/Zotero/storage/P9KX9QC3/Zhao et al. - 2022 - RestainNet a self-supervised digital re-stainer f.pdf}
}

@article{zhongAdversarialStyleAugmentation,
  title = {Adversarial {{Style Augmentation}} for {{Domain Generalized Urban-Scene Segmentation}}},
  author = {Zhong, Zhun and Zhao, Yuyang and Lee, Gim Hee and Sebe, Nicu},
  pages = {21},
  abstract = {In this paper, we consider the problem of domain generalization in semantic segmentation, which aims to learn a robust model using only labeled synthetic (source) data. The model is expected to perform well on unseen real (target) domains. Our study finds that the image style variation can largely influence the model's performance and the style features can be well represented by the channel-wise mean and standard deviation of images. Inspired by this, we propose a novel adversarial style augmentation (AdvStyle) approach, which can dynamically generate hard stylized images during training and thus can effectively prevent the model from overfitting on the source domain. Specifically, AdvStyle regards the style feature as a learnable parameter and updates it by adversarial training. The learned adversarial style feature is used to construct an adversarial image for robust model training. AdvStyle is easy to implement and can be readily applied to different models. Experiments on two synthetic-to-real semantic segmentation benchmarks demonstrate that AdvStyle can significantly improve the model performance on unseen real domains and show that we can achieve the state of the art. Moreover, AdvStyle can be employed to domain generalized image classification and produces a clear improvement on the considered datasets.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/CDB9I35X/Zhong et al. - Adversarial Style Augmentation for Domain Generali.pdf}
}

@article{zhouIBOTImageBERT2021,
  title = {{{iBOT}}: {{Image BERT Pre-Training}} with {{Online Tokenizer}}},
  shorttitle = {{{iBOT}}},
  author = {Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
  year = {2021},
  month = dec,
  journal = {arXiv:2111.07832 [cs]},
  eprint = {2111.07832},
  primaryclass = {cs},
  urldate = {2021-12-14},
  abstract = {The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM) (Devlin et al., 2019), where texts are first tokenized into semantically meaningful pieces. In this work, we study masked image modeling (MIM) and indicate the advantages and challenges of using a semantically meaningful visual tokenizer. We present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. Specifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage training pipeline where the tokenizer needs to be pretrained beforehand. We show the prominence of iBOT by achieving an 81.7\% linear probing accuracy and an 86.3\% fine-tuning accuracy evaluated on ImageNet1K. Beyond the state-of-the-art image classification results, we underline emerging local semantic patterns, which helps the models to obtain strong robustness against common corruptions and achieve leading results on dense downstream tasks, e.g., object detection, instance segmentation, and semantic segmentation. The code and models are publicly available at https://github.com/bytedance/ibot.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/NGM6NY3X/iBOT - 2021 - Zhouet al..pdf}
}

@misc{zhuInterpretablePredictionLung2022,
  title = {Interpretable {{Prediction}} of {{Lung Squamous Cell Carcinoma Recurrence With Self-supervised Learning}}},
  author = {Zhu, Weicheng and {Fernandez-Granda}, Carlos and Razavian, Narges},
  year = {2022},
  month = mar,
  number = {arXiv:2203.12204},
  eprint = {2203.12204},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.12204},
  urldate = {2022-11-21},
  abstract = {Lung squamous cell carcinoma (LSCC) has a high recurrence and metastasis rate. Factors influencing recurrence and metastasis are currently unknown and there are no distinct histopathological or morphological features indicating the risks of recurrence and metastasis in LSCC. Our study focuses on the recurrence prediction of LSCC based on H\&E-stained histopathological whole-slide images (WSI). Due to the small size of LSCC cohorts in terms of patients with available recurrence information, standard end-to-end learning with various convolutional neural networks for this task tends to overfit. Also, the predictions made by these models are hard to interpret. Histopathology WSIs are typically very large and are therefore processed as a set of smaller tiles. In this work, we propose a novel conditional self-supervised learning (SSL) method to learn representations of WSI at the tile level first, and leverage clustering algorithms to identify the tiles with similar histopathological representations. The resulting representations and clusters from self-supervision are used as features of a survival model for recurrence prediction at the patient level. Using two publicly available datasets from TCGA and CPTAC, we show that our LSCC recurrence prediction survival model outperforms both LSCC pathological stage-based approach and machine learning baselines such as multiple instance learning. The proposed method also enables us to explain the recurrence histopathological risk factors via the derived clusters. This can help pathologists derive new hypotheses regarding morphological features associated with LSCC recurrence.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/Hans/Zotero/storage/A59EY3AG/Zhu et al. - Interpretable Prediction of Lung Squamous Cell Carcinoma Recurrence With Self-supervised Learning.pdf}
}

@article{zimmerAutoPyTorchTabularMultiFidelity2020,
  title = {Auto-{{PyTorch Tabular}}: {{Multi-Fidelity MetaLearning}} for {{Efficient}} and {{Robust AutoDL}}},
  shorttitle = {Auto-{{PyTorch Tabular}}},
  author = {Zimmer, Lucas and Lindauer, Marius and Hutter, Frank},
  year = {2020},
  month = jun,
  urldate = {2022-02-09},
  abstract = {While early AutoML frameworks focused on optimizing traditional ML pipelines and their hyperparameters, a recent trend in AutoML is to focus on neural architecture search. In this paper, we introduce Auto-PyTorch, which brings the best of these two worlds together by jointly and robustly optimizing the architecture of networks and the training hyperparameters to enable fully automated deep learning (AutoDL). Auto-PyTorch achieves state-of-the-art performance on several tabular benchmarks by combining multi-fidelity optimization with portfolio construction for warmstarting and ensembling of deep neural networks (DNNs) and common baselines for tabular data. To thoroughly study our assumptions on how to design such an AutoDL system, we additionally introduce a new benchmark on learning curves for DNNs, dubbed LCBench, and run extensive ablation studies of the full Auto-PyTorch on typical AutoML benchmarks, eventually showing that Auto-PyTorch performs better than several state-of-the-art competitors on average.},
  langid = {english},
  file = {/Users/Hans/Zotero/storage/5ZXSPFFD/Auto-PyTorch Tabular - 2020 - Zimmer et al..pdf}
}
